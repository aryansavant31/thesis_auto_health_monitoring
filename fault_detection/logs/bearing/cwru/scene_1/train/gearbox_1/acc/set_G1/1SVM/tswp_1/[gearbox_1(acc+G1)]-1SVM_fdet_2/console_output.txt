=== SCRIPT EXECUTION LOG ===
Script: fault_detection.train.py
Base Name: [gearbox_1(acc+G1)]-1SVM_fdet_2
Start Time: 2025-09-01 18:21:29
End Time: 2025-09-01 18:21:56

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting fault detection model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) 0_N    : [OG]

- **Unhealthy configs**
  (1) 0_B-021    : [OG]

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) gearbox   : [acc]

Node group name: gearbox_1
Signal group name: acc



For ds_type 'OK' and others....

Maximum timesteps across all node types: 243,938

No data interpolation applied.

No 'fs_matrix' recieved from the data. Hence, using the currently set 'fs' in data_config. Current fs:
[[48000]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
---------------------------------------------
Total samples: 1948 
Train: 1520/1558 [OK=765, NOK=755, UK=0], Test: 320/389 [OK=161, NOK=159, UK=0], Val: 0/0 [OK=0, NOK=0, UK=0],
Remainder: 1 [OK=1, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 19
torch.Size([80, 1, 500, 1])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 4
torch.Size([80, 1, 500, 1]) 

---------------------------------------------------------------------------

Anomaly Detector Model Initialized with the following configurations:
Model type: OneClassSVM
Kernel: rbf
Gamma: scale
Nu: 0.01

---------------------------------------------------------------------------
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2

---------------------------------------------------------------------------

Initializing input processors for anomaly detection model...

>> Domain transformer initialized: freq(cutoff_freq=100)

>> No raw data normalization is applied

>> No feature normalization is applied

>> No frequency feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

No seperate features extracted, so using components as features: [comp0_dim0, comp1_dim0, comp2_dim0, comp3_dim0...comp246_dim0, comp247_dim0, comp248_dim0, comp249_dim0]

Fitting anomaly detection model...

Model fitted successfully in 0.26 seconds

Dataframe is as follows:
      comp0_dim0  comp1_dim0  comp2_dim0  ...   rep_num    scores  pred_label
0       0.004528    0.003895    0.021196  ...  1001.577  0.591705           0
1       0.005774    0.010583    0.036909  ...  1001.613  0.394860           0
2       0.003567    0.011046    0.011124  ...  1001.270  0.060641           0
3       0.006904    0.013225    0.004947  ...  1001.437  0.331276           0
4       0.004621    0.008662    0.009975  ...  1001.187  0.061326           0
...          ...         ...         ...  ...       ...       ...         ...
1515    0.033460    0.073867    0.043842  ...  1001.096  0.868919           0
1516    0.026332    0.054859    0.038399  ...  1001.942  0.576148           0
1517    0.030331    0.046579    0.017401  ...  1001.206  0.723842           0
1518    0.010972    0.016811    0.017145  ...  1001.064  0.067123           0
1519    0.042378    0.077278    0.013657  ...  1001.689  1.236642           0

[1520 rows x 254 columns]

Training accuracy: 0.51

Training hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2

Model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2\anomaly_detector.pkl

---------------------------------------------------------------------------

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2\test

Testing anomaly detection model...

Initializing input processors for anomaly detection model...

>> Domain transformer initialized: freq(cutoff_freq=100)

>> No raw data normalization is applied

>> No feature normalization is applied

>> No frequency feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

No seperate features extracted, so using components as features: [comp0_dim0, comp1_dim0, comp2_dim0, comp3_dim0...comp246_dim0, comp247_dim0, comp248_dim0, comp249_dim0]

Dataframe is as follows:
     comp0_dim0  comp1_dim0  comp2_dim0  ...   rep_num    scores  pred_label
0      0.022444    0.035512    0.017138  ...  1001.570  0.064392           0
1      0.003319    0.005483    0.000751  ...  1001.080  0.004576           0
2      0.001801    0.004776    0.001912  ...  1001.304 -0.002716           1
3      0.001757    0.009080    0.015591  ...  1001.575  0.056721           0
4      0.002313    0.027790    0.025767  ...  1001.748  1.031827           0
..          ...         ...         ...  ...       ...       ...         ...
315    0.007768    0.019672    0.008389  ...  1001.114  0.097730           0
316    0.011837    0.018049    0.012467  ...  1001.496  0.085406           0
317    0.002635    0.012447    0.013649  ...  1001.038  0.059171           0
318    0.033588    0.039596    0.035410  ...  1001.834  0.928072           0
319    0.036435    0.065770    0.029463  ...  1001.495  0.656225           0

[320 rows x 254 columns]

Test accuracy: 0.50
Precision: 0.50, Recall: 0.01, F1-score: 0.01

Testing hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2\test

---------------------------------------------------------------------------

<<<<<<<<<<<< CONFUSION MATRIX >>>>>>>>>>>>

> Creating confusion matrix for [gearbox_1(acc+G1)]-1SVM_fdet_2 / test...

Confusion matrix logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2\test


<<<<<<<<<<<< ANOMALY SCORE DISTRIBUTION (PRED_LABEL) >>>>>>>>>>>>

Creating anomaly score distribution plot for [gearbox_1(acc+G1)]-1SVM_fdet_2 / test...

Total OK samples (from pred_label) : 318
---------- Samples in OK bins ----------
- **Bin 0 (-0.01215, 0.01328)**: 1 (1,001.080), 89 (1,001.767), 107 (1,001.240), 188 (1,001.323), 232 (1,001.151), 235 (1,001.099), 269 (1,001.927)
- **Bin 1 (0.01328, 0.03870)**: 19 (1,001.386), 26 (1,001.484), 69 (1,001.734), 70 (1,001.749), 73 (1,001.310), 78 (1,001.196), 93 (1,001.729), 94 (1,001.692), 100 (1,001.551), 120 (1,001.805), 123 (1,001.003), 131 (1,001.602), 144 (1,001.925), 158 (1,001.490), 186 (1,001.040), 190 (1,001.278), 204 (1,001.025), 208 (1,001.243), 231 (1,001.161), 234 (1,001.497), 240 (1,001.168), 251 (1,001.709), 277 (1,001.625), 307 (1,001.668), 313 (1,001.483)
- **Bin 2 (0.03870, 0.06412)**: 3 (1,001.575), 9 (1,001.696), 16 (1,001.045), 18 (1,001.259), 20 (1,001.485), 22 (1,001.029), 30 (1,001.732), 32 (1,001.966), 35 (1,001.455), 38 (1,001.544), 40 (1,001.229), 45 (1,001.501), 47 (1,001.601), 55 (1,001.362), 58 (1,001.855), 63 (1,001.910), 64 (1,001.339), 67 (1,001.085), 80 (1,001.109), 84 (1,001.568), 86 (1,001.936), 95 (1,001.291), 96 (1,001.005), 98 (1,001.322), 103 (1,001.635), 104 (1,001.654), 111 (1,001.788), 116 (1,001.409), 142 (1,001.912), 148 (1,001.704), 166 (1,001.350), 167 (1,001.036), 168 (1,001.645), 176 (1,001.227), 180 (1,001.037), 203 (1,001.224), 209 (1,001.703), 214 (1,001.117), 220 (1,001.429), 224 (1,001.752), 229 (1,001.466), 233 (1,001.932), 241 (1,001.881), 242 (1,001.264), 259 (1,001.280), 263 (1,001.468), 265 (1,001.676), 268 (1,001.247), 279 (1,001.307), 280 (1,001.230), 282 (1,001.355), 286 (1,001.915), 288 (1,001.796), 291 (1,001.700), 297 (1,001.424), 299 (1,001.880), 300 (1,001.959), 302 (1,001.149), 308 (1,001.482), 312 (1,001.687), 317 (1,001.038)
- **Bin 3 (0.06412, 0.08955)**: 0 (1,001.570), 6 (1,001.564), 13 (1,001.101), 15 (1,001.647), 28 (1,001.062), 29 (1,001.453), 34 (1,001.631), 42 (1,001.556), 48 (1,001.335), 49 (1,001.660), 59 (1,001.220), 74 (1,001.238), 82 (1,001.886), 85 (1,001.242), 87 (1,001.461), 101 (1,001.075), 102 (1,001.390), 106 (1,001.471), 114 (1,001.110), 119 (1,001.143), 127 (1,001.223), 130 (1,001.190), 132 (1,001.019), 149 (1,001.756), 161 (1,001.691), 171 (1,001.772), 175 (1,001.035), 189 (1,001.917), 193 (1,001.366), 198 (1,001.907), 199 (1,001.822), 200 (1,001.419), 212 (1,001.269), 215 (1,001.478), 219 (1,001.832), 221 (1,001.083), 222 (1,001.357), 238 (1,001.574), 244 (1,001.505), 246 (1,001.002), 248 (1,001.786), 250 (1,001.217), 252 (1,001.860), 254 (1,001.682), 260 (1,001.941), 264 (1,001.251), 271 (1,001.438), 273 (1,001.871), 278 (1,001.089), 293 (1,001.670), 294 (1,001.525), 306 (1,001.853), 314 (1,001.824), 316 (1,001.496)
- **Bin 4 (0.08955, 0.11497)**: 36 (1,001.336), 39 (1,001.434), 72 (1,001.452), 76 (1,001.622), 81 (1,001.624), 139 (1,001.432), 146 (1,001.620), 156 (1,001.744), 172 (1,001.778), 217 (1,001.671), 225 (1,001.666), 281 (1,001.056), 304 (1,001.480), 315 (1,001.114)
- **Bin 5 (0.11497, 0.14040)**: 160 (1,001.585), 192 (1,001.484)
- **Bin 6 (0.14040, 0.16582)**: 141 (1,001.751)
- **Bin 7 (0.16582, 0.19124)**: 165 (1,001.738)
- **Bin 9 (0.21667, 0.24209)**: 37 (1,001.465), 133 (1,001.692), 194 (1,001.700)
- **Bin 10 (0.24209, 0.26752)**: 157 (1,001.801), 309 (1,001.539)
- **Bin 11 (0.26752, 0.29294)**: 239 (1,001.001), 258 (1,001.257)
- **Bin 12 (0.29294, 0.31837)**: 134 (1,001.860), 145 (1,001.625), 162 (1,001.383), 266 (1,001.139)
- **Bin 13 (0.31837, 0.34379)**: 7 (1,001.881), 21 (1,001.259)
- **Bin 14 (0.34379, 0.36921)**: 75 (1,001.673), 92 (1,001.635), 129 (1,001.815), 216 (1,001.307)
- **Bin 15 (0.36921, 0.39464)**: 83 (1,001.946), 113 (1,001.185), 115 (1,001.140), 128 (1,001.596)
- **Bin 16 (0.39464, 0.42006)**: 213 (1,001.680)
- **Bin 17 (0.42006, 0.44549)**: 27 (1,001.201), 136 (1,001.825), 159 (1,001.463)
- **Bin 18 (0.44549, 0.47091)**: 8 (1,001.090), 163 (1,001.518), 191 (1,001.352)
- **Bin 19 (0.47091, 0.49633)**: 23 (1,001.091), 88 (1,001.157), 91 (1,001.258), 205 (1,001.960), 223 (1,001.889), 290 (1,001.448)
- **Bin 20 (0.49633, 0.52176)**: 17 (1,001.674), 77 (1,001.284), 118 (1,001.502), 206 (1,001.532), 284 (1,001.874), 301 (1,001.242)
- **Bin 21 (0.52176, 0.54718)**: 135 (1,001.503), 218 (1,001.063), 272 (1,001.782), 283 (1,001.426), 311 (1,001.974)
- **Bin 22 (0.54718, 0.57261)**: 56 (1,001.873), 97 (1,001.114), 121 (1,001.003), 210 (1,001.583), 267 (1,001.945), 275 (1,001.847)
- **Bin 23 (0.57261, 0.59803)**: 24 (1,001.820), 41 (1,001.750), 50 (1,001.202), 79 (1,001.265), 211 (1,001.705), 245 (1,001.500)
- **Bin 24 (0.59803, 0.62346)**: 71 (1,001.542), 112 (1,001.535), 117 (1,001.677), 150 (1,001.171), 154 (1,001.887), 173 (1,001.330), 255 (1,001.209), 270 (1,001.823), 298 (1,001.239)
- **Bin 25 (0.62346, 0.64888)**: 14 (1,001.704), 51 (1,001.040), 61 (1,001.933), 143 (1,001.282), 147 (1,001.494), 151 (1,001.401), 243 (1,001.920), 296 (1,001.180)
- **Bin 26 (0.64888, 0.67430)**: 31 (1,001.152), 54 (1,001.534), 109 (1,001.697), 137 (1,001.849), 276 (1,001.715), 319 (1,001.495)
- **Bin 27 (0.67430, 0.69973)**: 5 (1,001.827), 152 (1,001.019), 177 (1,001.931), 181 (1,001.883)
- **Bin 28 (0.69973, 0.72515)**: 11 (1,001.786), 57 (1,001.926), 183 (1,001.789), 202 (1,001.838)
- **Bin 29 (0.72515, 0.75058)**: 52 (1,001.155), 110 (1,001.854), 125 (1,001.918), 140 (1,001.425), 179 (1,001.219), 253 (1,001.344), 295 (1,001.348)
- **Bin 30 (0.75058, 0.77600)**: 124 (1,001.417), 138 (1,001.343), 155 (1,001.182), 170 (1,001.287), 257 (1,001.204), 274 (1,001.446), 285 (1,001.472)
- **Bin 31 (0.77600, 0.80142)**: 25 (1,001.230), 236 (1,001.273)
- **Bin 32 (0.80142, 0.82685)**: 33 (1,001.304), 201 (1,001.693), 228 (1,001.835), 256 (1,001.836)
- **Bin 33 (0.82685, 0.85227)**: 99 (1,001.703), 122 (1,001.037), 174 (1,001.856), 178 (1,001.863), 226 (1,001.447), 227 (1,001.395), 247 (1,001.739), 289 (1,001.658)
- **Bin 34 (0.85227, 0.87770)**: 184 (1,001.886), 207 (1,001.122), 249 (1,001.133), 292 (1,001.501)
- **Bin 35 (0.87770, 0.90312)**: 44 (1,001.548), 60 (1,001.017), 262 (1,001.584)
- **Bin 36 (0.90312, 0.92855)**: 43 (1,001.840), 68 (1,001.791), 105 (1,001.402), 305 (1,001.491), 310 (1,001.826), 318 (1,001.834)
- **Bin 37 (0.92855, 0.95397)**: 62 (1,001.528), 65 (1,001.323), 153 (1,001.844)
- **Bin 38 (0.95397, 0.97939)**: 108 (1,001.387), 169 (1,001.523)
- **Bin 40 (1.00482, 1.03024)**: 196 (1,001.062)
- **Bin 41 (1.03024, 1.05567)**: 4 (1,001.748), 195 (1,001.690), 261 (1,001.940)
- **Bin 42 (1.05567, 1.08109)**: 185 (1,001.892), 303 (1,001.066)
- **Bin 43 (1.08109, 1.10652)**: 12 (1,001.678), 182 (1,001.018), 230 (1,001.008)
- **Bin 44 (1.10652, 1.13194)**: 90 (1,001.077), 197 (1,001.948)
- **Bin 45 (1.13194, 1.15736)**: 66 (1,001.620), 287 (1,001.300)
- **Bin 46 (1.15736, 1.18279)**: 53 (1,001.656), 187 (1,001.569)
- **Bin 48 (1.20821, 1.23364)**: 46 (1,001.811)
- **Bin 49 (1.23364, 1.25906)**: 164 (1,001.966), 237 (1,001.908)

Total NOK samples (from pred_label) : 2
---------- Samples in NOK bins ----------
- **Bin 0 (-0.01215, 0.01328)**: 2 (1,001.304), 10 (1,001.711)


Anomaly score distribution plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2\test


<<<<<<<<<<<< ANOMALY SCORE DISTRIBUTION (GIVEN_LABEL) >>>>>>>>>>>>

Creating anomaly score distribution plot for [gearbox_1(acc+G1)]-1SVM_fdet_2 / test...

Total OK samples (from given_label) : 161
---------- Samples in OK bins ----------
- **Bin 0 (-0.01215, 0.01328)**: 1 (1,001.080), 2 (1,001.304), 89 (1,001.767), 107 (1,001.240), 188 (1,001.323), 232 (1,001.151), 235 (1,001.099), 269 (1,001.927)
- **Bin 1 (0.01328, 0.03870)**: 19 (1,001.386), 26 (1,001.484), 69 (1,001.734), 70 (1,001.749), 73 (1,001.310), 78 (1,001.196), 93 (1,001.729), 94 (1,001.692), 100 (1,001.551), 120 (1,001.805), 123 (1,001.003), 131 (1,001.602), 144 (1,001.925), 158 (1,001.490), 186 (1,001.040), 190 (1,001.278), 204 (1,001.025), 208 (1,001.243), 231 (1,001.161), 234 (1,001.497), 240 (1,001.168), 251 (1,001.709), 277 (1,001.625), 307 (1,001.668), 313 (1,001.483)
- **Bin 2 (0.03870, 0.06412)**: 3 (1,001.575), 9 (1,001.696), 16 (1,001.045), 18 (1,001.259), 20 (1,001.485), 22 (1,001.029), 30 (1,001.732), 32 (1,001.966), 35 (1,001.455), 38 (1,001.544), 40 (1,001.229), 45 (1,001.501), 47 (1,001.601), 55 (1,001.362), 58 (1,001.855), 63 (1,001.910), 64 (1,001.339), 67 (1,001.085), 80 (1,001.109), 84 (1,001.568), 86 (1,001.936), 95 (1,001.291), 96 (1,001.005), 98 (1,001.322), 103 (1,001.635), 104 (1,001.654), 111 (1,001.788), 116 (1,001.409), 142 (1,001.912), 148 (1,001.704), 166 (1,001.350), 167 (1,001.036), 168 (1,001.645), 176 (1,001.227), 180 (1,001.037), 203 (1,001.224), 209 (1,001.703), 214 (1,001.117), 220 (1,001.429), 224 (1,001.752), 229 (1,001.466), 233 (1,001.932), 241 (1,001.881), 242 (1,001.264), 259 (1,001.280), 263 (1,001.468), 265 (1,001.676), 268 (1,001.247), 279 (1,001.307), 280 (1,001.230), 282 (1,001.355), 286 (1,001.915), 288 (1,001.796), 291 (1,001.700), 297 (1,001.424), 299 (1,001.880), 300 (1,001.959), 302 (1,001.149), 308 (1,001.482), 312 (1,001.687), 317 (1,001.038)
- **Bin 3 (0.06412, 0.08955)**: 0 (1,001.570), 6 (1,001.564), 13 (1,001.101), 15 (1,001.647), 28 (1,001.062), 29 (1,001.453), 34 (1,001.631), 42 (1,001.556), 48 (1,001.335), 49 (1,001.660), 59 (1,001.220), 74 (1,001.238), 82 (1,001.886), 85 (1,001.242), 87 (1,001.461), 101 (1,001.075), 102 (1,001.390), 106 (1,001.471), 114 (1,001.110), 119 (1,001.143), 127 (1,001.223), 130 (1,001.190), 132 (1,001.019), 149 (1,001.756), 161 (1,001.691), 171 (1,001.772), 175 (1,001.035), 189 (1,001.917), 193 (1,001.366), 198 (1,001.907), 199 (1,001.822), 200 (1,001.419), 212 (1,001.269), 215 (1,001.478), 219 (1,001.832), 221 (1,001.083), 222 (1,001.357), 238 (1,001.574), 244 (1,001.505), 246 (1,001.002), 248 (1,001.786), 250 (1,001.217), 252 (1,001.860), 254 (1,001.682), 260 (1,001.941), 264 (1,001.251), 271 (1,001.438), 273 (1,001.871), 278 (1,001.089), 293 (1,001.670), 294 (1,001.525), 306 (1,001.853), 314 (1,001.824), 316 (1,001.496)
- **Bin 4 (0.08955, 0.11497)**: 36 (1,001.336), 39 (1,001.434), 72 (1,001.452), 76 (1,001.622), 81 (1,001.624), 139 (1,001.432), 146 (1,001.620), 172 (1,001.778), 217 (1,001.671), 225 (1,001.666), 281 (1,001.056), 304 (1,001.480), 315 (1,001.114)

Total NOK samples (from given_label) : 159
---------- Samples in NOK bins ----------
- **Bin 0 (-0.01215, 0.01328)**: 10 (1,001.711)
- **Bin 4 (0.08955, 0.11497)**: 156 (1,001.744)
- **Bin 5 (0.11497, 0.14040)**: 160 (1,001.585), 192 (1,001.484)
- **Bin 6 (0.14040, 0.16582)**: 141 (1,001.751)
- **Bin 7 (0.16582, 0.19124)**: 165 (1,001.738)
- **Bin 9 (0.21667, 0.24209)**: 37 (1,001.465), 133 (1,001.692), 194 (1,001.700)
- **Bin 10 (0.24209, 0.26752)**: 157 (1,001.801), 309 (1,001.539)
- **Bin 11 (0.26752, 0.29294)**: 239 (1,001.001), 258 (1,001.257)
- **Bin 12 (0.29294, 0.31837)**: 134 (1,001.860), 145 (1,001.625), 162 (1,001.383), 266 (1,001.139)
- **Bin 13 (0.31837, 0.34379)**: 7 (1,001.881), 21 (1,001.259)
- **Bin 14 (0.34379, 0.36921)**: 75 (1,001.673), 92 (1,001.635), 129 (1,001.815), 216 (1,001.307)
- **Bin 15 (0.36921, 0.39464)**: 83 (1,001.946), 113 (1,001.185), 115 (1,001.140), 128 (1,001.596)
- **Bin 16 (0.39464, 0.42006)**: 213 (1,001.680)
- **Bin 17 (0.42006, 0.44549)**: 27 (1,001.201), 136 (1,001.825), 159 (1,001.463)
- **Bin 18 (0.44549, 0.47091)**: 8 (1,001.090), 163 (1,001.518), 191 (1,001.352)
- **Bin 19 (0.47091, 0.49633)**: 23 (1,001.091), 88 (1,001.157), 91 (1,001.258), 205 (1,001.960), 223 (1,001.889), 290 (1,001.448)
- **Bin 20 (0.49633, 0.52176)**: 17 (1,001.674), 77 (1,001.284), 118 (1,001.502), 206 (1,001.532), 284 (1,001.874), 301 (1,001.242)
- **Bin 21 (0.52176, 0.54718)**: 135 (1,001.503), 218 (1,001.063), 272 (1,001.782), 283 (1,001.426), 311 (1,001.974)
- **Bin 22 (0.54718, 0.57261)**: 56 (1,001.873), 97 (1,001.114), 121 (1,001.003), 210 (1,001.583), 267 (1,001.945), 275 (1,001.847)
- **Bin 23 (0.57261, 0.59803)**: 24 (1,001.820), 41 (1,001.750), 50 (1,001.202), 79 (1,001.265), 211 (1,001.705), 245 (1,001.500)
- **Bin 24 (0.59803, 0.62346)**: 71 (1,001.542), 112 (1,001.535), 117 (1,001.677), 150 (1,001.171), 154 (1,001.887), 173 (1,001.330), 255 (1,001.209), 270 (1,001.823), 298 (1,001.239)
- **Bin 25 (0.62346, 0.64888)**: 14 (1,001.704), 51 (1,001.040), 61 (1,001.933), 143 (1,001.282), 147 (1,001.494), 151 (1,001.401), 243 (1,001.920), 296 (1,001.180)
- **Bin 26 (0.64888, 0.67430)**: 31 (1,001.152), 54 (1,001.534), 109 (1,001.697), 137 (1,001.849), 276 (1,001.715), 319 (1,001.495)
- **Bin 27 (0.67430, 0.69973)**: 5 (1,001.827), 152 (1,001.019), 177 (1,001.931), 181 (1,001.883)
- **Bin 28 (0.69973, 0.72515)**: 11 (1,001.786), 57 (1,001.926), 183 (1,001.789), 202 (1,001.838)
- **Bin 29 (0.72515, 0.75058)**: 52 (1,001.155), 110 (1,001.854), 125 (1,001.918), 140 (1,001.425), 179 (1,001.219), 253 (1,001.344), 295 (1,001.348)
- **Bin 30 (0.75058, 0.77600)**: 124 (1,001.417), 138 (1,001.343), 155 (1,001.182), 170 (1,001.287), 257 (1,001.204), 274 (1,001.446), 285 (1,001.472)
- **Bin 31 (0.77600, 0.80142)**: 25 (1,001.230), 236 (1,001.273)
- **Bin 32 (0.80142, 0.82685)**: 33 (1,001.304), 201 (1,001.693), 228 (1,001.835), 256 (1,001.836)
- **Bin 33 (0.82685, 0.85227)**: 99 (1,001.703), 122 (1,001.037), 174 (1,001.856), 178 (1,001.863), 226 (1,001.447), 227 (1,001.395), 247 (1,001.739), 289 (1,001.658)
- **Bin 34 (0.85227, 0.87770)**: 184 (1,001.886), 207 (1,001.122), 249 (1,001.133), 292 (1,001.501)
- **Bin 35 (0.87770, 0.90312)**: 44 (1,001.548), 60 (1,001.017), 262 (1,001.584)
- **Bin 36 (0.90312, 0.92855)**: 43 (1,001.840), 68 (1,001.791), 105 (1,001.402), 305 (1,001.491), 310 (1,001.826), 318 (1,001.834)
- **Bin 37 (0.92855, 0.95397)**: 62 (1,001.528), 65 (1,001.323), 153 (1,001.844)
- **Bin 38 (0.95397, 0.97939)**: 108 (1,001.387), 169 (1,001.523)
- **Bin 40 (1.00482, 1.03024)**: 196 (1,001.062)
- **Bin 41 (1.03024, 1.05567)**: 4 (1,001.748), 195 (1,001.690), 261 (1,001.940)
- **Bin 42 (1.05567, 1.08109)**: 185 (1,001.892), 303 (1,001.066)
- **Bin 43 (1.08109, 1.10652)**: 12 (1,001.678), 182 (1,001.018), 230 (1,001.008)
- **Bin 44 (1.10652, 1.13194)**: 90 (1,001.077), 197 (1,001.948)
- **Bin 45 (1.13194, 1.15736)**: 66 (1,001.620), 287 (1,001.300)
- **Bin 46 (1.15736, 1.18279)**: 53 (1,001.656), 187 (1,001.569)
- **Bin 48 (1.20821, 1.23364)**: 46 (1,001.811)
- **Bin 49 (1.23364, 1.25906)**: 164 (1,001.966), 237 (1,001.908)


Anomaly score distribution plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2\test


<<<<<<<<<<<< PAIR PLOT >>>>>>>>>>>>

> Creating pair plot for [gearbox_1(acc+G1)]-1SVM_fdet_2 / test...

Pair plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\fault_detection\logs\bearing\cwru\scene_1\train\gearbox_1\acc\set_G1\1SVM\tswp_1\[gearbox_1(acc+G1)]-1SVM_fdet_2\test


===========================================================================

Fault detection model '[gearbox_1(acc+G1)]-1SVM_fdet_2' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-01 18:21:56
