=== SCRIPT EXECUTION LOG ===
Script: fault_detection.train.py
Base Name: [ammf(acc+E1)]-1SVM_fdet_1
Start Time: 2025-09-01 20:12:19
End Time: 2025-09-01 20:13:30

CPU: Intel64 Family 6 Model 79 Stepping 1, GenuineIntel (Cores: 12), Max Frequency: 2594.00 MHz
GPU: None detected
OS: Windows 10 (10.0.19045)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting fault detection model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) E1_set01_M=mAI26    : [OG]
  (2) E1_set01_M=mAQ10    : [OG]
  (3) E1_set01_M=mAQ87    : [OG]
  (4) E1_set01_M=mAS23    : [OG]
  (5) E1_set01_M=mAU64    : [OG]
  (6) E1_set01_M=mBF18    : [OG]
  (7) E1_set01_M=mBF38    : [OG]
  (8) E1_set01_M=mBF92    : [OG]
  (9) E1_set01_M=mBH22    : [OG]
  (10) E1_set01_M=mBI93    : [OG]
  (11) E1_set01_M=mBJ96    : [OG]
  (12) E1_set01_M=mBK70    : [OG]
  (13) E1_set01_M=mBN34    : [OG]
  (14) E1_set01_M=mBP09    : [OG]
  (15) E1_set01_M=mBP98    : [OG]
  (16) E1_set01_M=mBS36    : [OG]
  (17) E1_set01_M=mBS45    : [OG]
  (18) E1_set01_M=mBU17    : [OG]
  (19) E1_set01_M=mBW64    : [OG]
  (20) E1_set01_M=mBY13    : [OG]
  (21) E1_set01_M=mBY93    : [OG]
  (22) E1_set01_M=mCD86    : [OG]
  (23) E1_set01_M=mCF56    : [OG]
  (24) E1_set01_M=mCF78    : [OG]
  (25) E1_set01_M=mCF96    : [OG]
  (26) E1_set01_M=mCG89    : [OG]
  (27) E1_set01_M=mCO66    : [OG]
  (28) E1_set01_M=mCQ94    : [OG]
  (29) E1_set01_M=mCX30    : [OG]
  (30) E1_set01_M=mCZ47    : [OG]
  (31) E1_set01_M=mDD00    : [OG]
  (32) E1_set01_M=mDH60    : [OG]
  (33) E1_set01_M=mDI25    : [OG]
  (34) E1_set01_M=mDI82    : [OG]
  (35) E1_set01_M=mDJ62    : [OG]
  (36) E1_set01_M=mDK28    : [OG]
  (37) E1_set01_M=mDK50    : [OG]
  (38) E1_set01_M=mDL40    : [OG]
  (39) E1_set01_M=mDM53    : [OG]
  (40) E1_set01_M=mDN98    : [OG]
  (41) E1_set01_M=mDP98    : [OG]
  (42) E1_set01_M=mDR32    : [OG]
  (43) E1_set01_M=mDS38    : [OG]
  (44) E1_set01_M=mDT50    : [OG]
  (45) E1_set01_M=mDT62    : [OG]
  (46) E1_set01_M=mDT74    : [OG]
  (47) E1_set01_M=mED29    : [OG]
  (48) E1_set01_M=mED66    : [OG]
  (49) E1_set01_M=mEE75    : [OG]
  (50) E1_set01_M=mEK25    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) ammf   : [acc_rx, acc_ry, acc_rz, acc_x, acc_y, acc_z]

Node group name: ammf
Signal group name: acc



For ds_type 'OK' and others....

Maximum timesteps across all node types: 4,098

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[2500.0002, 2500.0002, 2500.0002, 2500.0002, 2500.0002, 2500.0002]]

Exclusive rep numbers found in keys of hdf5 file. Hence, using them as rep numbers.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
---------------------------------------------
Total samples: 2396 
Train: 1916/1916 [OK=1916, NOK=0, UK=0], Test: 479/479 [OK=479, NOK=0, UK=0], Val: 0/0 [OK=0, NOK=0, UK=0],
Remainder: 1 [OK=1, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 1916
torch.Size([1, 1, 1000, 6])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 479
torch.Size([1, 1, 1000, 6]) 

---------------------------------------------------------------------------

Anomaly Detector Model Initialized with the following configurations:
Model type: OneClassSVM
Kernel: rbf
Gamma: scale
Nu: 0.01

---------------------------------------------------------------------------
Model parameters saved to C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1.

Training environment set. Training will be logged at: C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1

---------------------------------------------------------------------------

Initializing input processors for anomaly detection model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> No raw data normalization is applied

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

No seperate features extracted, so using components as features: [comp0_dim0, comp0_dim1, comp0_dim2, comp0_dim3...comp999_dim2, comp999_dim3, comp999_dim4, comp999_dim5]

Fitting anomaly detection model...

Model fitted successfully in 6.84 seconds

Dataframe is as follows:
      comp0_dim0  comp0_dim1  comp0_dim2  comp0_dim3  ...  given_label    rep_num    scores  pred_label
0       0.000181    0.000260   -0.000065    0.000172  ...          0.0  12030.001  0.452928           0
1       0.000175    0.000319   -0.000759    0.000898  ...          0.0   7020.003  0.000299           0
2      -0.000173    0.000271    0.000003   -0.000755  ...          0.0   9006.004 -0.000085           1
3      -0.000171    0.000362   -0.000019    0.000035  ...          0.0   3030.002  0.430677           0
4       0.000003    0.000297   -0.000254    0.000424  ...          0.0   9009.004  0.522510           0
...          ...         ...         ...         ...  ...          ...        ...       ...         ...
1911   -0.000307   -0.000123    0.000223   -0.000222  ...          0.0   3002.004  0.326842           0
1912    0.000057   -0.000192    0.000183   -0.000555  ...          0.0   3003.003  0.207796           0
1913    0.000034    0.000023    0.000175   -0.000705  ...          0.0   9033.002  0.011069           0
1914    0.000175   -0.000111    0.000098   -0.000315  ...          0.0   7035.002  0.508108           0
1915   -0.000641   -0.000369    0.000045    0.000084  ...          0.0   3032.004 -0.000041           1

[1916 rows x 6004 columns]

Training accuracy: 0.95

Training hyperparameters logged for tensorboard at C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1

Model saved at C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1\anomaly_detector.pkl

---------------------------------------------------------------------------

Testing environment set. Testing will be logged at: C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1\test

Testing anomaly detection model...

Initializing input processors for anomaly detection model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> No raw data normalization is applied

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

No seperate features extracted, so using components as features: [comp0_dim0, comp0_dim1, comp0_dim2, comp0_dim3...comp999_dim2, comp999_dim3, comp999_dim4, comp999_dim5]

Dataframe is as follows:
     comp0_dim0  comp0_dim1  comp0_dim2  comp0_dim3  ...  given_label    rep_num    scores  pred_label
0      0.000117   -0.000152   -0.000138    0.000116  ...          0.0  10038.003  0.248969           0
1     -0.000235   -0.000182    0.000426   -0.000709  ...          0.0  11050.001  0.277255           0
2     -0.000150   -0.000335    0.000141   -0.000395  ...          0.0  10013.001  0.418781           0
3     -0.000220   -0.000500    0.000063   -0.000004  ...          0.0   8040.003  0.342023           0
4     -0.000036    0.000421   -0.000216    0.000478  ...          0.0   3046.002  0.228362           0
..          ...         ...         ...         ...  ...          ...        ...       ...         ...
474   -0.000168    0.000363   -0.000152    0.000238  ...          0.0   6047.004  0.561994           0
475   -0.000164   -0.000217    0.000160   -0.000303  ...          0.0   8037.004  0.392802           0
476    0.000150    0.000123    0.000118   -0.000265  ...          0.0   9010.002  0.250144           0
477   -0.000050   -0.000495    0.000132   -0.000157  ...          0.0   6031.001  0.336875           0
478   -0.000346   -0.000312    0.000044   -0.000039  ...          0.0   4011.001  0.333666           0

[479 rows x 6004 columns]

Test accuracy: 0.91
Precision: 0.00, Recall: 0.00, F1-score: 0.00

Testing hyperparameters logged for tensorboard at C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1\test

---------------------------------------------------------------------------

<<<<<<<<<<<< CONFUSION MATRIX >>>>>>>>>>>>

> Creating confusion matrix for [ammf(acc+E1)]-1SVM_fdet_1 / test...

Confusion matrix logged at C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1\test


<<<<<<<<<<<< ANOMALY SCORE DISTRIBUTION (PRED_LABEL) >>>>>>>>>>>>

Creating anomaly score distribution plot for [ammf(acc+E1)]-1SVM_fdet_1 / test...

Total OK samples (from pred_label) : 435
---------- Samples in OK bins ----------
- **Bin 13 (-0.00834, 0.00714)**: 187 (3,033.001), 429 (2,048.001)
- **Bin 14 (0.00714, 0.02261)**: 71 (5,020.003), 100 (8,046.001), 104 (6,007.003), 181 (1,017.004), 196 (8,011.001), 199 (2,032.001), 410 (10,023.002), 415 (11,005.001)
- **Bin 15 (0.02261, 0.03809)**: 247 (8,048.001), 273 (11,020.003), 450 (5,032.002)
- **Bin 16 (0.03809, 0.05357)**: 322 (6,048.001), 353 (3,005.001), 388 (12,033.001)
- **Bin 17 (0.05357, 0.06905)**: 158 (12,034.003), 378 (7,046.001), 401 (9,007.004), 469 (7,023.002)
- **Bin 18 (0.06905, 0.08453)**: 58 (3,011.002), 175 (8,032.003), 288 (9,034.003), 362 (10,033.003), 439 (8,032.004)
- **Bin 19 (0.08453, 0.10000)**: 15 (11,020.004), 108 (9,016.001), 249 (2,032.004), 316 (9,033.001), 321 (11,046.004), 364 (5,016.002), 409 (8,003.001), 465 (8,033.004)
- **Bin 20 (0.10000, 0.11548)**: 6 (7,033.004), 60 (8,005.003), 106 (6,034.003), 299 (3,016.003), 351 (9,032.001), 357 (2,023.002), 413 (4,023.001), 453 (7,023.004)
- **Bin 21 (0.11548, 0.13096)**: 39 (5,041.004), 306 (10,046.001), 319 (10,003.003), 326 (4,033.002), 473 (3,046.003)
- **Bin 22 (0.13096, 0.14644)**: 25 (5,019.003), 275 (2,033.003), 394 (7,028.002), 400 (8,003.002), 464 (4,048.003)
- **Bin 23 (0.14644, 0.16191)**: 33 (10,012.002), 98 (6,044.001), 164 (11,041.004), 177 (4,044.004), 193 (12,028.003), 267 (1,003.002), 277 (7,029.003), 279 (11,023.003), 289 (5,029.002), 293 (3,016.002)
- **Bin 24 (0.16191, 0.17739)**: 13 (10,028.001), 34 (1,034.003), 99 (10,019.002), 121 (11,044.004), 131 (3,022.004), 136 (6,003.004), 162 (11,034.004), 208 (7,028.003), 225 (4,040.003), 227 (10,040.001), 235 (9,028.001), 292 (2,016.002), 341 (8,029.003), 366 (5,038.003), 380 (8,023.001), 422 (8,038.003), 470 (7,034.002), 471 (3,011.001)
- **Bin 25 (0.17739, 0.19287)**: 56 (5,023.002), 75 (8,031.002), 84 (11,019.004), 92 (10,044.002), 123 (12,034.002), 135 (12,046.004), 152 (4,042.002), 182 (11,017.002), 231 (9,034.001), 250 (5,019.004), 252 (1,046.002), 256 (12,019.003), 274 (12,040.001), 298 (11,023.001), 338 (4,046.004), 436 (11,044.003), 445 (5,044.003), 447 (2,042.003), 462 (8,046.003)
- **Bin 26 (0.19287, 0.20835)**: 29 (12,044.003), 53 (6,046.002), 74 (3,003.002), 77 (4,046.003), 96 (5,022.003), 141 (4,029.003), 198 (4,017.003), 202 (10,044.003), 271 (12,008.003), 278 (6,023.003), 407 (8,018.002), 442 (8,042.002), 443 (6,029.001), 451 (12,002.001), 455 (8,029.004), 466 (7,044.001)
- **Bin 27 (0.20835, 0.22383)**: 11 (2,029.001), 12 (12,010.003), 40 (3,048.004), 55 (7,028.004), 80 (2,018.002), 95 (2,023.001), 127 (9,038.001), 143 (5,018.001), 156 (11,018.002), 165 (5,011.002), 166 (4,034.001), 220 (11,019.001), 233 (1,026.003), 240 (12,038.004), 245 (11,004.001), 266 (8,028.003), 325 (1,004.003), 373 (9,038.004), 432 (9,034.004), 460 (3,008.004)
- **Bin 28 (0.22383, 0.23930)**: 4 (3,046.002), 30 (2,034.003), 48 (6,044.003), 73 (3,031.002), 85 (12,012.002), 126 (5,018.002), 137 (1,018.003), 161 (2,040.003), 172 (7,038.002), 214 (12,021.001), 224 (2,008.002), 301 (7,036.001), 317 (7,048.002), 333 (10,028.004), 346 (10,022.004), 404 (2,011.002), 427 (11,034.001), 457 (8,012.001)
- **Bin 29 (0.23930, 0.25478)**: 0 (10,038.003), 19 (11,034.003), 24 (10,018.003), 79 (2,036.002), 174 (4,036.002), 180 (4,016.001), 212 (6,022.004), 221 (8,019.003), 236 (10,022.003), 243 (3,021.001), 291 (7,026.003), 340 (8,017.003), 381 (8,001.002), 454 (4,018.003), 476 (9,010.002)
- **Bin 30 (0.25478, 0.27026)**: 21 (7,022.003), 35 (7,001.001), 49 (9,023.003), 102 (8,031.003), 109 (7,036.002), 146 (8,034.002), 290 (3,001.003), 305 (7,050.004), 331 (5,031.004), 343 (6,012.003), 379 (5,042.004), 397 (8,010.001), 416 (12,027.001), 421 (12,023.003), 472 (2,050.001)
- **Bin 31 (0.27026, 0.28574)**: 1 (11,050.001), 65 (9,021.002), 81 (3,041.001), 155 (7,027.002), 201 (10,042.002), 268 (8,021.002), 284 (4,021.004), 355 (6,041.003)
- **Bin 32 (0.28574, 0.30121)**: 14 (7,019.003), 32 (3,041.003), 57 (10,042.001), 59 (9,027.001), 124 (10,048.001), 178 (10,008.003), 191 (10,038.001), 204 (10,010.002), 234 (11,050.002), 238 (4,036.001), 281 (10,042.003), 318 (11,036.003), 385 (5,001.002), 398 (11,045.002), 440 (12,041.001), 452 (11,011.004), 459 (7,038.003), 461 (7,045.003)
- **Bin 33 (0.30121, 0.31669)**: 67 (11,021.002), 89 (5,042.001), 97 (1,044.001), 144 (3,004.002), 149 (10,001.001), 150 (5,027.004), 151 (2,014.001), 186 (9,050.002), 239 (1,014.003), 248 (3,016.004), 282 (6,004.003), 342 (5,001.003), 354 (4,008.003), 356 (2,001.004), 384 (11,027.001), 391 (7,011.001), 395 (5,037.002), 399 (9,005.004)
- **Bin 34 (0.31669, 0.33217)**: 8 (2,008.001), 61 (3,026.004), 82 (4,040.001), 119 (6,026.002), 188 (6,027.003), 209 (8,012.002), 210 (7,008.001), 211 (10,041.004), 260 (4,008.001), 297 (11,049.003), 304 (1,002.002), 309 (9,026.003), 360 (7,050.003), 402 (7,045.001), 412 (6,037.002), 425 (8,036.003), 435 (3,018.004), 463 (4,014.002), 468 (8,036.004)
- **Bin 35 (0.33217, 0.34765)**: 3 (8,040.003), 18 (9,001.002), 23 (8,012.004), 37 (5,036.003), 43 (5,022.002), 113 (11,045.003), 116 (6,008.002), 138 (5,037.001), 140 (10,004.001), 183 (2,049.003), 192 (10,021.003), 200 (7,027.004), 232 (4,045.002), 241 (2,010.004), 244 (10,011.001), 254 (6,010.001), 264 (7,012.001), 283 (11,049.001), 310 (1,041.003), 329 (7,024.003), 334 (6,037.003), 336 (2,027.003), 344 (2,027.002), 345 (11,027.003), 372 (6,019.004), 383 (5,026.004), 403 (9,050.001), 456 (11,002.004), 477 (6,031.001), 478 (4,011.001)
- **Bin 36 (0.34765, 0.36312)**: 27 (2,014.004), 31 (9,004.003), 41 (4,021.002), 62 (8,045.004), 87 (1,040.001), 110 (8,014.003), 130 (1,042.001), 142 (3,014.002), 168 (5,050.004), 170 (4,024.004), 171 (8,037.001), 215 (12,049.002), 223 (10,024.003), 237 (4,041.001), 246 (1,050.002), 270 (12,026.004), 315 (12,027.003), 332 (4,011.003), 377 (5,037.003), 387 (1,014.004), 390 (7,047.002), 411 (7,041.001), 418 (3,031.003), 419 (1,011.003), 434 (1,027.004)
- **Bin 37 (0.36312, 0.37860)**: 45 (3,013.004), 51 (8,030.004), 54 (3,021.003), 93 (12,024.003), 115 (9,045.003), 132 (2,022.001), 134 (10,042.004), 179 (5,026.002), 195 (6,026.004), 197 (10,045.004), 206 (9,017.001), 217 (5,009.001), 219 (5,008.004), 261 (6,027.002), 280 (4,041.002), 302 (8,009.004), 330 (6,022.002), 339 (5,009.003), 347 (4,024.002), 350 (11,010.003), 363 (3,042.002), 365 (6,039.001), 371 (4,039.002), 406 (10,024.001)
- **Bin 38 (0.37860, 0.39408)**: 5 (2,019.001), 28 (3,037.001), 83 (12,045.003), 157 (11,037.001), 163 (12,036.003), 251 (8,045.001), 307 (9,024.001), 324 (10,049.003), 375 (6,002.002), 376 (5,030.004), 423 (3,049.003), 437 (10,024.002), 475 (8,037.004)
- **Bin 39 (0.39408, 0.40956)**: 44 (11,002.002), 139 (8,049.004), 160 (4,049.002), 190 (8,009.003), 335 (2,039.002), 444 (1,024.002)
- **Bin 40 (0.40956, 0.42504)**: 2 (10,013.001), 7 (10,009.003), 47 (12,015.002), 72 (9,027.004), 90 (1,024.004), 107 (2,009.003), 117 (7,002.003), 125 (5,024.001), 128 (5,014.001), 153 (7,025.003), 184 (9,013.001), 295 (9,049.003), 296 (4,049.001), 308 (1,024.003), 368 (1,029.001), 393 (1,039.001), 426 (10,002.002), 430 (10,009.002), 431 (2,030.001), 448 (6,024.001), 449 (5,024.002), 467 (6,036.002)
- **Bin 41 (0.42504, 0.44051)**: 10 (7,049.003), 20 (3,030.004), 26 (2,009.002), 38 (6,025.002), 46 (4,022.001), 52 (7,013.002), 76 (12,024.001), 88 (8,015.002), 103 (7,017.002), 105 (1,022.001), 111 (10,022.001), 169 (7,013.004), 205 (8,039.001), 216 (8,024.004), 218 (1,031.001), 228 (12,013.001), 242 (9,047.002), 285 (5,025.001), 294 (11,015.001), 348 (3,025.002), 389 (2,013.003), 396 (2,047.004)
- **Bin 42 (0.44051, 0.45599)**: 36 (12,039.004), 50 (10,030.002), 69 (10,025.004), 78 (3,013.002), 94 (12,030.002), 118 (1,030.003), 148 (11,009.001), 194 (1,013.004), 303 (6,013.004), 311 (6,047.002), 320 (3,013.001), 327 (2,047.001), 370 (12,047.003), 374 (9,030.002), 382 (1,009.003), 405 (11,030.001), 438 (8,024.001)
- **Bin 43 (0.45599, 0.47147)**: 9 (11,015.004), 112 (2,013.001), 263 (9,047.003), 269 (8,013.003), 414 (1,025.001)
- **Bin 44 (0.47147, 0.48695)**: 66 (6,025.004), 91 (11,047.001), 101 (4,015.004), 314 (6,049.004), 424 (6,009.004)
- **Bin 45 (0.48695, 0.50242)**: 86 (2,013.004), 120 (1,030.004), 159 (5,025.002), 262 (8,035.002), 300 (12,025.004), 433 (5,015.002)
- **Bin 46 (0.50242, 0.51790)**: 185 (5,015.004), 258 (9,030.003), 408 (12,047.004), 417 (11,047.002)
- **Bin 47 (0.51790, 0.53338)**: 68 (11,035.002), 129 (12,049.004), 313 (9,030.004), 358 (2,015.001), 361 (5,047.002)
- **Bin 48 (0.53338, 0.54886)**: 70 (1,035.004), 207 (6,035.004), 428 (8,047.003)
- **Bin 49 (0.54886, 0.56434)**: 167 (12,024.004), 474 (6,047.004)

Total NOK samples (from pred_label) : 44
---------- Samples in NOK bins ----------
- **Bin 0 (-0.20955, -0.19407)**: 16 (12,043.001), 42 (8,043.004), 259 (4,043.004), 337 (12,006.001), 369 (12,006.003)
- **Bin 1 (-0.19407, -0.17860)**: 64 (11,006.004), 176 (2,006.003), 222 (1,043.003), 255 (10,006.004), 265 (5,006.003), 272 (10,043.004), 392 (11,043.003)
- **Bin 2 (-0.17860, -0.16312)**: 17 (10,033.004), 328 (12,006.002), 420 (3,043.002)
- **Bin 3 (-0.16312, -0.14764)**: 122 (10,043.003), 147 (11,043.001), 312 (5,043.004)
- **Bin 5 (-0.13216, -0.11669)**: 213 (5,043.002), 276 (4,020.001)
- **Bin 6 (-0.11669, -0.10121)**: 189 (2,007.001), 230 (12,007.001), 323 (2,007.002)
- **Bin 8 (-0.08573, -0.07025)**: 229 (9,007.003), 458 (4,020.002)
- **Bin 9 (-0.07025, -0.05477)**: 367 (10,033.001)
- **Bin 10 (-0.05477, -0.03930)**: 22 (3,007.002), 63 (3,020.003), 114 (10,020.002), 203 (5,005.004)
- **Bin 11 (-0.03930, -0.02382)**: 133 (10,005.002), 173 (8,048.004), 287 (4,007.003), 352 (6,043.004), 359 (8,020.004), 446 (8,007.002)
- **Bin 12 (-0.02382, -0.00834)**: 145 (1,020.004), 154 (3,020.002), 253 (12,005.002), 257 (8,032.001), 286 (7,032.001), 349 (8,007.003), 386 (8,032.002)
- **Bin 13 (-0.00834, 0.00714)**: 441 (11,048.004)


Anomaly score distribution plot logged at C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1\test


<<<<<<<<<<<< ANOMALY SCORE DISTRIBUTION (GIVEN_LABEL) >>>>>>>>>>>>

Creating anomaly score distribution plot for [ammf(acc+E1)]-1SVM_fdet_1 / test...

Total OK samples (from given_label) : 479
---------- Samples in OK bins ----------
- **Bin 0 (-0.20955, -0.19407)**: 16 (12,043.001), 42 (8,043.004), 259 (4,043.004), 337 (12,006.001), 369 (12,006.003)
- **Bin 1 (-0.19407, -0.17860)**: 64 (11,006.004), 176 (2,006.003), 222 (1,043.003), 255 (10,006.004), 265 (5,006.003), 272 (10,043.004), 392 (11,043.003)
- **Bin 2 (-0.17860, -0.16312)**: 17 (10,033.004), 328 (12,006.002), 420 (3,043.002)
- **Bin 3 (-0.16312, -0.14764)**: 122 (10,043.003), 147 (11,043.001), 312 (5,043.004)
- **Bin 5 (-0.13216, -0.11669)**: 213 (5,043.002), 276 (4,020.001)
- **Bin 6 (-0.11669, -0.10121)**: 189 (2,007.001), 230 (12,007.001), 323 (2,007.002)
- **Bin 8 (-0.08573, -0.07025)**: 229 (9,007.003), 458 (4,020.002)
- **Bin 9 (-0.07025, -0.05477)**: 367 (10,033.001)
- **Bin 10 (-0.05477, -0.03930)**: 22 (3,007.002), 63 (3,020.003), 114 (10,020.002), 203 (5,005.004)
- **Bin 11 (-0.03930, -0.02382)**: 133 (10,005.002), 173 (8,048.004), 287 (4,007.003), 352 (6,043.004), 359 (8,020.004), 446 (8,007.002)
- **Bin 12 (-0.02382, -0.00834)**: 145 (1,020.004), 154 (3,020.002), 253 (12,005.002), 257 (8,032.001), 286 (7,032.001), 349 (8,007.003), 386 (8,032.002)
- **Bin 13 (-0.00834, 0.00714)**: 187 (3,033.001), 429 (2,048.001), 441 (11,048.004)
- **Bin 14 (0.00714, 0.02261)**: 71 (5,020.003), 100 (8,046.001), 104 (6,007.003), 181 (1,017.004), 196 (8,011.001), 199 (2,032.001), 410 (10,023.002), 415 (11,005.001)
- **Bin 15 (0.02261, 0.03809)**: 247 (8,048.001), 273 (11,020.003), 450 (5,032.002)
- **Bin 16 (0.03809, 0.05357)**: 322 (6,048.001), 353 (3,005.001), 388 (12,033.001)
- **Bin 17 (0.05357, 0.06905)**: 158 (12,034.003), 378 (7,046.001), 401 (9,007.004), 469 (7,023.002)
- **Bin 18 (0.06905, 0.08453)**: 58 (3,011.002), 175 (8,032.003), 288 (9,034.003), 362 (10,033.003), 439 (8,032.004)
- **Bin 19 (0.08453, 0.10000)**: 15 (11,020.004), 108 (9,016.001), 249 (2,032.004), 316 (9,033.001), 321 (11,046.004), 364 (5,016.002), 409 (8,003.001), 465 (8,033.004)
- **Bin 20 (0.10000, 0.11548)**: 6 (7,033.004), 60 (8,005.003), 106 (6,034.003), 299 (3,016.003), 351 (9,032.001), 357 (2,023.002), 413 (4,023.001), 453 (7,023.004)
- **Bin 21 (0.11548, 0.13096)**: 39 (5,041.004), 306 (10,046.001), 319 (10,003.003), 326 (4,033.002), 473 (3,046.003)
- **Bin 22 (0.13096, 0.14644)**: 25 (5,019.003), 275 (2,033.003), 394 (7,028.002), 400 (8,003.002), 464 (4,048.003)
- **Bin 23 (0.14644, 0.16191)**: 33 (10,012.002), 98 (6,044.001), 164 (11,041.004), 177 (4,044.004), 193 (12,028.003), 267 (1,003.002), 277 (7,029.003), 279 (11,023.003), 289 (5,029.002), 293 (3,016.002)
- **Bin 24 (0.16191, 0.17739)**: 13 (10,028.001), 34 (1,034.003), 99 (10,019.002), 121 (11,044.004), 131 (3,022.004), 136 (6,003.004), 162 (11,034.004), 208 (7,028.003), 225 (4,040.003), 227 (10,040.001), 235 (9,028.001), 292 (2,016.002), 341 (8,029.003), 366 (5,038.003), 380 (8,023.001), 422 (8,038.003), 470 (7,034.002), 471 (3,011.001)
- **Bin 25 (0.17739, 0.19287)**: 56 (5,023.002), 75 (8,031.002), 84 (11,019.004), 92 (10,044.002), 123 (12,034.002), 135 (12,046.004), 152 (4,042.002), 182 (11,017.002), 231 (9,034.001), 250 (5,019.004), 252 (1,046.002), 256 (12,019.003), 274 (12,040.001), 298 (11,023.001), 338 (4,046.004), 436 (11,044.003), 445 (5,044.003), 447 (2,042.003), 462 (8,046.003)
- **Bin 26 (0.19287, 0.20835)**: 29 (12,044.003), 53 (6,046.002), 74 (3,003.002), 77 (4,046.003), 96 (5,022.003), 141 (4,029.003), 198 (4,017.003), 202 (10,044.003), 271 (12,008.003), 278 (6,023.003), 407 (8,018.002), 442 (8,042.002), 443 (6,029.001), 451 (12,002.001), 455 (8,029.004), 466 (7,044.001)
- **Bin 27 (0.20835, 0.22383)**: 11 (2,029.001), 12 (12,010.003), 40 (3,048.004), 55 (7,028.004), 80 (2,018.002), 95 (2,023.001), 127 (9,038.001), 143 (5,018.001), 156 (11,018.002), 165 (5,011.002), 166 (4,034.001), 220 (11,019.001), 233 (1,026.003), 240 (12,038.004), 245 (11,004.001), 266 (8,028.003), 325 (1,004.003), 373 (9,038.004), 432 (9,034.004), 460 (3,008.004)
- **Bin 28 (0.22383, 0.23930)**: 4 (3,046.002), 30 (2,034.003), 48 (6,044.003), 73 (3,031.002), 85 (12,012.002), 126 (5,018.002), 137 (1,018.003), 161 (2,040.003), 172 (7,038.002), 214 (12,021.001), 224 (2,008.002), 301 (7,036.001), 317 (7,048.002), 333 (10,028.004), 346 (10,022.004), 404 (2,011.002), 427 (11,034.001), 457 (8,012.001)
- **Bin 29 (0.23930, 0.25478)**: 0 (10,038.003), 19 (11,034.003), 24 (10,018.003), 79 (2,036.002), 174 (4,036.002), 180 (4,016.001), 212 (6,022.004), 221 (8,019.003), 236 (10,022.003), 243 (3,021.001), 291 (7,026.003), 340 (8,017.003), 381 (8,001.002), 454 (4,018.003), 476 (9,010.002)
- **Bin 30 (0.25478, 0.27026)**: 21 (7,022.003), 35 (7,001.001), 49 (9,023.003), 102 (8,031.003), 109 (7,036.002), 146 (8,034.002), 290 (3,001.003), 305 (7,050.004), 331 (5,031.004), 343 (6,012.003), 379 (5,042.004), 397 (8,010.001), 416 (12,027.001), 421 (12,023.003), 472 (2,050.001)
- **Bin 31 (0.27026, 0.28574)**: 1 (11,050.001), 65 (9,021.002), 81 (3,041.001), 155 (7,027.002), 201 (10,042.002), 268 (8,021.002), 284 (4,021.004), 355 (6,041.003)
- **Bin 32 (0.28574, 0.30121)**: 14 (7,019.003), 32 (3,041.003), 57 (10,042.001), 59 (9,027.001), 124 (10,048.001), 178 (10,008.003), 191 (10,038.001), 204 (10,010.002), 234 (11,050.002), 238 (4,036.001), 281 (10,042.003), 318 (11,036.003), 385 (5,001.002), 398 (11,045.002), 440 (12,041.001), 452 (11,011.004), 459 (7,038.003), 461 (7,045.003)
- **Bin 33 (0.30121, 0.31669)**: 67 (11,021.002), 89 (5,042.001), 97 (1,044.001), 144 (3,004.002), 149 (10,001.001), 150 (5,027.004), 151 (2,014.001), 186 (9,050.002), 239 (1,014.003), 248 (3,016.004), 282 (6,004.003), 342 (5,001.003), 354 (4,008.003), 356 (2,001.004), 384 (11,027.001), 391 (7,011.001), 395 (5,037.002), 399 (9,005.004)
- **Bin 34 (0.31669, 0.33217)**: 8 (2,008.001), 61 (3,026.004), 82 (4,040.001), 119 (6,026.002), 188 (6,027.003), 209 (8,012.002), 210 (7,008.001), 211 (10,041.004), 260 (4,008.001), 297 (11,049.003), 304 (1,002.002), 309 (9,026.003), 360 (7,050.003), 402 (7,045.001), 412 (6,037.002), 425 (8,036.003), 435 (3,018.004), 463 (4,014.002), 468 (8,036.004)
- **Bin 35 (0.33217, 0.34765)**: 3 (8,040.003), 18 (9,001.002), 23 (8,012.004), 37 (5,036.003), 43 (5,022.002), 113 (11,045.003), 116 (6,008.002), 138 (5,037.001), 140 (10,004.001), 183 (2,049.003), 192 (10,021.003), 200 (7,027.004), 232 (4,045.002), 241 (2,010.004), 244 (10,011.001), 254 (6,010.001), 264 (7,012.001), 283 (11,049.001), 310 (1,041.003), 329 (7,024.003), 334 (6,037.003), 336 (2,027.003), 344 (2,027.002), 345 (11,027.003), 372 (6,019.004), 383 (5,026.004), 403 (9,050.001), 456 (11,002.004), 477 (6,031.001), 478 (4,011.001)
- **Bin 36 (0.34765, 0.36312)**: 27 (2,014.004), 31 (9,004.003), 41 (4,021.002), 62 (8,045.004), 87 (1,040.001), 110 (8,014.003), 130 (1,042.001), 142 (3,014.002), 168 (5,050.004), 170 (4,024.004), 171 (8,037.001), 215 (12,049.002), 223 (10,024.003), 237 (4,041.001), 246 (1,050.002), 270 (12,026.004), 315 (12,027.003), 332 (4,011.003), 377 (5,037.003), 387 (1,014.004), 390 (7,047.002), 411 (7,041.001), 418 (3,031.003), 419 (1,011.003), 434 (1,027.004)
- **Bin 37 (0.36312, 0.37860)**: 45 (3,013.004), 51 (8,030.004), 54 (3,021.003), 93 (12,024.003), 115 (9,045.003), 132 (2,022.001), 134 (10,042.004), 179 (5,026.002), 195 (6,026.004), 197 (10,045.004), 206 (9,017.001), 217 (5,009.001), 219 (5,008.004), 261 (6,027.002), 280 (4,041.002), 302 (8,009.004), 330 (6,022.002), 339 (5,009.003), 347 (4,024.002), 350 (11,010.003), 363 (3,042.002), 365 (6,039.001), 371 (4,039.002), 406 (10,024.001)
- **Bin 38 (0.37860, 0.39408)**: 5 (2,019.001), 28 (3,037.001), 83 (12,045.003), 157 (11,037.001), 163 (12,036.003), 251 (8,045.001), 307 (9,024.001), 324 (10,049.003), 375 (6,002.002), 376 (5,030.004), 423 (3,049.003), 437 (10,024.002), 475 (8,037.004)
- **Bin 39 (0.39408, 0.40956)**: 44 (11,002.002), 139 (8,049.004), 160 (4,049.002), 190 (8,009.003), 335 (2,039.002), 444 (1,024.002)
- **Bin 40 (0.40956, 0.42504)**: 2 (10,013.001), 7 (10,009.003), 47 (12,015.002), 72 (9,027.004), 90 (1,024.004), 107 (2,009.003), 117 (7,002.003), 125 (5,024.001), 128 (5,014.001), 153 (7,025.003), 184 (9,013.001), 295 (9,049.003), 296 (4,049.001), 308 (1,024.003), 368 (1,029.001), 393 (1,039.001), 426 (10,002.002), 430 (10,009.002), 431 (2,030.001), 448 (6,024.001), 449 (5,024.002), 467 (6,036.002)
- **Bin 41 (0.42504, 0.44051)**: 10 (7,049.003), 20 (3,030.004), 26 (2,009.002), 38 (6,025.002), 46 (4,022.001), 52 (7,013.002), 76 (12,024.001), 88 (8,015.002), 103 (7,017.002), 105 (1,022.001), 111 (10,022.001), 169 (7,013.004), 205 (8,039.001), 216 (8,024.004), 218 (1,031.001), 228 (12,013.001), 242 (9,047.002), 285 (5,025.001), 294 (11,015.001), 348 (3,025.002), 389 (2,013.003), 396 (2,047.004)
- **Bin 42 (0.44051, 0.45599)**: 36 (12,039.004), 50 (10,030.002), 69 (10,025.004), 78 (3,013.002), 94 (12,030.002), 118 (1,030.003), 148 (11,009.001), 194 (1,013.004), 303 (6,013.004), 311 (6,047.002), 320 (3,013.001), 327 (2,047.001), 370 (12,047.003), 374 (9,030.002), 382 (1,009.003), 405 (11,030.001), 438 (8,024.001)
- **Bin 43 (0.45599, 0.47147)**: 9 (11,015.004), 112 (2,013.001), 263 (9,047.003), 269 (8,013.003), 414 (1,025.001)
- **Bin 44 (0.47147, 0.48695)**: 66 (6,025.004), 91 (11,047.001), 101 (4,015.004), 314 (6,049.004), 424 (6,009.004)
- **Bin 45 (0.48695, 0.50242)**: 86 (2,013.004), 120 (1,030.004), 159 (5,025.002), 262 (8,035.002), 300 (12,025.004), 433 (5,015.002)
- **Bin 46 (0.50242, 0.51790)**: 185 (5,015.004), 258 (9,030.003), 408 (12,047.004), 417 (11,047.002)
- **Bin 47 (0.51790, 0.53338)**: 68 (11,035.002), 129 (12,049.004), 313 (9,030.004), 358 (2,015.001), 361 (5,047.002)
- **Bin 48 (0.53338, 0.54886)**: 70 (1,035.004), 207 (6,035.004), 428 (8,047.003)
- **Bin 49 (0.54886, 0.56434)**: 167 (12,024.004), 474 (6,047.004)

Total NOK samples (from given_label) : 0
---------- Samples in NOK bins ----------


Anomaly score distribution plot logged at C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1\test


<<<<<<<<<<<< PAIR PLOT >>>>>>>>>>>>

> Creating pair plot for [ammf(acc+E1)]-1SVM_fdet_1 / test...

Pair plot logged at C:\AFD\fault_detection\logs\asml\NXE\full_wafer\train\ammf\acc\set_E1\1SVM\tswp_1\[ammf(acc+E1)]-1SVM_fdet_1\test


===========================================================================

Fault detection model '[ammf(acc+E1)]-1SVM_fdet_1' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-01 20:13:30
