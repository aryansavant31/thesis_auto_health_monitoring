{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9906d9cd",
   "metadata": {},
   "source": [
    "# Topology Estimation Model\n",
    "\n",
    "This notebook contains all the steps of topology model developement. \n",
    "\n",
    "In this notebook, the model can be:\n",
    "- trained and tested\n",
    "- loaded and run for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a89842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097a33d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run from main script...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting run from main script...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbe905",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7f0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Train' type dataset selected:\n",
      "\n",
      "\n",
      "ds_subtype selections:\n",
      "\n",
      "(<ds_subtype_num>) <ds_subtype> : [<augments>]\n",
      "---------------------------------------------\n",
      ">> Healthy configs\n",
      "(1) series_tp_(fs=1000)    : [OG]\n",
      "\n",
      ">> Unhealthy configs\n",
      "\n",
      ">> Unknown configs\n",
      "\n",
      "\n",
      "Node and signal types are set as follows:\n",
      "\n",
      "(<node_num>) <node_type> : [<signal_types>]\n",
      "---------------------------------------------\n",
      "(1) node_group_name   : [m, 0, 0, 4]\n",
      "(2) group   : [mass_1, mass_2, mass_3, mass_4]\n",
      "\n",
      "\n",
      "For ds_type 'OK' and others....\n",
      "\n",
      "Maximum timesteps across all node types: 100001\n",
      "\n",
      "'fs' is updated in data_config as given in loaded healthy (or unknown) data.\n",
      "New fs:\n",
      "[[1000., 1000., 1000.],\n",
      " [1000., 1000., 1000.],\n",
      " [1000., 1000., 1000.],\n",
      " [1000., 1000., 1000.]]\n",
      "\n",
      "No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.\n",
      "\n",
      "\n",
      "[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]\n",
      "---------------------------------------------\n",
      "Total samples: 1000 \n",
      "Train: 800/800 [OK=800, NOK=0, UK=0], Test: 100/100 [OK=100, NOK=0, UK=0], Val: 100/100 [OK=100, NOK=0, UK=0],\n",
      "Remainder: 0 [OK=0, NOK=0, UK=0]\n",
      "\n",
      "train_data_loader statistics:\n",
      "Number of batches: 16\n",
      "torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)\n",
      "\n",
      "test_data_loader statistics:\n",
      "Number of batches: 2\n",
      "torch.Size([50, 4, 100, 3]) \n",
      "\n",
      "val_data_loader statistics:\n",
      "Number of batches: 2\n",
      "torch.Size([50, 4, 100, 3]) \n",
      "\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from data.prep import DataPreprocessor\n",
    "from data.config import DataConfig\n",
    "\n",
    "data_config = DataConfig(run_type='train')\n",
    "data_preprocessor = DataPreprocessor(package='topology_estimation')\n",
    "\n",
    "# load datalaoders\n",
    "train_package, test_package, val_package = data_preprocessor.get_training_data_package(data_config, train_rt=0.8, test_rt=0.1, val_rt=0.1, batch_size=50)\n",
    "\n",
    "train_loader, train_data_stats = train_package\n",
    "test_loader, test_data_stats = test_package\n",
    "val_loader, val_data_stats = val_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df95956",
   "metadata": {},
   "source": [
    "#### Set the number of timesteps and dimensions of the node data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8343f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4\n",
      "Number of timesteps: 100\n",
      "Number of dimensions: 3\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = next(dataiter)\n",
    "\n",
    "n_nodes = data[0].shape[1]\n",
    "n_timesteps = data[0].shape[2]\n",
    "n_dims = data[0].shape[3]\n",
    "\n",
    "print(f\"Number of nodes: {n_nodes}\")\n",
    "print(f\"Number of timesteps: {n_timesteps}\")  \n",
    "print(f\"Number of dimensions: {n_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b540dc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93d1b",
   "metadata": {},
   "source": [
    "### Prepare the relation matrix for encoder input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0fcf9",
   "metadata": {},
   "source": [
    "##### Generate off-diagonal fully connected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f82490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Relation Matrices...\n",
      "\n",
      "Reciever relation matrix:\n",
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]]) \n",
      "shape: torch.Size([12, 4])\n",
      "\n",
      "Sender relation matrix:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]]) \n",
      "shape: torch.Size([12, 4])\n",
      "\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from settings.manager import NRITrainManager\n",
    "from topology_estimation.relations import RelationMatrixMaker\n",
    "\n",
    "nri_config = NRITrainManager(data_config)\n",
    "\n",
    "rm = RelationMatrixMaker(nri_config.spf_config)\n",
    "rec_rel, send_rel = rm.get_relation_matrix(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f6f7e",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the topology estimator model blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc088ce8",
   "metadata": {},
   "source": [
    "### Prepare params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd5cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from torchinfo import summary\n",
    "import inspect\n",
    "\n",
    "def get_encoder_params(tp_config, train_loader):\n",
    "    \"\"\"\n",
    "    Get the encoder parameters required for initializing the NRI model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    enc_model_params : dict\n",
    "        Dictionary containing the encoder model parameters.\n",
    "    enc_run_params : dict\n",
    "        Dictionary containing the encoder run parameters.\n",
    "    \"\"\"\n",
    "    # encoder params\n",
    "    req_enc_model_params = inspect.signature(Encoder.__init__).parameters.keys()\n",
    "    req_enc_run_params = inspect.signature(Encoder.set_run_params).parameters.keys()\n",
    "\n",
    "    enc_model_params = {\n",
    "        key.removesuffix('_enc'): value for key, value in tp_config.__dict__.items() if key.removesuffix('_enc') in req_enc_model_params and key not in ['hyperparams']\n",
    "    }\n",
    "    enc_run_params = {\n",
    "        key.removeprefix('enc_'): value for key, value in tp_config.__dict__.items() if key.removeprefix('enc_') in req_enc_run_params and key not in ['data_config']\n",
    "    }\n",
    "    # get n_comps and n_dims for encoder and decoder\n",
    "    none_dict = {param: None for param in inspect.signature(Encoder).parameters.keys()}\n",
    "    pre_enc = Encoder(**none_dict)\n",
    "    pre_enc.set_run_params(**enc_run_params)\n",
    "    n_comps, n_dims = pre_enc.process_input_data(next(iter(train_loader))[0], get_data_shape=True)\n",
    "\n",
    "    enc_model_params['n_comps'] = n_comps\n",
    "    enc_model_params['n_dims'] = n_dims\n",
    "\n",
    "    print(\"\\n<<<<< ENCODER PARAMETERS >>>>>\")\n",
    "    print(\"Encoder model parameters:\")\n",
    "    print(15 * \"-\")\n",
    "    for key, value in enc_model_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nEncoder run parameters:\")\n",
    "    print(15 * \"-\")\n",
    "    for key, value in enc_run_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print('\\n' + 35*'-')\n",
    "\n",
    "def get_decoder_params(tp_config, train_loader):\n",
    "    \"\"\"\n",
    "    Get the decoder parameters\n",
    "    \"\"\"\n",
    "    # decoder params\n",
    "    req_dec_model_params = [param for param in Decoder().__dict__.keys()] \n",
    "    req_dec_run_params = inspect.signature(Decoder.set_run_params).parameters.keys()\n",
    "\n",
    "    dec_model_params = {\n",
    "        key.removesuffix('_dec'): value for key, value in tp_config.__dict__.items() if key.removesuffix('_dec') in req_dec_model_params and key not in ['hyperparams']\n",
    "    }\n",
    "    dec_run_params = {\n",
    "        key.removeprefix('dec_'): value for key, value in tp_config.__dict__.items() if key.removeprefix('dec_') in req_dec_run_params and key not in ['data_config']\n",
    "    }\n",
    "    dec_model_params['n_dims'] = next(iter(train_loader))[0].shape[3]\n",
    "\n",
    "    print(\"\\n<<<<< DECODER PARAMETERS >>>>>\")\n",
    "    print(\"\\nDecoder model parameters:\")\n",
    "    print(15 * \"-\")\n",
    "    for key, value in dec_model_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nDecoder run parameters:\")\n",
    "    print(15 * \"-\")\n",
    "    for key, value in dec_run_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print('\\n' + 35*'-')\n",
    "\n",
    "    return dec_model_params, dec_run_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7f24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_model_params, enc_run_params = get_encoder_params()\n",
    "# dec_model_params, dec_run_params = get_decoder_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c45280",
   "metadata": {},
   "source": [
    "### Make NRI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a4dd464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nri import NRI\n",
    "\n",
    "# nri_model = NRI(enc_model_params, dec_model_params)\n",
    "# nri_model.set_run_params(enc_run_params, dec_run_params, nri_config.temp, nri_config.is_hard)\n",
    "\n",
    "\n",
    "# print(\"\\nNRI Model Summary:\")\n",
    "# # print(summary(nri_model, (train_loader.batch_size, n_nodes, n_timesteps, n_dims)))\n",
    "# print(nri_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3fa60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.custom_loader import CombinedDataLoader\n",
    "\n",
    "# train_loader_full = CombinedDataLoader(train_loader, rel_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38adb53",
   "metadata": {},
   "source": [
    "## 3. Training NRI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c38b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning import Trainer\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks import RichProgressBar\n",
    "# from utils.custom_loader import CombinedDataLoader\n",
    "\n",
    "# nri_model.set_training_params()\n",
    "\n",
    "# # get log path\n",
    "# train_log_path = nri_config.get_train_log_path(n_comps=enc_model_params['n_comps'], n_dim=dec_model_params['n_dims'])\n",
    "\n",
    "# if nri_config.is_log:\n",
    "#     nri_config.save_params()\n",
    "#     logger = TensorBoardLogger(os.path.dirname(train_log_path), name=\"\", version=os.path.basename(train_log_path))\n",
    "\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=nri_config.max_epochs,\n",
    "#     logger=logger,\n",
    "#     enable_progress_bar=True,\n",
    "#     log_every_n_steps=1,)\n",
    "\n",
    "# trainer.fit(model=nri_model, train_dataloaders=CombinedDataLoader(train_loader, rel_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa86d6",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e785c7",
   "metadata": {},
   "source": [
    "### Prep params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b36b12cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<<< DECODER PARAMETERS >>>>>\n",
      "\n",
      "Decoder model parameters:\n",
      "---------------\n",
      "n_edge_types: 1\n",
      "msg_out_size: 64\n",
      "edge_mlp_config: [[64, 'tanh'], [32, 'tanh'], [16, 'tanh'], [64, None]]\n",
      "out_mlp_config: [[64, 'tanh'], [32, 'tanh'], [16, 'tanh'], [64, None]]\n",
      "do_prob: 0\n",
      "is_batch_norm: True\n",
      "recur_emb_type: gru\n",
      "n_dims: 3\n",
      "\n",
      "Decoder run parameters:\n",
      "---------------\n",
      "domain_config: {'type': 'time', 'cutoff_freq': 0}\n",
      "raw_data_norm: None\n",
      "feat_configs: []\n",
      "reduc_config: None\n",
      "feat_norm: None\n",
      "skip_first_edge_type: False\n",
      "pred_steps: 1\n",
      "is_burn_in: False\n",
      "burn_in_steps: 1\n",
      "is_dynamic_graph: False\n",
      "temp: 1.0\n",
      "is_hard: True\n",
      "\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from settings.manager import DecoderTrainManager\n",
    "from decoder import Decoder\n",
    "from torchinfo import summary\n",
    "import inspect\n",
    "\n",
    "dec_config = DecoderTrainManager(data_config)\n",
    "dec_model_params, dec_run_params = get_decoder_params(dec_config, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5a665",
   "metadata": {},
   "source": [
    "### Make Deocder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a60fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Decoder Model Initialized with the following configurations:\n",
      "\n",
      "Decoder Model Summary:\n",
      "Decoder(\n",
      "  (edge_mlp_fn): ModuleList(\n",
      "    (0): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Tanh()\n",
      "        (3): Dropout(p=0, inplace=False)\n",
      "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): Tanh()\n",
      "        (7): Dropout(p=0, inplace=False)\n",
      "        (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (10): Tanh()\n",
      "        (11): Dropout(p=0, inplace=False)\n",
      "        (12): Linear(in_features=16, out_features=64, bias=True)\n",
      "        (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (recurrent_emb_fn): GRU(\n",
      "    (input_u): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (hidden_u): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (input_r): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (hidden_r): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (input_h): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (hidden_h): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (mean_mlp): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "      (3): Dropout(p=0, inplace=False)\n",
      "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Tanh()\n",
      "      (7): Dropout(p=0, inplace=False)\n",
      "      (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Tanh()\n",
      "      (11): Dropout(p=0, inplace=False)\n",
      "      (12): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (var_mlp): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "      (3): Dropout(p=0, inplace=False)\n",
      "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Tanh()\n",
      "      (7): Dropout(p=0, inplace=False)\n",
      "      (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Tanh()\n",
      "      (11): Dropout(p=0, inplace=False)\n",
      "      (12): Linear(in_features=16, out_features=64, bias=True)\n",
      "      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (mean_output_layer): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (var_output_layer): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from topology_estimation.decoder import Decoder\n",
    "\n",
    "# prep hyperparams\n",
    "dec_config.hyperparams.update({\n",
    "    'n_dims': str(dec_model_params['n_dims']),\n",
    "    'n_nodes': str(next(iter(train_loader))[0].shape[1])  \n",
    "    })\n",
    "\n",
    "decoder_model = Decoder()\n",
    "for key, value in dec_model_params.items():\n",
    "    setattr(decoder_model, key, value)\n",
    "\n",
    "decoder_model.hyperparams = dec_config.hyperparams\n",
    "decoder_model.build_model()\n",
    "\n",
    "decoder_model.set_input_graph(rec_rel, send_rel)\n",
    "\n",
    "edge_matrix = 0.5*(rec_rel + send_rel).sum(dim=-1, keepdim=True)\n",
    "edge_matrix = edge_matrix.unsqueeze(0).repeat(train_loader.batch_size, 1, 1)\n",
    "\n",
    "decoder_model.set_edge_matrix(edge_matrix)\n",
    "\n",
    "decoder_model.set_run_params(\n",
    "    **dec_run_params, \n",
    "    data_config=data_config, \n",
    "    data_stats=train_data_stats\n",
    "    )\n",
    "\n",
    "decoder_model.set_training_params(\n",
    "    lr=dec_config.lr, \n",
    "    optimizer=dec_config.optimizer,\n",
    "    loss_type=dec_config.loss_type\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + 75*'-')\n",
    "print(\"\\nDecoder Model Initialized with the following configurations:\")\n",
    "print(\"\\nDecoder Model Summary:\")\n",
    "print(decoder_model)\n",
    "# print(summary(decoder_model, (next(iter(train_loader))[0].shape), device=device))\n",
    "\n",
    "print('\\n' + 75*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ce75a",
   "metadata": {},
   "source": [
    "## Train Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a59cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'(m004)-(gru)_decoder_1.1' already exists in the log path 'c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\g=m004\\D=gru\\(m004)-(gru)_decoder_1.1'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type       | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | edge_mlp_fn       | ModuleList | 12.3 K | train\n",
      "1 | recurrent_emb_fn  | GRU        | 13.2 K | train\n",
      "2 | mean_mlp          | MLP        | 8.2 K  | train\n",
      "3 | var_mlp           | MLP        | 8.2 K  | train\n",
      "4 | mean_output_layer | Linear     | 195    | train\n",
      "5 | var_output_layer  | Linear     | 195    | train\n",
      "---------------------------------------------------------\n",
      "42.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.4 K    Total params\n",
      "0.169     Total estimated model params size (MB)\n",
      "52        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote '(m004)-(gru)_decoder_1.1' from the log path c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\g=m004\\D=gru\\(m004)-(gru)_decoder_1.1.\n",
      "Model parameters saved to c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\g=m004\\D=gru\\(m004)-(gru)_decoder_1.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\afd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\Anaconda3\\envs\\afd_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing input processors for decoder model...\n",
      "\n",
      ">> Domain transformer initialized for 'time' domain\n",
      "\n",
      ">> No raw data normalization is applied\n",
      "\n",
      ">> No feature normalization is applied\n",
      "\n",
      ">> No time feature extraction is applied\n",
      "\n",
      ">> No feature reduction is applied\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:11<00:00,  1.37it/s, v_num=_1.1, val_loss=173.0, train_loss=223.0]\n",
      "Epoch 1/5 completed\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:10<00:00,  1.47it/s, v_num=_1.1, val_loss=110.0, train_loss=164.0]\n",
      "Epoch 2/5 completed\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:10<00:00,  1.48it/s, v_num=_1.1, val_loss=45.20, train_loss=104.0]\n",
      "Epoch 3/5 completed\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:11<00:00,  1.44it/s, v_num=_1.1, val_loss=57.60, train_loss=41.10]\n",
      "Epoch 4/5 completed\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:11<00:00,  1.44it/s, v_num=_1.1, val_loss=-13.7, train_loss=-10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5 completed\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:11<00:00,  1.43it/s, v_num=_1.1, val_loss=-13.7, train_loss=-10.7]\n",
      "\n",
      "Training completed in 59.04 seconds or 0.98 minutes or 0.016399316522810195 hours.\n",
      "\n",
      "Training completed for model '(m004)-(gru)_decoder_1.1'. Trained model saved at c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\g=m004\\D=gru\\(m004)-(gru)_decoder_1.1\\checkpoints\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>\n",
      "\n",
      "Creating training loss plot for (m004)-(gru)_decoder_1.1...\n",
      "\n",
      "Training loss (train + val) plot logged at c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\g=m004\\D=gru\\(m004)-(gru)_decoder_1.1\n",
      "\n",
      "\n",
      "<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN)>>>>>>>>>>>>\n",
      "\n",
      "Creating decoder output plot for rep '1,001.378' for (m004)-(gru)_decoder_1.1...\n",
      "\n",
      "Decoder output plot for rep '1001.378' logged at c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\g=m004\\D=gru\\(m004)-(gru)_decoder_1.1\n",
      "\n",
      "\n",
      "<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL)>>>>>>>>>>>>\n",
      "\n",
      "Creating decoder output plot for rep '1,001.937' for (m004)-(gru)_decoder_1.1...\n",
      "\n",
      "Decoder output plot for rep '1001.937' logged at c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\g=m004\\D=gru\\(m004)-(gru)_decoder_1.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from utils.custom_loader import CombinedDataLoader\n",
    "\n",
    "decoder_model.set_training_params()\n",
    "\n",
    "# get log path\n",
    "train_log_path = dec_config.get_train_log_path(n_dim=dec_model_params['n_dims'])\n",
    "\n",
    "if dec_config.is_log:\n",
    "    dec_config.save_params()\n",
    "    logger = TensorBoardLogger(os.path.dirname(train_log_path), name=\"\", version=os.path.basename(train_log_path))\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=dec_config.max_epochs,\n",
    "    logger=logger,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    "    num_sanity_val_steps=0)\n",
    "\n",
    "trainer.fit(model=decoder_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb662657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".ckpt_files available in c:\\Aryan_Savant\\Thesis_Projects\\my_work\\AFD_thesis\\topology_estimation\\logs\\mass_sp_dm\\M004\\scene_1\\decoder\\train\\etypes=1\\(mass_1+mass_2+mass_3+mass_4)\\D=gru\\(gru)_decoder_1.1\\checkpoints:\n",
      "\n",
      "['epoch=4-step=80.ckpt']\n"
     ]
    }
   ],
   "source": [
    "from topology_estimation.settings.manager import get_checkpoint_path\n",
    "\n",
    "trained_decoder_model = Decoder.load_from_checkpoint(get_checkpoint_path(train_log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "169d1d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_decoder_model.n_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dac9c2",
   "metadata": {},
   "source": [
    "#### Plot and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "\n",
    "event_dir = os.path.join('model_logs', 'trials', 'nri_model_trial3', 'version_0')\n",
    "print(event_dir)\n",
    "\n",
    "ea = event_accumulator.EventAccumulator(event_dir)\n",
    "ea.Reload()\n",
    "\n",
    "# List all tags\n",
    "# print(ea.Tags())\n",
    "\n",
    "loss_events = ea.Scalars('train_loss')\n",
    "losses = [event.value for event in loss_events]\n",
    "steps = [event.step for event in loss_events]\n",
    "\n",
    "print(\"Steps:\", steps)\n",
    "print(\"Losses:\", losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps, losses, label='Train Loss')\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "\n",
    "# Convert matplotlib fig to image\n",
    "buf = io.BytesIO()\n",
    "fig.savefig(buf, format='png', dpi=1000)\n",
    "buf.seek(0)\n",
    "image = Image.open(buf)\n",
    "image_np = np.array(image)\n",
    "\n",
    "writer = SummaryWriter(\"model_logs\\\\trials\\\\nri_model_trial3\")\n",
    "\n",
    "writer.add_image(\"test_plot\", image_np.transpose(2, 0, 1), global_step=0)\n",
    "buf.close()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeab4bd",
   "metadata": {},
   "source": [
    "### Trying the config file stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198706c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7affc556",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'augment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtopology_estimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainNRIConfig, SelectTopologyEstimatorModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConfig\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprep\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_spring_particle_data\n\u001b[32m      5\u001b[39m run_type = \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# or 'predict'\u001b[39;00m\n\u001b[32m      6\u001b[39m tp_config = TrainNRIConfig()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AFD\\data\\prep.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConfig\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mh5py\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maugment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_gaussian_noise\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConfig\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'augment'"
     ]
    }
   ],
   "source": [
    "from topology_estimation.config import TrainNRIConfig, SelectTopologyEstimatorModel\n",
    "from data.config import DataConfig\n",
    "from data.prep import load_spring_particle_data\n",
    "\n",
    "run_type = 'train'  # or 'predict'\n",
    "tp_config = TrainNRIConfig()\n",
    "data_config = DataConfig()\n",
    "\n",
    "tp_config.set_encoder_params()\n",
    "tp_config.set_decoder_params()\n",
    "\n",
    "# if tp_config.is_sparsifier:\n",
    "#     tp_config.get_sparsif_config()\n",
    "    \n",
    "# load data\n",
    "data_config = DataConfig()\n",
    "data_config.set_train_dataset()\n",
    "\n",
    "# get node and edge dataset path from which data will be loaded\n",
    "node_ds_paths, edge_ds_paths = data_config.get_dataset_paths()\n",
    "\n",
    "# load datalaoders\n",
    "train_loader, valid_loader, test_loader, data_stats = load_spring_particle_data(node_ds_paths, edge_ds_paths)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "data = next(dataiter)\n",
    "\n",
    "n_nodes = data[0].shape[1]\n",
    "n_timesteps = data[0].shape[2]\n",
    "n_dims = data[0].shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4dbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'edge_estimator_2.1' already exists in the log path 'c:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\train\\etypes=2\\edge_estimator_2.1'.\n",
      "Overwrote 'edge_estimator_2.1' from the log path c:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\train\\etypes=2\\edge_estimator_2.1.\n",
      "c:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\train\\etypes=2\\edge_estimator_2.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.config import DataConfig\n",
    "from topology_estimation.config import TrainNRIConfig\n",
    "\n",
    "data_config = DataConfig()\n",
    "data_config.set_train_dataset()\n",
    "\n",
    "tp_config = TrainNRIConfig()\n",
    "tp_config.set_encoder_params()\n",
    "tp_config.set_decoder_params()\n",
    "\n",
    "log_path_nri = tp_config.get_train_log_path(500, 1)\n",
    "print(log_path_nri)\n",
    "\n",
    "if log_path_nri is not None:\n",
    "    os.makedirs(log_path_nri, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703eb838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters saved to c:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\train\\etypes=2\\edge_estimator_2.1.\n"
     ]
    }
   ],
   "source": [
    "tp_config.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0269e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1 already exists in the log path 'logs\\spring_particles\\P005\\scenario_1\\directed_graph\\enc=mlp_1-dec=gru\\healthy\\H1_[OG]\\dp=49-dim=4-etype=2\\sparsif=[knn+time]\\(sparsif)=_no_fex\\[enc]=freq-[dec]=time\\(enc)=PCA+first_n_modes-(dec)=PCA\\v1'.\n",
      "Removed version 1 from the log path logs\\spring_particles\\P005\\scenario_1\\directed_graph\\enc=mlp_1-dec=gru\\healthy\\H1_[OG]\\dp=49-dim=4-etype=2\\sparsif=[knn+time]\\(sparsif)=_no_fex\\[enc]=freq-[dec]=time\\(enc)=PCA+first_n_modes-(dec)=PCA\\v1.\n"
     ]
    }
   ],
   "source": [
    "tp_config.check_if_version_exists(log_path_nri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b3ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs\\spring_particles\\P005\\scenario_1\\directed_graph\\enc=mlp_1_dec=gru\\dp=49\\healthy\\H1_[OG]\\sparsif_knn\\(sparsif)_no_fex\\(nri)_no_fex\\v2\n"
     ]
    }
   ],
   "source": [
    "print(tp_config.log_path)\n",
    "os.makedirs(tp_config.log_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f988d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'first_n_modes', 'n_modes': 5}, {'type': 'lucas', 'weight': 0.5, 'height': 9, 'age': 20}]\n",
      "[{'type': 'first_n_modes', 'n_modes': 69}]\n"
     ]
    }
   ],
   "source": [
    "print(tp_config.fex_configs_sparsif)\n",
    "print(tp_config.fex_configs_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46950b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">bearing</span>\n",
       "â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">cwru</span>\n",
       "    â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">scene_1</span>\n",
       "        â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">directed_graph</span>\n",
       "            â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;n_edge_types&gt;</span>\n",
       "            â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">etypes=2</span>\n",
       "                â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;ds_type&gt;</span>\n",
       "                â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">OK_NOK</span>\n",
       "                    â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;ds_subtype&gt;</span>\n",
       "                    â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0_N[OG--gau_m=0.1s=0.2--gau_m=0.2s=0.3]_+_0_B-007[gau_m=0.0s=0.1--gau_m=0.1s=0.2]_+_0_B-021</span>\n",
       "                        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">[OG]</span>\n",
       "                        â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;model&gt;</span>\n",
       "                        â””â”€â”€ <span style=\"color: #ffff00; text-decoration-color: #ffff00\">E=mlp_1-D=gru</span>\n",
       "                            â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;ds_stats&gt;</span>\n",
       "                            â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">T500_m=[acc]</span>\n",
       "                                â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;sparsif_type&gt;</span>\n",
       "                                â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">spf=_no_spf</span>\n",
       "                                    â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;domain&gt;</span>\n",
       "                                    â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">[E]=freq-[D]=time</span>\n",
       "                                        â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;nri_fex_type&gt;</span>\n",
       "                                        â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">(E)=_no_fex-(D)=[first_n_modes]</span>\n",
       "                                            â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;shape_compatibility&gt;</span>\n",
       "                                            â””â”€â”€ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">E(comps)=500-D(dims)=1</span>\n",
       "                                                â”œâ”€â”€ <span style=\"color: #000080; text-decoration-color: #000080\">&lt;versions&gt;</span>\n",
       "                                                â””â”€â”€ <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold; font-style: italic\">v1</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">[0]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mbearing\u001b[0m\n",
       "â””â”€â”€ \u001b[32mcwru\u001b[0m\n",
       "    â””â”€â”€ \u001b[32mscene_1\u001b[0m\n",
       "        â””â”€â”€ \u001b[32mdirected_graph\u001b[0m\n",
       "            â”œâ”€â”€ \u001b[34m<n_edge_types>\u001b[0m\n",
       "            â””â”€â”€ \u001b[37metypes=2\u001b[0m\n",
       "                â”œâ”€â”€ \u001b[34m<ds_type>\u001b[0m\n",
       "                â””â”€â”€ \u001b[37mOK_NOK\u001b[0m\n",
       "                    â”œâ”€â”€ \u001b[34m<ds_subtype>\u001b[0m\n",
       "                    â””â”€â”€ \u001b[37m0_N[OG--gau_m=0.1s=0.2--gau_m=0.2s=0.3]_+_0_B-007[gau_m=0.0s=0.1--gau_m=0.1s=0.2]_+_0_B-021\u001b[0m\n",
       "                        \u001b[37m[OG]\u001b[0m\n",
       "                        â”œâ”€â”€ \u001b[34m<model>\u001b[0m\n",
       "                        â””â”€â”€ \u001b[93mE=mlp_1-D=gru\u001b[0m\n",
       "                            â”œâ”€â”€ \u001b[34m<ds_stats>\u001b[0m\n",
       "                            â””â”€â”€ \u001b[37mT500_m=[acc]\u001b[0m\n",
       "                                â”œâ”€â”€ \u001b[34m<sparsif_type>\u001b[0m\n",
       "                                â””â”€â”€ \u001b[37mspf=_no_spf\u001b[0m\n",
       "                                    â”œâ”€â”€ \u001b[34m<domain>\u001b[0m\n",
       "                                    â””â”€â”€ \u001b[37m[E]=freq-[D]=time\u001b[0m\n",
       "                                        â”œâ”€â”€ \u001b[34m<nri_fex_type>\u001b[0m\n",
       "                                        â””â”€â”€ \u001b[37m(E)=_no_fex-(D)=[first_n_modes]\u001b[0m\n",
       "                                            â”œâ”€â”€ \u001b[34m<shape_compatibility>\u001b[0m\n",
       "                                            â””â”€â”€ \u001b[37mE(comps)=500-D(dims)=1\u001b[0m\n",
       "                                                â”œâ”€â”€ \u001b[34m<versions>\u001b[0m\n",
       "                                                â””â”€â”€ \u001b[1;3;93mv1\u001b[0m \u001b[96m[0]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available version paths:\n",
      "0: logs/bearing\\cwru\\scene_1\\directed_graph\\etypes=2\\OK_NOK\\0_N[OG--gau_m=0.1s=0.2--gau_m=0.2s=0.3]_+_0_B-007[gau_m=0.0s=0.1--gau_m=0.1s=0.2]_+_0_B-021[OG]\\E=mlp_1-D=gru\\T500_m=[acc]\\spf=_no_spf\\[E]=freq-[D]=time\\(E)=_no_fex-(D)=[first_n_modes]\\E(comps)=500-D(dims)=1\\v1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtopology_estimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SelectTopologyEstimatorModel\n\u001b[32m      3\u001b[39m model_selector = SelectTopologyEstimatorModel(\n\u001b[32m      4\u001b[39m                                               framework=\u001b[33m'\u001b[39m\u001b[33mdirected_graph\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m ckpt = \u001b[43mmodel_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_ckpt_and_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AFD\\topology_estimation\\config.py:707\u001b[39m, in \u001b[36mSelectTopologyEstimatorModel.select_ckpt_and_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo version paths found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m idx = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mEnter the index number of the version path to select: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx >= \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.version_paths):\n\u001b[32m    709\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInvalid index.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "from topology_estimation.config import SelectTopologyEstimatorModel\n",
    "\n",
    "model_selector = SelectTopologyEstimatorModel(\n",
    "                                              framework='directed_graph')\n",
    "\n",
    "ckpt = model_selector.select_ckpt_and_params()\n",
    "print(f\"Selected checkpoint: {ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f32bc",
   "metadata": {},
   "source": [
    "predict and custom test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d0c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e230c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predict number 2 for already exists for model_2.2 in the log path 'C:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\predict\\etypes=2\\model_2.2\\predict_2.2'.\n",
      "C:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\predict\\etypes=2\\model_2.2\\predict_2.6\n",
      "Predict parameters saved to C:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\predict\\etypes=2\\model_2.2\\predict_2.6.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.config import DataConfig\n",
    "from topology_estimation.config import PredictNRIConfig\n",
    "\n",
    "data_config = DataConfig()\n",
    "data_config.set_predict_dataset()\n",
    "\n",
    "tp_config = PredictNRIConfig()\n",
    "\n",
    "log_path_nri = tp_config.get_predict_log_path()\n",
    "print(log_path_nri)\n",
    "\n",
    "if log_path_nri is not None:\n",
    "    os.makedirs(log_path_nri, exist_ok=True)\n",
    "\n",
    "tp_config.save_predict_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ec6e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "custom_test number 2 for already exists for model_2.2 in the log path 'C:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\custom_test\\etypes=2\\model_2.2\\custom_test_2.2'.\n",
      "C:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\custom_test\\etypes=2\\model_2.2\\custom_test_2.5\n",
      "Custom test parameters saved to C:\\AFD\\topology_estimation\\logs\\bearing\\cwru\\scene_1\\directed_graph\\custom_test\\etypes=2\\model_2.2\\custom_test_2.5.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.config import DataConfig\n",
    "from topology_estimation.config import PredictNRIConfig\n",
    "\n",
    "data_config = DataConfig()\n",
    "data_config.set_custom_test_dataset()\n",
    "\n",
    "tp_config = PredictNRIConfig()\n",
    "\n",
    "log_path_nri = tp_config.get_custom_test_log_path()\n",
    "print(log_path_nri)\n",
    "\n",
    "if log_path_nri is not None:\n",
    "    os.makedirs(log_path_nri, exist_ok=True)\n",
    "\n",
    "tp_config.save_custom_test_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
