=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m005_(apv+G)]-gru_dec_1.1
Start Time: 2025-09-21 20:43:48
End Time: 2025-09-21 21:06:10

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting decoder model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]
  (5) mass_5   : [acc, pos, vel]

Node group name: m005
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.

Target rep_num 1001.0001 found at index 0. This sample will be included in the test set.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 450/500 [OK=450, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 5, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 5, 100, 3]) 

val_data_loader statistics:
Number of batches: 9
torch.Size([50, 5, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (5, 5)
     n1   n2   n3   n4   n5
n1  0.0  0.0  0.0  0.0  0.0
n2  0.0  0.0  1.0  0.0  0.0
n3  0.0  1.0  0.0  1.0  0.0
n4  0.0  0.0  1.0  0.0  1.0
n5  0.0  0.0  0.0  1.0  0.0


**Receiver relation matrix** => shape: (20, 5)
      n1   n2   n3   n4   n5
e12  0.0  0.0  0.0  0.0  0.0
e13  0.0  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0  0.0
e15  0.0  0.0  0.0  0.0  0.0
e21  0.0  0.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0  0.0
e24  0.0  0.0  0.0  0.0  0.0
e25  0.0  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0  0.0
e34  0.0  0.0  0.0  1.0  0.0
e35  0.0  0.0  0.0  0.0  0.0
e41  0.0  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0  0.0
e43  0.0  0.0  1.0  0.0  0.0
e45  0.0  0.0  0.0  0.0  1.0
e51  0.0  0.0  0.0  0.0  0.0
e52  0.0  0.0  0.0  0.0  0.0
e53  0.0  0.0  0.0  0.0  0.0
e54  0.0  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (20, 5)
      n1   n2   n3   n4   n5
e12  0.0  0.0  0.0  0.0  0.0
e13  0.0  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0  0.0
e15  0.0  0.0  0.0  0.0  0.0
e21  0.0  0.0  0.0  0.0  0.0
e23  0.0  1.0  0.0  0.0  0.0
e24  0.0  0.0  0.0  0.0  0.0
e25  0.0  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0  0.0
e32  0.0  0.0  1.0  0.0  0.0
e34  0.0  0.0  1.0  0.0  0.0
e35  0.0  0.0  0.0  0.0  0.0
e41  0.0  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0  0.0
e43  0.0  0.0  0.0  1.0  0.0
e45  0.0  0.0  0.0  1.0  0.0
e51  0.0  0.0  0.0  0.0  0.0
e52  0.0  0.0  0.0  0.0  0.0
e53  0.0  0.0  0.0  0.0  0.0
e54  0.0  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 1
msg_out_size: 128
edge_mlp_config: [[128, 'tanh'], [128, 'tanh']]
out_mlp_config: [[128, 'relu'], [128, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
n_dims: 3

Decoder run parameters:
-------------------------
skip_first_edge_type: False
pred_steps: 10
is_burn_in: True
final_pred_steps: 50
is_dynamic_graph: False
temp: 1.0
is_hard: True
show_conf_band: False
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1

---------------------------------------------------------------------------

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Training parameters set to: 
lr=0.001, 
optimizer=adam, 
loss_type=mae

---------------------------------------------------------------------------

Decoder Model Initialized with the following configurations:

Decoder Model Summary:
Decoder(
  (edge_mlp_fn): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=128, out_features=128, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=128, bias=True)
    (hidden_u): Linear(in_features=128, out_features=128, bias=True)
    (input_r): Linear(in_features=3, out_features=128, bias=True)
    (hidden_r): Linear(in_features=128, out_features=128, bias=True)
    (input_h): Linear(in_features=3, out_features=128, bias=True)
    (hidden_h): Linear(in_features=128, out_features=128, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=128, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=128, out_features=3, bias=True)
)

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Step 0, Epoch 1/50, Batch 0/80
train_loss: 0.9351

Step 5, Epoch 1/50, Batch 5/80
train_loss: 0.1905

Step 10, Epoch 1/50, Batch 10/80
train_loss: 0.1699

Step 15, Epoch 1/50, Batch 15/80
train_loss: 0.1557

Step 20, Epoch 1/50, Batch 20/80
train_loss: 0.1532

Step 25, Epoch 1/50, Batch 25/80
train_loss: 0.1523

Step 30, Epoch 1/50, Batch 30/80
train_loss: 0.1460

Step 35, Epoch 1/50, Batch 35/80
train_loss: 0.1363

Step 40, Epoch 1/50, Batch 40/80
train_loss: 0.1285

Step 45, Epoch 1/50, Batch 45/80
train_loss: 0.1204

Step 50, Epoch 1/50, Batch 50/80
train_loss: 0.1173

Step 55, Epoch 1/50, Batch 55/80
train_loss: 0.1189

Step 60, Epoch 1/50, Batch 60/80
train_loss: 0.1163

Step 65, Epoch 1/50, Batch 65/80
train_loss: 0.1179

Step 70, Epoch 1/50, Batch 70/80
train_loss: 0.1150

Step 75, Epoch 1/50, Batch 75/80
train_loss: 0.1170

Epoch 1/50 completed, Global Step: 79
train_loss: 0.1170, val_loss: 0.1154

---------------------------------------------------------------------------


Step 80, Epoch 2/50, Batch 0/80
train_loss: 0.1169

Step 85, Epoch 2/50, Batch 5/80
train_loss: 0.1155

Step 90, Epoch 2/50, Batch 10/80
train_loss: 0.1135

Step 95, Epoch 2/50, Batch 15/80
train_loss: 0.1130

Step 100, Epoch 2/50, Batch 20/80
train_loss: 0.1129

Step 105, Epoch 2/50, Batch 25/80
train_loss: 0.1126

Step 110, Epoch 2/50, Batch 30/80
train_loss: 0.1102

Step 115, Epoch 2/50, Batch 35/80
train_loss: 0.1096

Step 120, Epoch 2/50, Batch 40/80
train_loss: 0.1144

Step 125, Epoch 2/50, Batch 45/80
train_loss: 0.1112

Step 130, Epoch 2/50, Batch 50/80
train_loss: 0.1099

Step 135, Epoch 2/50, Batch 55/80
train_loss: 0.1091

Step 140, Epoch 2/50, Batch 60/80
train_loss: 0.1094

Step 145, Epoch 2/50, Batch 65/80
train_loss: 0.1135

Step 150, Epoch 2/50, Batch 70/80
train_loss: 0.1078

Step 155, Epoch 2/50, Batch 75/80
train_loss: 0.1068

Epoch 2/50 completed, Global Step: 159
train_loss: 0.1068, val_loss: 0.1077

---------------------------------------------------------------------------


Step 160, Epoch 3/50, Batch 0/80
train_loss: 0.1067

Step 165, Epoch 3/50, Batch 5/80
train_loss: 0.1054

Step 170, Epoch 3/50, Batch 10/80
train_loss: 0.1056

Step 175, Epoch 3/50, Batch 15/80
train_loss: 0.1069

Step 180, Epoch 3/50, Batch 20/80
train_loss: 0.1072

Step 185, Epoch 3/50, Batch 25/80
train_loss: 0.1061

Step 190, Epoch 3/50, Batch 30/80
train_loss: 0.1059

Step 195, Epoch 3/50, Batch 35/80
train_loss: 0.1046

Step 200, Epoch 3/50, Batch 40/80
train_loss: 0.1044

Step 205, Epoch 3/50, Batch 45/80
train_loss: 0.1029

Step 210, Epoch 3/50, Batch 50/80
train_loss: 0.1032

Step 215, Epoch 3/50, Batch 55/80
train_loss: 0.1036

Step 220, Epoch 3/50, Batch 60/80
train_loss: 0.1029

Step 225, Epoch 3/50, Batch 65/80
train_loss: 0.1025

Step 230, Epoch 3/50, Batch 70/80
train_loss: 0.1013

Step 235, Epoch 3/50, Batch 75/80
train_loss: 0.1060

Epoch 3/50 completed, Global Step: 239
train_loss: 0.1060, val_loss: 0.1060

---------------------------------------------------------------------------


Step 240, Epoch 4/50, Batch 0/80
train_loss: 0.1057

Step 245, Epoch 4/50, Batch 5/80
train_loss: 0.1020

Step 250, Epoch 4/50, Batch 10/80
train_loss: 0.0994

Step 255, Epoch 4/50, Batch 15/80
train_loss: 0.1012

Step 260, Epoch 4/50, Batch 20/80
train_loss: 0.1017

Step 265, Epoch 4/50, Batch 25/80
train_loss: 0.1046

Step 270, Epoch 4/50, Batch 30/80
train_loss: 0.0996

Step 275, Epoch 4/50, Batch 35/80
train_loss: 0.0991

Step 280, Epoch 4/50, Batch 40/80
train_loss: 0.1011

Step 285, Epoch 4/50, Batch 45/80
train_loss: 0.1001

Step 290, Epoch 4/50, Batch 50/80
train_loss: 0.0984

Step 295, Epoch 4/50, Batch 55/80
train_loss: 0.0998

Step 300, Epoch 4/50, Batch 60/80
train_loss: 0.0969

Step 305, Epoch 4/50, Batch 65/80
train_loss: 0.0982

Step 310, Epoch 4/50, Batch 70/80
train_loss: 0.0969

Step 315, Epoch 4/50, Batch 75/80
train_loss: 0.0992

Epoch 4/50 completed, Global Step: 319
train_loss: 0.0992, val_loss: 0.1026

---------------------------------------------------------------------------


Step 320, Epoch 5/50, Batch 0/80
train_loss: 0.1020

Step 325, Epoch 5/50, Batch 5/80
train_loss: 0.1003

Step 330, Epoch 5/50, Batch 10/80
train_loss: 0.0999

Step 335, Epoch 5/50, Batch 15/80
train_loss: 0.0977

Step 340, Epoch 5/50, Batch 20/80
train_loss: 0.0960

Step 345, Epoch 5/50, Batch 25/80
train_loss: 0.0958

Step 350, Epoch 5/50, Batch 30/80
train_loss: 0.0957

Step 355, Epoch 5/50, Batch 35/80
train_loss: 0.0962

Step 360, Epoch 5/50, Batch 40/80
train_loss: 0.0951

Step 365, Epoch 5/50, Batch 45/80
train_loss: 0.0942

Step 370, Epoch 5/50, Batch 50/80
train_loss: 0.0956

Step 375, Epoch 5/50, Batch 55/80
train_loss: 0.0943

Step 380, Epoch 5/50, Batch 60/80
train_loss: 0.0979

Step 385, Epoch 5/50, Batch 65/80
train_loss: 0.0941

Step 390, Epoch 5/50, Batch 70/80
train_loss: 0.0941

Step 395, Epoch 5/50, Batch 75/80
train_loss: 0.0940

Epoch 5/50 completed, Global Step: 399
train_loss: 0.0940, val_loss: 0.0944

---------------------------------------------------------------------------


Step 400, Epoch 6/50, Batch 0/80
train_loss: 0.0939

Step 405, Epoch 6/50, Batch 5/80
train_loss: 0.0945

Step 410, Epoch 6/50, Batch 10/80
train_loss: 0.0943

Step 415, Epoch 6/50, Batch 15/80
train_loss: 0.0939

Step 420, Epoch 6/50, Batch 20/80
train_loss: 0.0958

Step 425, Epoch 6/50, Batch 25/80
train_loss: 0.0934

Step 430, Epoch 6/50, Batch 30/80
train_loss: 0.0950

Step 435, Epoch 6/50, Batch 35/80
train_loss: 0.0949

Step 440, Epoch 6/50, Batch 40/80
train_loss: 0.0924

Step 445, Epoch 6/50, Batch 45/80
train_loss: 0.0913

Step 450, Epoch 6/50, Batch 50/80
train_loss: 0.0919

Step 455, Epoch 6/50, Batch 55/80
train_loss: 0.0905

Step 460, Epoch 6/50, Batch 60/80
train_loss: 0.0901

Step 465, Epoch 6/50, Batch 65/80
train_loss: 0.0933

Step 470, Epoch 6/50, Batch 70/80
train_loss: 0.0901

Step 475, Epoch 6/50, Batch 75/80
train_loss: 0.0926

Epoch 6/50 completed, Global Step: 479
train_loss: 0.0926, val_loss: 0.0900

---------------------------------------------------------------------------


Step 480, Epoch 7/50, Batch 0/80
train_loss: 0.0896

Step 485, Epoch 7/50, Batch 5/80
train_loss: 0.0898

Step 490, Epoch 7/50, Batch 10/80
train_loss: 0.0887

Step 495, Epoch 7/50, Batch 15/80
train_loss: 0.0870

Step 500, Epoch 7/50, Batch 20/80
train_loss: 0.0873

Step 505, Epoch 7/50, Batch 25/80
train_loss: 0.0938

Step 510, Epoch 7/50, Batch 30/80
train_loss: 0.0895

Step 515, Epoch 7/50, Batch 35/80
train_loss: 0.0874

Step 520, Epoch 7/50, Batch 40/80
train_loss: 0.0876

Step 525, Epoch 7/50, Batch 45/80
train_loss: 0.0864

Step 530, Epoch 7/50, Batch 50/80
train_loss: 0.0868

Step 535, Epoch 7/50, Batch 55/80
train_loss: 0.0842

Step 540, Epoch 7/50, Batch 60/80
train_loss: 0.0840

Step 545, Epoch 7/50, Batch 65/80
train_loss: 0.0847

Step 550, Epoch 7/50, Batch 70/80
train_loss: 0.0881

Step 555, Epoch 7/50, Batch 75/80
train_loss: 0.0857

Epoch 7/50 completed, Global Step: 559
train_loss: 0.0857, val_loss: 0.0848

---------------------------------------------------------------------------


Step 560, Epoch 8/50, Batch 0/80
train_loss: 0.0850

Step 565, Epoch 8/50, Batch 5/80
train_loss: 0.0863

Step 570, Epoch 8/50, Batch 10/80
train_loss: 0.0832

Step 575, Epoch 8/50, Batch 15/80
train_loss: 0.0833

Step 580, Epoch 8/50, Batch 20/80
train_loss: 0.0889

Step 585, Epoch 8/50, Batch 25/80
train_loss: 0.0836

Step 590, Epoch 8/50, Batch 30/80
train_loss: 0.0821

Step 595, Epoch 8/50, Batch 35/80
train_loss: 0.0807

Step 600, Epoch 8/50, Batch 40/80
train_loss: 0.0811

Step 605, Epoch 8/50, Batch 45/80
train_loss: 0.0815

Step 610, Epoch 8/50, Batch 50/80
train_loss: 0.0805

Step 615, Epoch 8/50, Batch 55/80
train_loss: 0.0876

Step 620, Epoch 8/50, Batch 60/80
train_loss: 0.0846

Step 625, Epoch 8/50, Batch 65/80
train_loss: 0.0786

Step 630, Epoch 8/50, Batch 70/80
train_loss: 0.0761

Step 635, Epoch 8/50, Batch 75/80
train_loss: 0.0806

Epoch 8/50 completed, Global Step: 639
train_loss: 0.0806, val_loss: 0.0787

---------------------------------------------------------------------------


Step 640, Epoch 9/50, Batch 0/80
train_loss: 0.0783

Step 645, Epoch 9/50, Batch 5/80
train_loss: 0.0768

Step 650, Epoch 9/50, Batch 10/80
train_loss: 0.0836

Step 655, Epoch 9/50, Batch 15/80
train_loss: 0.0779

Step 660, Epoch 9/50, Batch 20/80
train_loss: 0.0769

Step 665, Epoch 9/50, Batch 25/80
train_loss: 0.0768

Step 670, Epoch 9/50, Batch 30/80
train_loss: 0.0738

Step 675, Epoch 9/50, Batch 35/80
train_loss: 0.0733

Step 680, Epoch 9/50, Batch 40/80
train_loss: 0.0716

Step 685, Epoch 9/50, Batch 45/80
train_loss: 0.0817

Step 690, Epoch 9/50, Batch 50/80
train_loss: 0.0740

Step 695, Epoch 9/50, Batch 55/80
train_loss: 0.0707

Step 700, Epoch 9/50, Batch 60/80
train_loss: 0.0705

Step 705, Epoch 9/50, Batch 65/80
train_loss: 0.0677

Step 710, Epoch 9/50, Batch 70/80
train_loss: 0.0744

Step 715, Epoch 9/50, Batch 75/80
train_loss: 0.0708

Epoch 9/50 completed, Global Step: 719
train_loss: 0.0708, val_loss: 0.0682

---------------------------------------------------------------------------


Step 720, Epoch 10/50, Batch 0/80
train_loss: 0.0679

Step 725, Epoch 10/50, Batch 5/80
train_loss: 0.0657

Step 730, Epoch 10/50, Batch 10/80
train_loss: 0.0663

Step 735, Epoch 10/50, Batch 15/80
train_loss: 0.0660

Step 740, Epoch 10/50, Batch 20/80
train_loss: 0.0665

Step 745, Epoch 10/50, Batch 25/80
train_loss: 0.0635

Step 750, Epoch 10/50, Batch 30/80
train_loss: 0.0624

Step 755, Epoch 10/50, Batch 35/80
train_loss: 0.0672

Step 760, Epoch 10/50, Batch 40/80
train_loss: 0.0715

Step 765, Epoch 10/50, Batch 45/80
train_loss: 0.0646

Step 770, Epoch 10/50, Batch 50/80
train_loss: 0.0667

Step 775, Epoch 10/50, Batch 55/80
train_loss: 0.0679

Step 780, Epoch 10/50, Batch 60/80
train_loss: 0.0656

Step 785, Epoch 10/50, Batch 65/80
train_loss: 0.0639

Step 790, Epoch 10/50, Batch 70/80
train_loss: 0.0603

Step 795, Epoch 10/50, Batch 75/80
train_loss: 0.0570

Epoch 10/50 completed, Global Step: 799
train_loss: 0.0570, val_loss: 0.0633

---------------------------------------------------------------------------


Step 800, Epoch 11/50, Batch 0/80
train_loss: 0.0631

Step 805, Epoch 11/50, Batch 5/80
train_loss: 0.0636

Step 810, Epoch 11/50, Batch 10/80
train_loss: 0.0801

Step 815, Epoch 11/50, Batch 15/80
train_loss: 0.0686

Step 820, Epoch 11/50, Batch 20/80
train_loss: 0.0646

Step 825, Epoch 11/50, Batch 25/80
train_loss: 0.0641

Step 830, Epoch 11/50, Batch 30/80
train_loss: 0.0555

Step 835, Epoch 11/50, Batch 35/80
train_loss: 0.0612

Step 840, Epoch 11/50, Batch 40/80
train_loss: 0.0656

Step 845, Epoch 11/50, Batch 45/80
train_loss: 0.0571

Step 850, Epoch 11/50, Batch 50/80
train_loss: 0.0694

Step 855, Epoch 11/50, Batch 55/80
train_loss: 0.0642

Step 860, Epoch 11/50, Batch 60/80
train_loss: 0.0683

Step 865, Epoch 11/50, Batch 65/80
train_loss: 0.0575

Step 870, Epoch 11/50, Batch 70/80
train_loss: 0.0550

Step 875, Epoch 11/50, Batch 75/80
train_loss: 0.0542

Epoch 11/50 completed, Global Step: 879
train_loss: 0.0542, val_loss: 0.0540

---------------------------------------------------------------------------


Step 880, Epoch 12/50, Batch 0/80
train_loss: 0.0538

Step 885, Epoch 12/50, Batch 5/80
train_loss: 0.0576

Step 890, Epoch 12/50, Batch 10/80
train_loss: 0.0443

Step 895, Epoch 12/50, Batch 15/80
train_loss: 0.0530

Step 900, Epoch 12/50, Batch 20/80
train_loss: 0.0536

Step 905, Epoch 12/50, Batch 25/80
train_loss: 0.0721

Step 910, Epoch 12/50, Batch 30/80
train_loss: 0.0699

Step 915, Epoch 12/50, Batch 35/80
train_loss: 0.0651

Step 920, Epoch 12/50, Batch 40/80
train_loss: 0.0638

Step 925, Epoch 12/50, Batch 45/80
train_loss: 0.0598

Step 930, Epoch 12/50, Batch 50/80
train_loss: 0.0568

Step 935, Epoch 12/50, Batch 55/80
train_loss: 0.0563

Step 940, Epoch 12/50, Batch 60/80
train_loss: 0.0525

Step 945, Epoch 12/50, Batch 65/80
train_loss: 0.0501

Step 950, Epoch 12/50, Batch 70/80
train_loss: 0.0523

Step 955, Epoch 12/50, Batch 75/80
train_loss: 0.0413

Epoch 12/50 completed, Global Step: 959
train_loss: 0.0413, val_loss: 0.0448

---------------------------------------------------------------------------


Step 960, Epoch 13/50, Batch 0/80
train_loss: 0.0446

Step 965, Epoch 13/50, Batch 5/80
train_loss: 0.0516

Step 970, Epoch 13/50, Batch 10/80
train_loss: 0.0473

Step 975, Epoch 13/50, Batch 15/80
train_loss: 0.0508

Step 980, Epoch 13/50, Batch 20/80
train_loss: 0.0435

Step 985, Epoch 13/50, Batch 25/80
train_loss: 0.0427

Step 990, Epoch 13/50, Batch 30/80
train_loss: 0.0377

Step 995, Epoch 13/50, Batch 35/80
train_loss: 0.0486

Step 1000, Epoch 13/50, Batch 40/80
train_loss: 0.0370

Step 1005, Epoch 13/50, Batch 45/80
train_loss: 0.0385

Step 1010, Epoch 13/50, Batch 50/80
train_loss: 0.0422

Step 1015, Epoch 13/50, Batch 55/80
train_loss: 0.0309

Step 1020, Epoch 13/50, Batch 60/80
train_loss: 0.0401

Step 1025, Epoch 13/50, Batch 65/80
train_loss: 0.0501

Step 1030, Epoch 13/50, Batch 70/80
train_loss: 0.0438

Step 1035, Epoch 13/50, Batch 75/80
train_loss: 0.0350

Epoch 13/50 completed, Global Step: 1039
train_loss: 0.0350, val_loss: 0.0269

---------------------------------------------------------------------------


Step 1040, Epoch 14/50, Batch 0/80
train_loss: 0.0271

Step 1045, Epoch 14/50, Batch 5/80
train_loss: 0.0435

Step 1050, Epoch 14/50, Batch 10/80
train_loss: 0.0254

Step 1055, Epoch 14/50, Batch 15/80
train_loss: 0.0227

Step 1060, Epoch 14/50, Batch 20/80
train_loss: 0.0266

Step 1065, Epoch 14/50, Batch 25/80
train_loss: 0.0250

Step 1070, Epoch 14/50, Batch 30/80
train_loss: 0.0272

Step 1075, Epoch 14/50, Batch 35/80
train_loss: 0.0282

Step 1080, Epoch 14/50, Batch 40/80
train_loss: 0.0252

Step 1085, Epoch 14/50, Batch 45/80
train_loss: 0.0196

Step 1090, Epoch 14/50, Batch 50/80
train_loss: 0.0217

Step 1095, Epoch 14/50, Batch 55/80
train_loss: 0.0309

Step 1100, Epoch 14/50, Batch 60/80
train_loss: 0.0212

Step 1105, Epoch 14/50, Batch 65/80
train_loss: 0.0166

Step 1110, Epoch 14/50, Batch 70/80
train_loss: 0.0173

Step 1115, Epoch 14/50, Batch 75/80
train_loss: 0.0269

Epoch 14/50 completed, Global Step: 1119
train_loss: 0.0269, val_loss: 0.0235

---------------------------------------------------------------------------


Step 1120, Epoch 15/50, Batch 0/80
train_loss: 0.0240

Step 1125, Epoch 15/50, Batch 5/80
train_loss: 0.0212

Step 1130, Epoch 15/50, Batch 10/80
train_loss: 0.0286

Step 1135, Epoch 15/50, Batch 15/80
train_loss: 0.0184

Step 1140, Epoch 15/50, Batch 20/80
train_loss: 0.0234

Step 1145, Epoch 15/50, Batch 25/80
train_loss: 0.0230

Step 1150, Epoch 15/50, Batch 30/80
train_loss: 0.0261

Step 1155, Epoch 15/50, Batch 35/80
train_loss: 0.0195

Step 1160, Epoch 15/50, Batch 40/80
train_loss: 0.0199

Step 1165, Epoch 15/50, Batch 45/80
train_loss: 0.0165

Step 1170, Epoch 15/50, Batch 50/80
train_loss: 0.0221

Step 1175, Epoch 15/50, Batch 55/80
train_loss: 0.0267

Step 1180, Epoch 15/50, Batch 60/80
train_loss: 0.0206

Step 1185, Epoch 15/50, Batch 65/80
train_loss: 0.0238

Step 1190, Epoch 15/50, Batch 70/80
train_loss: 0.0202

Step 1195, Epoch 15/50, Batch 75/80
train_loss: 0.0234

Epoch 15/50 completed, Global Step: 1199
train_loss: 0.0234, val_loss: 0.0184

---------------------------------------------------------------------------


Step 1200, Epoch 16/50, Batch 0/80
train_loss: 0.0180

Step 1205, Epoch 16/50, Batch 5/80
train_loss: 0.0186

Step 1210, Epoch 16/50, Batch 10/80
train_loss: 0.0181

Step 1215, Epoch 16/50, Batch 15/80
train_loss: 0.0199

Step 1220, Epoch 16/50, Batch 20/80
train_loss: 0.0164

Step 1225, Epoch 16/50, Batch 25/80
train_loss: 0.0144

Step 1230, Epoch 16/50, Batch 30/80
train_loss: 0.0172

Step 1235, Epoch 16/50, Batch 35/80
train_loss: 0.0149

Step 1240, Epoch 16/50, Batch 40/80
train_loss: 0.0134

Step 1245, Epoch 16/50, Batch 45/80
train_loss: 0.0130

Step 1250, Epoch 16/50, Batch 50/80
train_loss: 0.0171

Step 1255, Epoch 16/50, Batch 55/80
train_loss: 0.0210

Step 1260, Epoch 16/50, Batch 60/80
train_loss: 0.0224

Step 1265, Epoch 16/50, Batch 65/80
train_loss: 0.0155

Step 1270, Epoch 16/50, Batch 70/80
train_loss: 0.0134

Step 1275, Epoch 16/50, Batch 75/80
train_loss: 0.0185

Epoch 16/50 completed, Global Step: 1279
train_loss: 0.0185, val_loss: 0.0222

---------------------------------------------------------------------------


Step 1280, Epoch 17/50, Batch 0/80
train_loss: 0.0225

Step 1285, Epoch 17/50, Batch 5/80
train_loss: 0.0249

Step 1290, Epoch 17/50, Batch 10/80
train_loss: 0.0195

Step 1295, Epoch 17/50, Batch 15/80
train_loss: 0.0169

Step 1300, Epoch 17/50, Batch 20/80
train_loss: 0.0141

Step 1305, Epoch 17/50, Batch 25/80
train_loss: 0.0177

Step 1310, Epoch 17/50, Batch 30/80
train_loss: 0.0139

Step 1315, Epoch 17/50, Batch 35/80
train_loss: 0.0171

Step 1320, Epoch 17/50, Batch 40/80
train_loss: 0.0235

Step 1325, Epoch 17/50, Batch 45/80
train_loss: 0.0186

Step 1330, Epoch 17/50, Batch 50/80
train_loss: 0.0182

Step 1335, Epoch 17/50, Batch 55/80
train_loss: 0.0179

Step 1340, Epoch 17/50, Batch 60/80
train_loss: 0.0184

Step 1345, Epoch 17/50, Batch 65/80
train_loss: 0.0155

Step 1350, Epoch 17/50, Batch 70/80
train_loss: 0.0164

Step 1355, Epoch 17/50, Batch 75/80
train_loss: 0.0138

Epoch 17/50 completed, Global Step: 1359
train_loss: 0.0138, val_loss: 0.0222

---------------------------------------------------------------------------


Step 1360, Epoch 18/50, Batch 0/80
train_loss: 0.0215

Step 1365, Epoch 18/50, Batch 5/80
train_loss: 0.0135

Step 1370, Epoch 18/50, Batch 10/80
train_loss: 0.0198

Step 1375, Epoch 18/50, Batch 15/80
train_loss: 0.0167

Step 1380, Epoch 18/50, Batch 20/80
train_loss: 0.0173

Step 1385, Epoch 18/50, Batch 25/80
train_loss: 0.0161

Step 1390, Epoch 18/50, Batch 30/80
train_loss: 0.0156

Step 1395, Epoch 18/50, Batch 35/80
train_loss: 0.0145

Step 1400, Epoch 18/50, Batch 40/80
train_loss: 0.0169

Step 1405, Epoch 18/50, Batch 45/80
train_loss: 0.0183

Step 1410, Epoch 18/50, Batch 50/80
train_loss: 0.0184

Step 1415, Epoch 18/50, Batch 55/80
train_loss: 0.0164

Step 1420, Epoch 18/50, Batch 60/80
train_loss: 0.0295

Step 1425, Epoch 18/50, Batch 65/80
train_loss: 0.0210

Step 1430, Epoch 18/50, Batch 70/80
train_loss: 0.0229

Step 1435, Epoch 18/50, Batch 75/80
train_loss: 0.0154

Epoch 18/50 completed, Global Step: 1439
train_loss: 0.0154, val_loss: 0.0144

---------------------------------------------------------------------------


Step 1440, Epoch 19/50, Batch 0/80
train_loss: 0.0145

Step 1445, Epoch 19/50, Batch 5/80
train_loss: 0.0154

Step 1450, Epoch 19/50, Batch 10/80
train_loss: 0.0180

Step 1455, Epoch 19/50, Batch 15/80
train_loss: 0.0196

Step 1460, Epoch 19/50, Batch 20/80
train_loss: 0.0165

Step 1465, Epoch 19/50, Batch 25/80
train_loss: 0.0121

Step 1470, Epoch 19/50, Batch 30/80
train_loss: 0.0224

Step 1475, Epoch 19/50, Batch 35/80
train_loss: 0.0189

Step 1480, Epoch 19/50, Batch 40/80
train_loss: 0.0144

Step 1485, Epoch 19/50, Batch 45/80
train_loss: 0.0141

Step 1490, Epoch 19/50, Batch 50/80
train_loss: 0.0153

Step 1495, Epoch 19/50, Batch 55/80
train_loss: 0.0148

Step 1500, Epoch 19/50, Batch 60/80
train_loss: 0.0206

Step 1505, Epoch 19/50, Batch 65/80
train_loss: 0.0166

Step 1510, Epoch 19/50, Batch 70/80
train_loss: 0.0155

Step 1515, Epoch 19/50, Batch 75/80
train_loss: 0.0117

Epoch 19/50 completed, Global Step: 1519
train_loss: 0.0117, val_loss: 0.0139

---------------------------------------------------------------------------


Step 1520, Epoch 20/50, Batch 0/80
train_loss: 0.0136

Step 1525, Epoch 20/50, Batch 5/80
train_loss: 0.0131

Step 1530, Epoch 20/50, Batch 10/80
train_loss: 0.0120

Step 1535, Epoch 20/50, Batch 15/80
train_loss: 0.0116

Step 1540, Epoch 20/50, Batch 20/80
train_loss: 0.0100

Step 1545, Epoch 20/50, Batch 25/80
train_loss: 0.0141

Step 1550, Epoch 20/50, Batch 30/80
train_loss: 0.0125

Step 1555, Epoch 20/50, Batch 35/80
train_loss: 0.0112

Step 1560, Epoch 20/50, Batch 40/80
train_loss: 0.0110

Step 1565, Epoch 20/50, Batch 45/80
train_loss: 0.0112

Step 1570, Epoch 20/50, Batch 50/80
train_loss: 0.0110

Step 1575, Epoch 20/50, Batch 55/80
train_loss: 0.0115

Step 1580, Epoch 20/50, Batch 60/80
train_loss: 0.0148

Step 1585, Epoch 20/50, Batch 65/80
train_loss: 0.0168

Step 1590, Epoch 20/50, Batch 70/80
train_loss: 0.0123

Step 1595, Epoch 20/50, Batch 75/80
train_loss: 0.0205

Epoch 20/50 completed, Global Step: 1599
train_loss: 0.0205, val_loss: 0.0139

---------------------------------------------------------------------------


Step 1600, Epoch 21/50, Batch 0/80
train_loss: 0.0140

Step 1605, Epoch 21/50, Batch 5/80
train_loss: 0.0127

Step 1610, Epoch 21/50, Batch 10/80
train_loss: 0.0105

Step 1615, Epoch 21/50, Batch 15/80
train_loss: 0.0113

Step 1620, Epoch 21/50, Batch 20/80
train_loss: 0.0140

Step 1625, Epoch 21/50, Batch 25/80
train_loss: 0.0135

Step 1630, Epoch 21/50, Batch 30/80
train_loss: 0.0117

Step 1635, Epoch 21/50, Batch 35/80
train_loss: 0.0121

Step 1640, Epoch 21/50, Batch 40/80
train_loss: 0.0120

Step 1645, Epoch 21/50, Batch 45/80
train_loss: 0.0115

Step 1650, Epoch 21/50, Batch 50/80
train_loss: 0.0117

Step 1655, Epoch 21/50, Batch 55/80
train_loss: 0.0120

Step 1660, Epoch 21/50, Batch 60/80
train_loss: 0.0140

Step 1665, Epoch 21/50, Batch 65/80
train_loss: 0.0157

Step 1670, Epoch 21/50, Batch 70/80
train_loss: 0.0153

Step 1675, Epoch 21/50, Batch 75/80
train_loss: 0.0146

Epoch 21/50 completed, Global Step: 1679
train_loss: 0.0146, val_loss: 0.0130

---------------------------------------------------------------------------


Step 1680, Epoch 22/50, Batch 0/80
train_loss: 0.0128

Step 1685, Epoch 22/50, Batch 5/80
train_loss: 0.0133

Step 1690, Epoch 22/50, Batch 10/80
train_loss: 0.0153

Step 1695, Epoch 22/50, Batch 15/80
train_loss: 0.0124

Step 1700, Epoch 22/50, Batch 20/80
train_loss: 0.0088

Step 1705, Epoch 22/50, Batch 25/80
train_loss: 0.0106

Step 1710, Epoch 22/50, Batch 30/80
train_loss: 0.0081

Step 1715, Epoch 22/50, Batch 35/80
train_loss: 0.0143

Step 1720, Epoch 22/50, Batch 40/80
train_loss: 0.0167

Step 1725, Epoch 22/50, Batch 45/80
train_loss: 0.0164

Step 1730, Epoch 22/50, Batch 50/80
train_loss: 0.0248

Step 1735, Epoch 22/50, Batch 55/80
train_loss: 0.0214

Step 1740, Epoch 22/50, Batch 60/80
train_loss: 0.0172

Step 1745, Epoch 22/50, Batch 65/80
train_loss: 0.0182

Step 1750, Epoch 22/50, Batch 70/80
train_loss: 0.0143

Step 1755, Epoch 22/50, Batch 75/80
train_loss: 0.0131

Epoch 22/50 completed, Global Step: 1759
train_loss: 0.0131, val_loss: 0.0145

---------------------------------------------------------------------------


Step 1760, Epoch 23/50, Batch 0/80
train_loss: 0.0148

Step 1765, Epoch 23/50, Batch 5/80
train_loss: 0.0135

Step 1770, Epoch 23/50, Batch 10/80
train_loss: 0.0182

Step 1775, Epoch 23/50, Batch 15/80
train_loss: 0.0167

Step 1780, Epoch 23/50, Batch 20/80
train_loss: 0.0112

Step 1785, Epoch 23/50, Batch 25/80
train_loss: 0.0109

Step 1790, Epoch 23/50, Batch 30/80
train_loss: 0.0119

Step 1795, Epoch 23/50, Batch 35/80
train_loss: 0.0129

Step 1800, Epoch 23/50, Batch 40/80
train_loss: 0.0122

Step 1805, Epoch 23/50, Batch 45/80
train_loss: 0.0132

Step 1810, Epoch 23/50, Batch 50/80
train_loss: 0.0103

Step 1815, Epoch 23/50, Batch 55/80
train_loss: 0.0110

Step 1820, Epoch 23/50, Batch 60/80
train_loss: 0.0107

Step 1825, Epoch 23/50, Batch 65/80
train_loss: 0.0121

Step 1830, Epoch 23/50, Batch 70/80
train_loss: 0.0104

Step 1835, Epoch 23/50, Batch 75/80
train_loss: 0.0106

Epoch 23/50 completed, Global Step: 1839
train_loss: 0.0106, val_loss: 0.0092

---------------------------------------------------------------------------


Step 1840, Epoch 24/50, Batch 0/80
train_loss: 0.0087

Step 1845, Epoch 24/50, Batch 5/80
train_loss: 0.0080

Step 1850, Epoch 24/50, Batch 10/80
train_loss: 0.0094

Step 1855, Epoch 24/50, Batch 15/80
train_loss: 0.0099

Step 1860, Epoch 24/50, Batch 20/80
train_loss: 0.0080

Step 1865, Epoch 24/50, Batch 25/80
train_loss: 0.0085

Step 1870, Epoch 24/50, Batch 30/80
train_loss: 0.0128

Step 1875, Epoch 24/50, Batch 35/80
train_loss: 0.0102

Step 1880, Epoch 24/50, Batch 40/80
train_loss: 0.0116

Step 1885, Epoch 24/50, Batch 45/80
train_loss: 0.0114

Step 1890, Epoch 24/50, Batch 50/80
train_loss: 0.0124

Step 1895, Epoch 24/50, Batch 55/80
train_loss: 0.0084

Step 1900, Epoch 24/50, Batch 60/80
train_loss: 0.0146

Step 1905, Epoch 24/50, Batch 65/80
train_loss: 0.0126

Step 1910, Epoch 24/50, Batch 70/80
train_loss: 0.0123

Step 1915, Epoch 24/50, Batch 75/80
train_loss: 0.0111

Epoch 24/50 completed, Global Step: 1919
train_loss: 0.0111, val_loss: 0.0112

---------------------------------------------------------------------------


Step 1920, Epoch 25/50, Batch 0/80
train_loss: 0.0110

Step 1925, Epoch 25/50, Batch 5/80
train_loss: 0.0189

Step 1930, Epoch 25/50, Batch 10/80
train_loss: 0.0170

Step 1935, Epoch 25/50, Batch 15/80
train_loss: 0.0128

Step 1940, Epoch 25/50, Batch 20/80
train_loss: 0.0116

Step 1945, Epoch 25/50, Batch 25/80
train_loss: 0.0103

Step 1950, Epoch 25/50, Batch 30/80
train_loss: 0.0093

Step 1955, Epoch 25/50, Batch 35/80
train_loss: 0.0108

Step 1960, Epoch 25/50, Batch 40/80
train_loss: 0.0141

Step 1965, Epoch 25/50, Batch 45/80
train_loss: 0.0132

Step 1970, Epoch 25/50, Batch 50/80
train_loss: 0.0126

Step 1975, Epoch 25/50, Batch 55/80
train_loss: 0.0149

Step 1980, Epoch 25/50, Batch 60/80
train_loss: 0.0101

Step 1985, Epoch 25/50, Batch 65/80
train_loss: 0.0078

Step 1990, Epoch 25/50, Batch 70/80
train_loss: 0.0103

Step 1995, Epoch 25/50, Batch 75/80
train_loss: 0.0106

Epoch 25/50 completed, Global Step: 1999
train_loss: 0.0106, val_loss: 0.0113

---------------------------------------------------------------------------


Step 2000, Epoch 26/50, Batch 0/80
train_loss: 0.0114

Step 2005, Epoch 26/50, Batch 5/80
train_loss: 0.0137

Step 2010, Epoch 26/50, Batch 10/80
train_loss: 0.0109

Step 2015, Epoch 26/50, Batch 15/80
train_loss: 0.0125

Step 2020, Epoch 26/50, Batch 20/80
train_loss: 0.0112

Step 2025, Epoch 26/50, Batch 25/80
train_loss: 0.0125

Step 2030, Epoch 26/50, Batch 30/80
train_loss: 0.0112

Step 2035, Epoch 26/50, Batch 35/80
train_loss: 0.0134

Step 2040, Epoch 26/50, Batch 40/80
train_loss: 0.0105

Step 2045, Epoch 26/50, Batch 45/80
train_loss: 0.0089

Step 2050, Epoch 26/50, Batch 50/80
train_loss: 0.0089

Step 2055, Epoch 26/50, Batch 55/80
train_loss: 0.0105

Step 2060, Epoch 26/50, Batch 60/80
train_loss: 0.0071

Step 2065, Epoch 26/50, Batch 65/80
train_loss: 0.0115

Step 2070, Epoch 26/50, Batch 70/80
train_loss: 0.0091

Step 2075, Epoch 26/50, Batch 75/80
train_loss: 0.0120

Epoch 26/50 completed, Global Step: 2079
train_loss: 0.0120, val_loss: 0.0130

---------------------------------------------------------------------------


Step 2080, Epoch 27/50, Batch 0/80
train_loss: 0.0127

Step 2085, Epoch 27/50, Batch 5/80
train_loss: 0.0105

Step 2090, Epoch 27/50, Batch 10/80
train_loss: 0.0100

Step 2095, Epoch 27/50, Batch 15/80
train_loss: 0.0159

Step 2100, Epoch 27/50, Batch 20/80
train_loss: 0.0102

Step 2105, Epoch 27/50, Batch 25/80
train_loss: 0.0136

Step 2110, Epoch 27/50, Batch 30/80
train_loss: 0.0129

Step 2115, Epoch 27/50, Batch 35/80
train_loss: 0.0196

Step 2120, Epoch 27/50, Batch 40/80
train_loss: 0.0125

Step 2125, Epoch 27/50, Batch 45/80
train_loss: 0.0115

Step 2130, Epoch 27/50, Batch 50/80
train_loss: 0.0106

Step 2135, Epoch 27/50, Batch 55/80
train_loss: 0.0111

Step 2140, Epoch 27/50, Batch 60/80
train_loss: 0.0114

Step 2145, Epoch 27/50, Batch 65/80
train_loss: 0.0119

Step 2150, Epoch 27/50, Batch 70/80
train_loss: 0.0098

Step 2155, Epoch 27/50, Batch 75/80
train_loss: 0.0114

Epoch 27/50 completed, Global Step: 2159
train_loss: 0.0114, val_loss: 0.0111

---------------------------------------------------------------------------


Step 2160, Epoch 28/50, Batch 0/80
train_loss: 0.0112

Step 2165, Epoch 28/50, Batch 5/80
train_loss: 0.0131

Step 2170, Epoch 28/50, Batch 10/80
train_loss: 0.0103

Step 2175, Epoch 28/50, Batch 15/80
train_loss: 0.0111

Step 2180, Epoch 28/50, Batch 20/80
train_loss: 0.0101

Step 2185, Epoch 28/50, Batch 25/80
train_loss: 0.0088

Step 2190, Epoch 28/50, Batch 30/80
train_loss: 0.0095

Step 2195, Epoch 28/50, Batch 35/80
train_loss: 0.0102

Step 2200, Epoch 28/50, Batch 40/80
train_loss: 0.0085

Step 2205, Epoch 28/50, Batch 45/80
train_loss: 0.0077

Step 2210, Epoch 28/50, Batch 50/80
train_loss: 0.0081

Step 2215, Epoch 28/50, Batch 55/80
train_loss: 0.0086

Step 2220, Epoch 28/50, Batch 60/80
train_loss: 0.0112

Step 2225, Epoch 28/50, Batch 65/80
train_loss: 0.0103

Step 2230, Epoch 28/50, Batch 70/80
train_loss: 0.0101

Step 2235, Epoch 28/50, Batch 75/80
train_loss: 0.0114

Epoch 28/50 completed, Global Step: 2239
train_loss: 0.0114, val_loss: 0.0119

---------------------------------------------------------------------------


Step 2240, Epoch 29/50, Batch 0/80
train_loss: 0.0116

Step 2245, Epoch 29/50, Batch 5/80
train_loss: 0.0082

Step 2250, Epoch 29/50, Batch 10/80
train_loss: 0.0086

Step 2255, Epoch 29/50, Batch 15/80
train_loss: 0.0079

Step 2260, Epoch 29/50, Batch 20/80
train_loss: 0.0088

Step 2265, Epoch 29/50, Batch 25/80
train_loss: 0.0077

Step 2270, Epoch 29/50, Batch 30/80
train_loss: 0.0127

Step 2275, Epoch 29/50, Batch 35/80
train_loss: 0.0109

Step 2280, Epoch 29/50, Batch 40/80
train_loss: 0.0139

Step 2285, Epoch 29/50, Batch 45/80
train_loss: 0.0117

Step 2290, Epoch 29/50, Batch 50/80
train_loss: 0.0118

Step 2295, Epoch 29/50, Batch 55/80
train_loss: 0.0117

Step 2300, Epoch 29/50, Batch 60/80
train_loss: 0.0130

Step 2305, Epoch 29/50, Batch 65/80
train_loss: 0.0095

Step 2310, Epoch 29/50, Batch 70/80
train_loss: 0.0117

Step 2315, Epoch 29/50, Batch 75/80
train_loss: 0.0105

Epoch 29/50 completed, Global Step: 2319
train_loss: 0.0105, val_loss: 0.0089

---------------------------------------------------------------------------


Step 2320, Epoch 30/50, Batch 0/80
train_loss: 0.0089

Step 2325, Epoch 30/50, Batch 5/80
train_loss: 0.0122

Step 2330, Epoch 30/50, Batch 10/80
train_loss: 0.0094

Step 2335, Epoch 30/50, Batch 15/80
train_loss: 0.0102

Step 2340, Epoch 30/50, Batch 20/80
train_loss: 0.0118

Step 2345, Epoch 30/50, Batch 25/80
train_loss: 0.0110

Step 2350, Epoch 30/50, Batch 30/80
train_loss: 0.0093

Step 2355, Epoch 30/50, Batch 35/80
train_loss: 0.0083

Step 2360, Epoch 30/50, Batch 40/80
train_loss: 0.0079

Step 2365, Epoch 30/50, Batch 45/80
train_loss: 0.0077

Step 2370, Epoch 30/50, Batch 50/80
train_loss: 0.0071

Step 2375, Epoch 30/50, Batch 55/80
train_loss: 0.0108

Step 2380, Epoch 30/50, Batch 60/80
train_loss: 0.0090

Step 2385, Epoch 30/50, Batch 65/80
train_loss: 0.0087

Step 2390, Epoch 30/50, Batch 70/80
train_loss: 0.0088

Step 2395, Epoch 30/50, Batch 75/80
train_loss: 0.0088

Epoch 30/50 completed, Global Step: 2399
train_loss: 0.0088, val_loss: 0.0089

---------------------------------------------------------------------------


Step 2400, Epoch 31/50, Batch 0/80
train_loss: 0.0090

Step 2405, Epoch 31/50, Batch 5/80
train_loss: 0.0066

Step 2410, Epoch 31/50, Batch 10/80
train_loss: 0.0096

Step 2415, Epoch 31/50, Batch 15/80
train_loss: 0.0130

Step 2420, Epoch 31/50, Batch 20/80
train_loss: 0.0115

Step 2425, Epoch 31/50, Batch 25/80
train_loss: 0.0074

Step 2430, Epoch 31/50, Batch 30/80
train_loss: 0.0103

Step 2435, Epoch 31/50, Batch 35/80
train_loss: 0.0092

Step 2440, Epoch 31/50, Batch 40/80
train_loss: 0.0092

Step 2445, Epoch 31/50, Batch 45/80
train_loss: 0.0110

Step 2450, Epoch 31/50, Batch 50/80
train_loss: 0.0103

Step 2455, Epoch 31/50, Batch 55/80
train_loss: 0.0096

Step 2460, Epoch 31/50, Batch 60/80
train_loss: 0.0096

Step 2465, Epoch 31/50, Batch 65/80
train_loss: 0.0119

Step 2470, Epoch 31/50, Batch 70/80
train_loss: 0.0107

Step 2475, Epoch 31/50, Batch 75/80
train_loss: 0.0088

Epoch 31/50 completed, Global Step: 2479
train_loss: 0.0088, val_loss: 0.0088

---------------------------------------------------------------------------


Step 2480, Epoch 32/50, Batch 0/80
train_loss: 0.0089

Step 2485, Epoch 32/50, Batch 5/80
train_loss: 0.0070

Step 2490, Epoch 32/50, Batch 10/80
train_loss: 0.0090

Step 2495, Epoch 32/50, Batch 15/80
train_loss: 0.0095

Step 2500, Epoch 32/50, Batch 20/80
train_loss: 0.0112

Step 2505, Epoch 32/50, Batch 25/80
train_loss: 0.0082

Step 2510, Epoch 32/50, Batch 30/80
train_loss: 0.0091

Step 2515, Epoch 32/50, Batch 35/80
train_loss: 0.0076

Step 2520, Epoch 32/50, Batch 40/80
train_loss: 0.0075

Step 2525, Epoch 32/50, Batch 45/80
train_loss: 0.0064

Step 2530, Epoch 32/50, Batch 50/80
train_loss: 0.0060

Step 2535, Epoch 32/50, Batch 55/80
train_loss: 0.0078

Step 2540, Epoch 32/50, Batch 60/80
train_loss: 0.0057

Step 2545, Epoch 32/50, Batch 65/80
train_loss: 0.0079

Step 2550, Epoch 32/50, Batch 70/80
train_loss: 0.0075

Step 2555, Epoch 32/50, Batch 75/80
train_loss: 0.0109

Epoch 32/50 completed, Global Step: 2559
train_loss: 0.0109, val_loss: 0.0104

---------------------------------------------------------------------------


Step 2560, Epoch 33/50, Batch 0/80
train_loss: 0.0103

Step 2565, Epoch 33/50, Batch 5/80
train_loss: 0.0103

Step 2570, Epoch 33/50, Batch 10/80
train_loss: 0.0107

Step 2575, Epoch 33/50, Batch 15/80
train_loss: 0.0109

Step 2580, Epoch 33/50, Batch 20/80
train_loss: 0.0147

Step 2585, Epoch 33/50, Batch 25/80
train_loss: 0.0097

Step 2590, Epoch 33/50, Batch 30/80
train_loss: 0.0127

Step 2595, Epoch 33/50, Batch 35/80
train_loss: 0.0087

Step 2600, Epoch 33/50, Batch 40/80
train_loss: 0.0071

Step 2605, Epoch 33/50, Batch 45/80
train_loss: 0.0071

Step 2610, Epoch 33/50, Batch 50/80
train_loss: 0.0086

Step 2615, Epoch 33/50, Batch 55/80
train_loss: 0.0095

Step 2620, Epoch 33/50, Batch 60/80
train_loss: 0.0059

Step 2625, Epoch 33/50, Batch 65/80
train_loss: 0.0087

Step 2630, Epoch 33/50, Batch 70/80
train_loss: 0.0110

Step 2635, Epoch 33/50, Batch 75/80
train_loss: 0.0111

Epoch 33/50 completed, Global Step: 2639
train_loss: 0.0111, val_loss: 0.0081

---------------------------------------------------------------------------


Step 2640, Epoch 34/50, Batch 0/80
train_loss: 0.0082

Step 2645, Epoch 34/50, Batch 5/80
train_loss: 0.0090

Step 2650, Epoch 34/50, Batch 10/80
train_loss: 0.0104

Step 2655, Epoch 34/50, Batch 15/80
train_loss: 0.0094

Step 2660, Epoch 34/50, Batch 20/80
train_loss: 0.0115

Step 2665, Epoch 34/50, Batch 25/80
train_loss: 0.0076

Step 2670, Epoch 34/50, Batch 30/80
train_loss: 0.0085

Step 2675, Epoch 34/50, Batch 35/80
train_loss: 0.0096

Step 2680, Epoch 34/50, Batch 40/80
train_loss: 0.0103

Step 2685, Epoch 34/50, Batch 45/80
train_loss: 0.0118

Step 2690, Epoch 34/50, Batch 50/80
train_loss: 0.0085

Step 2695, Epoch 34/50, Batch 55/80
train_loss: 0.0114

Step 2700, Epoch 34/50, Batch 60/80
train_loss: 0.0096

Step 2705, Epoch 34/50, Batch 65/80
train_loss: 0.0074

Step 2710, Epoch 34/50, Batch 70/80
train_loss: 0.0104

Step 2715, Epoch 34/50, Batch 75/80
train_loss: 0.0097

Epoch 34/50 completed, Global Step: 2719
train_loss: 0.0097, val_loss: 0.0082

---------------------------------------------------------------------------


Step 2720, Epoch 35/50, Batch 0/80
train_loss: 0.0082

Step 2725, Epoch 35/50, Batch 5/80
train_loss: 0.0071

Step 2730, Epoch 35/50, Batch 10/80
train_loss: 0.0094

Step 2735, Epoch 35/50, Batch 15/80
train_loss: 0.0081

Step 2740, Epoch 35/50, Batch 20/80
train_loss: 0.0075

Step 2745, Epoch 35/50, Batch 25/80
train_loss: 0.0092

Step 2750, Epoch 35/50, Batch 30/80
train_loss: 0.0087

Step 2755, Epoch 35/50, Batch 35/80
train_loss: 0.0097

Step 2760, Epoch 35/50, Batch 40/80
train_loss: 0.0129

Step 2765, Epoch 35/50, Batch 45/80
train_loss: 0.0094

Step 2770, Epoch 35/50, Batch 50/80
train_loss: 0.0111

Step 2775, Epoch 35/50, Batch 55/80
train_loss: 0.0089

Step 2780, Epoch 35/50, Batch 60/80
train_loss: 0.0112

Step 2785, Epoch 35/50, Batch 65/80
train_loss: 0.0086

Step 2790, Epoch 35/50, Batch 70/80
train_loss: 0.0093

Step 2795, Epoch 35/50, Batch 75/80
train_loss: 0.0075

Epoch 35/50 completed, Global Step: 2799
train_loss: 0.0075, val_loss: 0.0102

---------------------------------------------------------------------------


Step 2800, Epoch 36/50, Batch 0/80
train_loss: 0.0097

Step 2805, Epoch 36/50, Batch 5/80
train_loss: 0.0091

Step 2810, Epoch 36/50, Batch 10/80
train_loss: 0.0090

Step 2815, Epoch 36/50, Batch 15/80
train_loss: 0.0086

Step 2820, Epoch 36/50, Batch 20/80
train_loss: 0.0092

Step 2825, Epoch 36/50, Batch 25/80
train_loss: 0.0091

Step 2830, Epoch 36/50, Batch 30/80
train_loss: 0.0075

Step 2835, Epoch 36/50, Batch 35/80
train_loss: 0.0090

Step 2840, Epoch 36/50, Batch 40/80
train_loss: 0.0084

Step 2845, Epoch 36/50, Batch 45/80
train_loss: 0.0085

Step 2850, Epoch 36/50, Batch 50/80
train_loss: 0.0075

Step 2855, Epoch 36/50, Batch 55/80
train_loss: 0.0067

Step 2860, Epoch 36/50, Batch 60/80
train_loss: 0.0080

Step 2865, Epoch 36/50, Batch 65/80
train_loss: 0.0068

Step 2870, Epoch 36/50, Batch 70/80
train_loss: 0.0098

Step 2875, Epoch 36/50, Batch 75/80
train_loss: 0.0079

Epoch 36/50 completed, Global Step: 2879
train_loss: 0.0079, val_loss: 0.0077

---------------------------------------------------------------------------


Step 2880, Epoch 37/50, Batch 0/80
train_loss: 0.0076

Step 2885, Epoch 37/50, Batch 5/80
train_loss: 0.0087

Step 2890, Epoch 37/50, Batch 10/80
train_loss: 0.0085

Step 2895, Epoch 37/50, Batch 15/80
train_loss: 0.0105

Step 2900, Epoch 37/50, Batch 20/80
train_loss: 0.0093

Step 2905, Epoch 37/50, Batch 25/80
train_loss: 0.0079

Step 2910, Epoch 37/50, Batch 30/80
train_loss: 0.0085

Step 2915, Epoch 37/50, Batch 35/80
train_loss: 0.0105

Step 2920, Epoch 37/50, Batch 40/80
train_loss: 0.0086

Step 2925, Epoch 37/50, Batch 45/80
train_loss: 0.0106

Step 2930, Epoch 37/50, Batch 50/80
train_loss: 0.0106

Step 2935, Epoch 37/50, Batch 55/80
train_loss: 0.0097

Step 2940, Epoch 37/50, Batch 60/80
train_loss: 0.0114

Step 2945, Epoch 37/50, Batch 65/80
train_loss: 0.0108

Step 2950, Epoch 37/50, Batch 70/80
train_loss: 0.0087

Step 2955, Epoch 37/50, Batch 75/80
train_loss: 0.0084

Epoch 37/50 completed, Global Step: 2959
train_loss: 0.0084, val_loss: 0.0102

---------------------------------------------------------------------------


Step 2960, Epoch 38/50, Batch 0/80
train_loss: 0.0102

Step 2965, Epoch 38/50, Batch 5/80
train_loss: 0.0111

Step 2970, Epoch 38/50, Batch 10/80
train_loss: 0.0093

Step 2975, Epoch 38/50, Batch 15/80
train_loss: 0.0080

Step 2980, Epoch 38/50, Batch 20/80
train_loss: 0.0092

Step 2985, Epoch 38/50, Batch 25/80
train_loss: 0.0084

Step 2990, Epoch 38/50, Batch 30/80
train_loss: 0.0089

Step 2995, Epoch 38/50, Batch 35/80
train_loss: 0.0070

Step 3000, Epoch 38/50, Batch 40/80
train_loss: 0.0080

Step 3005, Epoch 38/50, Batch 45/80
train_loss: 0.0105

Step 3010, Epoch 38/50, Batch 50/80
train_loss: 0.0116

Step 3015, Epoch 38/50, Batch 55/80
train_loss: 0.0103

Step 3020, Epoch 38/50, Batch 60/80
train_loss: 0.0084

Step 3025, Epoch 38/50, Batch 65/80
train_loss: 0.0103

Step 3030, Epoch 38/50, Batch 70/80
train_loss: 0.0091

Step 3035, Epoch 38/50, Batch 75/80
train_loss: 0.0104

Epoch 38/50 completed, Global Step: 3039
train_loss: 0.0104, val_loss: 0.0099

---------------------------------------------------------------------------


Step 3040, Epoch 39/50, Batch 0/80
train_loss: 0.0100

Step 3045, Epoch 39/50, Batch 5/80
train_loss: 0.0110

Step 3050, Epoch 39/50, Batch 10/80
train_loss: 0.0080

Step 3055, Epoch 39/50, Batch 15/80
train_loss: 0.0100

Step 3060, Epoch 39/50, Batch 20/80
train_loss: 0.0091

Step 3065, Epoch 39/50, Batch 25/80
train_loss: 0.0092

Step 3070, Epoch 39/50, Batch 30/80
train_loss: 0.0091

Step 3075, Epoch 39/50, Batch 35/80
train_loss: 0.0104

Step 3080, Epoch 39/50, Batch 40/80
train_loss: 0.0087

Step 3085, Epoch 39/50, Batch 45/80
train_loss: 0.0118

Step 3090, Epoch 39/50, Batch 50/80
train_loss: 0.0106

Step 3095, Epoch 39/50, Batch 55/80
train_loss: 0.0098

Step 3100, Epoch 39/50, Batch 60/80
train_loss: 0.0090

Step 3105, Epoch 39/50, Batch 65/80
train_loss: 0.0098

Step 3110, Epoch 39/50, Batch 70/80
train_loss: 0.0085

Step 3115, Epoch 39/50, Batch 75/80
train_loss: 0.0077

Epoch 39/50 completed, Global Step: 3119
train_loss: 0.0077, val_loss: 0.0064

---------------------------------------------------------------------------


Step 3120, Epoch 40/50, Batch 0/80
train_loss: 0.0064

Step 3125, Epoch 40/50, Batch 5/80
train_loss: 0.0064

Step 3130, Epoch 40/50, Batch 10/80
train_loss: 0.0059

Step 3135, Epoch 40/50, Batch 15/80
train_loss: 0.0062

Step 3140, Epoch 40/50, Batch 20/80
train_loss: 0.0069

Step 3145, Epoch 40/50, Batch 25/80
train_loss: 0.0072

Step 3150, Epoch 40/50, Batch 30/80
train_loss: 0.0085

Step 3155, Epoch 40/50, Batch 35/80
train_loss: 0.0056

Step 3160, Epoch 40/50, Batch 40/80
train_loss: 0.0074

Step 3165, Epoch 40/50, Batch 45/80
train_loss: 0.0068

Step 3170, Epoch 40/50, Batch 50/80
train_loss: 0.0078

Step 3175, Epoch 40/50, Batch 55/80
train_loss: 0.0072

Step 3180, Epoch 40/50, Batch 60/80
train_loss: 0.0060

Step 3185, Epoch 40/50, Batch 65/80
train_loss: 0.0069

Step 3190, Epoch 40/50, Batch 70/80
train_loss: 0.0077

Step 3195, Epoch 40/50, Batch 75/80
train_loss: 0.0089

Epoch 40/50 completed, Global Step: 3199
train_loss: 0.0089, val_loss: 0.0106

---------------------------------------------------------------------------


Step 3200, Epoch 41/50, Batch 0/80
train_loss: 0.0108

Step 3205, Epoch 41/50, Batch 5/80
train_loss: 0.0076

Step 3210, Epoch 41/50, Batch 10/80
train_loss: 0.0084

Step 3215, Epoch 41/50, Batch 15/80
train_loss: 0.0104

Step 3220, Epoch 41/50, Batch 20/80
train_loss: 0.0077

Step 3225, Epoch 41/50, Batch 25/80
train_loss: 0.0096

Step 3230, Epoch 41/50, Batch 30/80
train_loss: 0.0082

Step 3235, Epoch 41/50, Batch 35/80
train_loss: 0.0095

Step 3240, Epoch 41/50, Batch 40/80
train_loss: 0.0073

Step 3245, Epoch 41/50, Batch 45/80
train_loss: 0.0068

Step 3250, Epoch 41/50, Batch 50/80
train_loss: 0.0077

Step 3255, Epoch 41/50, Batch 55/80
train_loss: 0.0085

Step 3260, Epoch 41/50, Batch 60/80
train_loss: 0.0078

Step 3265, Epoch 41/50, Batch 65/80
train_loss: 0.0066

Step 3270, Epoch 41/50, Batch 70/80
train_loss: 0.0076

Step 3275, Epoch 41/50, Batch 75/80
train_loss: 0.0064

Epoch 41/50 completed, Global Step: 3279
train_loss: 0.0064, val_loss: 0.0109

---------------------------------------------------------------------------


Step 3280, Epoch 42/50, Batch 0/80
train_loss: 0.0107

Step 3285, Epoch 42/50, Batch 5/80
train_loss: 0.0073

Step 3290, Epoch 42/50, Batch 10/80
train_loss: 0.0066

Step 3295, Epoch 42/50, Batch 15/80
train_loss: 0.0057

Step 3300, Epoch 42/50, Batch 20/80
train_loss: 0.0064

Step 3305, Epoch 42/50, Batch 25/80
train_loss: 0.0054

Step 3310, Epoch 42/50, Batch 30/80
train_loss: 0.0050

Step 3315, Epoch 42/50, Batch 35/80
train_loss: 0.0056

Step 3320, Epoch 42/50, Batch 40/80
train_loss: 0.0069

Step 3325, Epoch 42/50, Batch 45/80
train_loss: 0.0064

Step 3330, Epoch 42/50, Batch 50/80
train_loss: 0.0064

Step 3335, Epoch 42/50, Batch 55/80
train_loss: 0.0073

Step 3340, Epoch 42/50, Batch 60/80
train_loss: 0.0063

Step 3345, Epoch 42/50, Batch 65/80
train_loss: 0.0081

Step 3350, Epoch 42/50, Batch 70/80
train_loss: 0.0092

Step 3355, Epoch 42/50, Batch 75/80
train_loss: 0.0070

Epoch 42/50 completed, Global Step: 3359
train_loss: 0.0070, val_loss: 0.0094

---------------------------------------------------------------------------


Step 3360, Epoch 43/50, Batch 0/80
train_loss: 0.0094

Step 3365, Epoch 43/50, Batch 5/80
train_loss: 0.0082

Step 3370, Epoch 43/50, Batch 10/80
train_loss: 0.0062

Step 3375, Epoch 43/50, Batch 15/80
train_loss: 0.0062

Step 3380, Epoch 43/50, Batch 20/80
train_loss: 0.0063

Step 3385, Epoch 43/50, Batch 25/80
train_loss: 0.0081

Step 3390, Epoch 43/50, Batch 30/80
train_loss: 0.0078

Step 3395, Epoch 43/50, Batch 35/80
train_loss: 0.0078

Step 3400, Epoch 43/50, Batch 40/80
train_loss: 0.0065

Step 3405, Epoch 43/50, Batch 45/80
train_loss: 0.0070

Step 3410, Epoch 43/50, Batch 50/80
train_loss: 0.0117

Step 3415, Epoch 43/50, Batch 55/80
train_loss: 0.0091

Step 3420, Epoch 43/50, Batch 60/80
train_loss: 0.0066

Step 3425, Epoch 43/50, Batch 65/80
train_loss: 0.0072

Step 3430, Epoch 43/50, Batch 70/80
train_loss: 0.0091

Step 3435, Epoch 43/50, Batch 75/80
train_loss: 0.0078

Epoch 43/50 completed, Global Step: 3439
train_loss: 0.0078, val_loss: 0.0098

---------------------------------------------------------------------------


Step 3440, Epoch 44/50, Batch 0/80
train_loss: 0.0099

Step 3445, Epoch 44/50, Batch 5/80
train_loss: 0.0067

Step 3450, Epoch 44/50, Batch 10/80
train_loss: 0.0074

Step 3455, Epoch 44/50, Batch 15/80
train_loss: 0.0072

Step 3460, Epoch 44/50, Batch 20/80
train_loss: 0.0081

Step 3465, Epoch 44/50, Batch 25/80
train_loss: 0.0077

Step 3470, Epoch 44/50, Batch 30/80
train_loss: 0.0073

Step 3475, Epoch 44/50, Batch 35/80
train_loss: 0.0078

Step 3480, Epoch 44/50, Batch 40/80
train_loss: 0.0079

Step 3485, Epoch 44/50, Batch 45/80
train_loss: 0.0094

Step 3490, Epoch 44/50, Batch 50/80
train_loss: 0.0090

Step 3495, Epoch 44/50, Batch 55/80
train_loss: 0.0072

Step 3500, Epoch 44/50, Batch 60/80
train_loss: 0.0071

Step 3505, Epoch 44/50, Batch 65/80
train_loss: 0.0070

Step 3510, Epoch 44/50, Batch 70/80
train_loss: 0.0098

Step 3515, Epoch 44/50, Batch 75/80
train_loss: 0.0074

Epoch 44/50 completed, Global Step: 3519
train_loss: 0.0074, val_loss: 0.0052

---------------------------------------------------------------------------


Step 3520, Epoch 45/50, Batch 0/80
train_loss: 0.0050

Step 3525, Epoch 45/50, Batch 5/80
train_loss: 0.0073

Step 3530, Epoch 45/50, Batch 10/80
train_loss: 0.0075

Step 3535, Epoch 45/50, Batch 15/80
train_loss: 0.0059

Step 3540, Epoch 45/50, Batch 20/80
train_loss: 0.0062

Step 3545, Epoch 45/50, Batch 25/80
train_loss: 0.0056

Step 3550, Epoch 45/50, Batch 30/80
train_loss: 0.0061

Step 3555, Epoch 45/50, Batch 35/80
train_loss: 0.0057

Step 3560, Epoch 45/50, Batch 40/80
train_loss: 0.0058

Step 3565, Epoch 45/50, Batch 45/80
train_loss: 0.0070

Step 3570, Epoch 45/50, Batch 50/80
train_loss: 0.0070

Step 3575, Epoch 45/50, Batch 55/80
train_loss: 0.0080

Step 3580, Epoch 45/50, Batch 60/80
train_loss: 0.0065

Step 3585, Epoch 45/50, Batch 65/80
train_loss: 0.0098

Step 3590, Epoch 45/50, Batch 70/80
train_loss: 0.0082

Step 3595, Epoch 45/50, Batch 75/80
train_loss: 0.0068

Epoch 45/50 completed, Global Step: 3599
train_loss: 0.0068, val_loss: 0.0069

---------------------------------------------------------------------------


Step 3600, Epoch 46/50, Batch 0/80
train_loss: 0.0066

Step 3605, Epoch 46/50, Batch 5/80
train_loss: 0.0055

Step 3610, Epoch 46/50, Batch 10/80
train_loss: 0.0101

Step 3615, Epoch 46/50, Batch 15/80
train_loss: 0.0089

Step 3620, Epoch 46/50, Batch 20/80
train_loss: 0.0086

Step 3625, Epoch 46/50, Batch 25/80
train_loss: 0.0114

Step 3630, Epoch 46/50, Batch 30/80
train_loss: 0.0098

Step 3635, Epoch 46/50, Batch 35/80
train_loss: 0.0101

Step 3640, Epoch 46/50, Batch 40/80
train_loss: 0.0081

Step 3645, Epoch 46/50, Batch 45/80
train_loss: 0.0106

Step 3650, Epoch 46/50, Batch 50/80
train_loss: 0.0081

Step 3655, Epoch 46/50, Batch 55/80
train_loss: 0.0086

Step 3660, Epoch 46/50, Batch 60/80
train_loss: 0.0089

Step 3665, Epoch 46/50, Batch 65/80
train_loss: 0.0098

Step 3670, Epoch 46/50, Batch 70/80
train_loss: 0.0087

Step 3675, Epoch 46/50, Batch 75/80
train_loss: 0.0063

Epoch 46/50 completed, Global Step: 3679
train_loss: 0.0063, val_loss: 0.0075

---------------------------------------------------------------------------


Step 3680, Epoch 47/50, Batch 0/80
train_loss: 0.0075

Step 3685, Epoch 47/50, Batch 5/80
train_loss: 0.0069

Step 3690, Epoch 47/50, Batch 10/80
train_loss: 0.0086

Step 3695, Epoch 47/50, Batch 15/80
train_loss: 0.0071

Step 3700, Epoch 47/50, Batch 20/80
train_loss: 0.0067

Step 3705, Epoch 47/50, Batch 25/80
train_loss: 0.0075

Step 3710, Epoch 47/50, Batch 30/80
train_loss: 0.0085

Step 3715, Epoch 47/50, Batch 35/80
train_loss: 0.0079

Step 3720, Epoch 47/50, Batch 40/80
train_loss: 0.0063

Step 3725, Epoch 47/50, Batch 45/80
train_loss: 0.0090

Step 3730, Epoch 47/50, Batch 50/80
train_loss: 0.0092

Step 3735, Epoch 47/50, Batch 55/80
train_loss: 0.0103

Step 3740, Epoch 47/50, Batch 60/80
train_loss: 0.0101

Step 3745, Epoch 47/50, Batch 65/80
train_loss: 0.0085

Step 3750, Epoch 47/50, Batch 70/80
train_loss: 0.0074

Step 3755, Epoch 47/50, Batch 75/80
train_loss: 0.0094

Epoch 47/50 completed, Global Step: 3759
train_loss: 0.0094, val_loss: 0.0070

---------------------------------------------------------------------------


Step 3760, Epoch 48/50, Batch 0/80
train_loss: 0.0069

Step 3765, Epoch 48/50, Batch 5/80
train_loss: 0.0086

Step 3770, Epoch 48/50, Batch 10/80
train_loss: 0.0075

Step 3775, Epoch 48/50, Batch 15/80
train_loss: 0.0074

Step 3780, Epoch 48/50, Batch 20/80
train_loss: 0.0087

Step 3785, Epoch 48/50, Batch 25/80
train_loss: 0.0069

Step 3790, Epoch 48/50, Batch 30/80
train_loss: 0.0071

Step 3795, Epoch 48/50, Batch 35/80
train_loss: 0.0058

Step 3800, Epoch 48/50, Batch 40/80
train_loss: 0.0061

Step 3805, Epoch 48/50, Batch 45/80
train_loss: 0.0053

Step 3810, Epoch 48/50, Batch 50/80
train_loss: 0.0055

Step 3815, Epoch 48/50, Batch 55/80
train_loss: 0.0057

Step 3820, Epoch 48/50, Batch 60/80
train_loss: 0.0057

Step 3825, Epoch 48/50, Batch 65/80
train_loss: 0.0046

Step 3830, Epoch 48/50, Batch 70/80
train_loss: 0.0054

Step 3835, Epoch 48/50, Batch 75/80
train_loss: 0.0053

Epoch 48/50 completed, Global Step: 3839
train_loss: 0.0053, val_loss: 0.0053

---------------------------------------------------------------------------


Step 3840, Epoch 49/50, Batch 0/80
train_loss: 0.0052

Step 3845, Epoch 49/50, Batch 5/80
train_loss: 0.0065

Step 3850, Epoch 49/50, Batch 10/80
train_loss: 0.0103

Step 3855, Epoch 49/50, Batch 15/80
train_loss: 0.0072

Step 3860, Epoch 49/50, Batch 20/80
train_loss: 0.0069

Step 3865, Epoch 49/50, Batch 25/80
train_loss: 0.0073

Step 3870, Epoch 49/50, Batch 30/80
train_loss: 0.0076

Step 3875, Epoch 49/50, Batch 35/80
train_loss: 0.0093

Step 3880, Epoch 49/50, Batch 40/80
train_loss: 0.0075

Step 3885, Epoch 49/50, Batch 45/80
train_loss: 0.0079

Step 3890, Epoch 49/50, Batch 50/80
train_loss: 0.0061

Step 3895, Epoch 49/50, Batch 55/80
train_loss: 0.0067

Step 3900, Epoch 49/50, Batch 60/80
train_loss: 0.0062

Step 3905, Epoch 49/50, Batch 65/80
train_loss: 0.0063

Step 3910, Epoch 49/50, Batch 70/80
train_loss: 0.0066

Step 3915, Epoch 49/50, Batch 75/80
train_loss: 0.0090

Epoch 49/50 completed, Global Step: 3919
train_loss: 0.0090, val_loss: 0.0063

---------------------------------------------------------------------------


Step 3920, Epoch 50/50, Batch 0/80
train_loss: 0.0062

Step 3925, Epoch 50/50, Batch 5/80
train_loss: 0.0093

Step 3930, Epoch 50/50, Batch 10/80
train_loss: 0.0070

Step 3935, Epoch 50/50, Batch 15/80
train_loss: 0.0095

Step 3940, Epoch 50/50, Batch 20/80
train_loss: 0.0102

Step 3945, Epoch 50/50, Batch 25/80
train_loss: 0.0064

Step 3950, Epoch 50/50, Batch 30/80
train_loss: 0.0067

Step 3955, Epoch 50/50, Batch 35/80
train_loss: 0.0059

Step 3960, Epoch 50/50, Batch 40/80
train_loss: 0.0051

Step 3965, Epoch 50/50, Batch 45/80
train_loss: 0.0056

Step 3970, Epoch 50/50, Batch 50/80
train_loss: 0.0066

Step 3975, Epoch 50/50, Batch 55/80
train_loss: 0.0049

Step 3980, Epoch 50/50, Batch 60/80
train_loss: 0.0061

Step 3985, Epoch 50/50, Batch 65/80
train_loss: 0.0072

Step 3990, Epoch 50/50, Batch 70/80
train_loss: 0.0066

Step 3995, Epoch 50/50, Batch 75/80
train_loss: 0.0083

Epoch 50/50 completed, Global Step: 3999
train_loss: 0.0083, val_loss: 0.0080

---------------------------------------------------------------------------


Training completed in 1313.80 seconds or 21.90 minutes or 0.36494373566574523 hours.
Total training steps: 3999

Training completed for model '[m005_(apv+G)]-gru_dec_1.1'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m005_(apv+G)]-gru_dec_1.1...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.4645' for [m005_(apv+G)]-gru_dec_1.1...

Decoder output plot for rep '1001.4645' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.1374' for [m005_(apv+G)]-gru_dec_1.1...

Decoder output plot for rep '1001.1374' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1


---------------------------------------------------------------------------

TESTING TRAINED DECODER MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1\checkpoints:

['best-model-epoch=43-val_loss=0.0052.ckpt']

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1\test

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Trained Decoder Model Loaded for testing.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                       | 0/? [00:00<?, ?it/s]
Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                  | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|     | 0/10 [00:00<?, ?it/s]
Found rep_num = 1001.0001 in batch 0 of epoch 43. Decoder output plot will be made for this data.
Testing DataLoader 0:  10%|1| 1/10 [00:00<00:02,  3.02iTesting DataLoader 0:  20%|2| 2/10 [00:00<00:01,  4.51iTesting DataLoader 0:  30%|3| 3/10 [00:00<00:01,  5.09iTesting DataLoader 0:  40%|4| 4/10 [00:00<00:01,  5.45iTesting DataLoader 0:  50%|5| 5/10 [00:00<00:00,  5.82iTesting DataLoader 0:  60%|6| 6/10 [00:00<00:00,  6.27iTesting DataLoader 0:  70%|7| 7/10 [00:01<00:00,  6.33iTesting DataLoader 0:  80%|8| 8/10 [00:01<00:00,  6.29iTesting DataLoader 0:  90%|9| 9/10 [00:01<00:00,  6.28iTesting DataLoader 0: 100%|#| 10/10 [00:01<00:00,  6.40
Testing completed in 1.56 seconds or 0.03 minutes or 0.0004344821638531155 hours.

test_loss: 0.0063

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.0001' for [m005_(apv+G)]-gru_dec_1.1...

Decoder output plot for rep '1001.0001' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M005\scene_1\decoder\train\etypes=1\m005\apv\set_G\D=gru\tswp_0\[m005_(apv+G)]-gru_dec_1.1\test

Testing DataLoader 0: 100%|#| 10/10 [00:04<00:00,  2.23

        Test metric               DataLoader 0       

         test_loss            0.006326478905975819   


===========================================================================

Decoder model '[m005_(apv+G)]-gru_dec_1.1' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-21 21:06:10
