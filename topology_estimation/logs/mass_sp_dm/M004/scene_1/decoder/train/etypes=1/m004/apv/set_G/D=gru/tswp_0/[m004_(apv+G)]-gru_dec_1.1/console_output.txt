=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m004_(apv+G)]-gru_dec_1.1
Start Time: 2025-09-21 10:00:22
End Time: 2025-09-21 10:13:14

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting decoder model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]

Node group name: m004
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.

Target rep_num 1001.0001 found at index 0. This sample will be included in the test set.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 450/500 [OK=450, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 9
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (4, 4)
     n1   n2   n3   n4
n1  0.0  1.0  0.0  0.0
n2  1.0  0.0  1.0  0.0
n3  0.0  1.0  0.0  1.0
n4  0.0  0.0  1.0  0.0


**Receiver relation matrix** => shape: (12, 4)
      n1   n2   n3   n4
e12  0.0  1.0  0.0  0.0
e13  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0
e21  1.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0
e24  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0
e34  0.0  0.0  0.0  1.0
e41  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0
e43  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (12, 4)
      n1   n2   n3   n4
e12  1.0  0.0  0.0  0.0
e13  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0
e21  0.0  1.0  0.0  0.0
e23  0.0  1.0  0.0  0.0
e24  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0
e32  0.0  0.0  1.0  0.0
e34  0.0  0.0  1.0  0.0
e41  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0
e43  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 1
msg_out_size: 64
edge_mlp_config: [[64, 'tanh'], [64, 'tanh']]
out_mlp_config: [[64, 'relu'], [64, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
n_dims: 3

Decoder run parameters:
-------------------------
skip_first_edge_type: False
pred_steps: 10
is_burn_in: True
final_pred_steps: 30
is_dynamic_graph: False
temp: 1.0
is_hard: True
show_conf_band: False

'[m004_(apv+G)]-gru_dec_1.1' already exists in the log path 'C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1'.
(a) Overwrite exsiting version, (b) create new version, (c) stop training (Choose 'a', 'b' or 'c'):  Are you sure you want to remove the '[m004_(apv+G)]-gru_dec_1.1' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1? (y/n): Overwrote '[m004_(apv+G)]-gru_dec_1.1' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1.
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1

---------------------------------------------------------------------------

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Training parameters set to: 
lr=0.001, 
optimizer=adam, 
loss_type=mse

---------------------------------------------------------------------------

Decoder Model Initialized with the following configurations:

Decoder Model Summary:
Decoder(
  (edge_mlp_fn): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=64, bias=True)
    (hidden_u): Linear(in_features=64, out_features=64, bias=True)
    (input_r): Linear(in_features=3, out_features=64, bias=True)
    (hidden_r): Linear(in_features=64, out_features=64, bias=True)
    (input_h): Linear(in_features=3, out_features=64, bias=True)
    (hidden_h): Linear(in_features=64, out_features=64, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=64, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=64, out_features=3, bias=True)
)

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Step 0, Epoch 1/30, Batch 0/80
train_loss: 1.0235

Step 5, Epoch 1/30, Batch 5/80
train_loss: 0.0486

Step 10, Epoch 1/30, Batch 10/80
train_loss: 0.0614

Step 15, Epoch 1/30, Batch 15/80
train_loss: 0.0426

Step 20, Epoch 1/30, Batch 20/80
train_loss: 0.0264

Step 25, Epoch 1/30, Batch 25/80
train_loss: 0.0259

Step 30, Epoch 1/30, Batch 30/80
train_loss: 0.0255

Step 35, Epoch 1/30, Batch 35/80
train_loss: 0.0245

Step 40, Epoch 1/30, Batch 40/80
train_loss: 0.0233

Step 45, Epoch 1/30, Batch 45/80
train_loss: 0.0224

Step 50, Epoch 1/30, Batch 50/80
train_loss: 0.0218

Step 55, Epoch 1/30, Batch 55/80
train_loss: 0.0214

Step 60, Epoch 1/30, Batch 60/80
train_loss: 0.0206

Step 65, Epoch 1/30, Batch 65/80
train_loss: 0.0200

Step 70, Epoch 1/30, Batch 70/80
train_loss: 0.0197

Step 75, Epoch 1/30, Batch 75/80
train_loss: 0.0193

Epoch 1/30 completed, Global Step: 79
train_loss: 0.0193, val_loss: 0.0190

---------------------------------------------------------------------------


Step 80, Epoch 2/30, Batch 0/80
train_loss: 0.0190

Step 85, Epoch 2/30, Batch 5/80
train_loss: 0.0188

Step 90, Epoch 2/30, Batch 10/80
train_loss: 0.0190

Step 95, Epoch 2/30, Batch 15/80
train_loss: 0.0185

Step 100, Epoch 2/30, Batch 20/80
train_loss: 0.0184

Step 105, Epoch 2/30, Batch 25/80
train_loss: 0.0184

Step 110, Epoch 2/30, Batch 30/80
train_loss: 0.0178

Step 115, Epoch 2/30, Batch 35/80
train_loss: 0.0183

Step 120, Epoch 2/30, Batch 40/80
train_loss: 0.0182

Step 125, Epoch 2/30, Batch 45/80
train_loss: 0.0178

Step 130, Epoch 2/30, Batch 50/80
train_loss: 0.0179

Step 135, Epoch 2/30, Batch 55/80
train_loss: 0.0179

Step 140, Epoch 2/30, Batch 60/80
train_loss: 0.0177

Step 145, Epoch 2/30, Batch 65/80
train_loss: 0.0173

Step 150, Epoch 2/30, Batch 70/80
train_loss: 0.0174

Step 155, Epoch 2/30, Batch 75/80
train_loss: 0.0174

Epoch 2/30 completed, Global Step: 159
train_loss: 0.0174, val_loss: 0.0173

---------------------------------------------------------------------------


Step 160, Epoch 3/30, Batch 0/80
train_loss: 0.0174

Step 165, Epoch 3/30, Batch 5/80
train_loss: 0.0170

Step 170, Epoch 3/30, Batch 10/80
train_loss: 0.0170

Step 175, Epoch 3/30, Batch 15/80
train_loss: 0.0171

Step 180, Epoch 3/30, Batch 20/80
train_loss: 0.0172

Step 185, Epoch 3/30, Batch 25/80
train_loss: 0.0167

Step 190, Epoch 3/30, Batch 30/80
train_loss: 0.0171

Step 195, Epoch 3/30, Batch 35/80
train_loss: 0.0166

Step 200, Epoch 3/30, Batch 40/80
train_loss: 0.0168

Step 205, Epoch 3/30, Batch 45/80
train_loss: 0.0166

Step 210, Epoch 3/30, Batch 50/80
train_loss: 0.0165

Step 215, Epoch 3/30, Batch 55/80
train_loss: 0.0165

Step 220, Epoch 3/30, Batch 60/80
train_loss: 0.0164

Step 225, Epoch 3/30, Batch 65/80
train_loss: 0.0165

Step 230, Epoch 3/30, Batch 70/80
train_loss: 0.0164

Step 235, Epoch 3/30, Batch 75/80
train_loss: 0.0161

Epoch 3/30 completed, Global Step: 239
train_loss: 0.0161, val_loss: 0.0162

---------------------------------------------------------------------------


Step 240, Epoch 4/30, Batch 0/80
train_loss: 0.0162

Step 245, Epoch 4/30, Batch 5/80
train_loss: 0.0166

Step 250, Epoch 4/30, Batch 10/80
train_loss: 0.0165

Step 255, Epoch 4/30, Batch 15/80
train_loss: 0.0162

Step 260, Epoch 4/30, Batch 20/80
train_loss: 0.0161

Step 265, Epoch 4/30, Batch 25/80
train_loss: 0.0160

Step 270, Epoch 4/30, Batch 30/80
train_loss: 0.0159

Step 275, Epoch 4/30, Batch 35/80
train_loss: 0.0161

Step 280, Epoch 4/30, Batch 40/80
train_loss: 0.0160

Step 285, Epoch 4/30, Batch 45/80
train_loss: 0.0159

Step 290, Epoch 4/30, Batch 50/80
train_loss: 0.0160

Step 295, Epoch 4/30, Batch 55/80
train_loss: 0.0158

Step 300, Epoch 4/30, Batch 60/80
train_loss: 0.0159

Step 305, Epoch 4/30, Batch 65/80
train_loss: 0.0157

Step 310, Epoch 4/30, Batch 70/80
train_loss: 0.0154

Step 315, Epoch 4/30, Batch 75/80
train_loss: 0.0155

Epoch 4/30 completed, Global Step: 319
train_loss: 0.0155, val_loss: 0.0155

---------------------------------------------------------------------------


Step 320, Epoch 5/30, Batch 0/80
train_loss: 0.0154

Step 325, Epoch 5/30, Batch 5/80
train_loss: 0.0155

Step 330, Epoch 5/30, Batch 10/80
train_loss: 0.0155

Step 335, Epoch 5/30, Batch 15/80
train_loss: 0.0153

Step 340, Epoch 5/30, Batch 20/80
train_loss: 0.0156

Step 345, Epoch 5/30, Batch 25/80
train_loss: 0.0154

Step 350, Epoch 5/30, Batch 30/80
train_loss: 0.0154

Step 355, Epoch 5/30, Batch 35/80
train_loss: 0.0153

Step 360, Epoch 5/30, Batch 40/80
train_loss: 0.0151

Step 365, Epoch 5/30, Batch 45/80
train_loss: 0.0152

Step 370, Epoch 5/30, Batch 50/80
train_loss: 0.0150

Step 375, Epoch 5/30, Batch 55/80
train_loss: 0.0151

Step 380, Epoch 5/30, Batch 60/80
train_loss: 0.0150

Step 385, Epoch 5/30, Batch 65/80
train_loss: 0.0149

Step 390, Epoch 5/30, Batch 70/80
train_loss: 0.0147

Step 395, Epoch 5/30, Batch 75/80
train_loss: 0.0145

Epoch 5/30 completed, Global Step: 399
train_loss: 0.0145, val_loss: 0.0146

---------------------------------------------------------------------------


Step 400, Epoch 6/30, Batch 0/80
train_loss: 0.0147

Step 405, Epoch 6/30, Batch 5/80
train_loss: 0.0148

Step 410, Epoch 6/30, Batch 10/80
train_loss: 0.0144

Step 415, Epoch 6/30, Batch 15/80
train_loss: 0.0142

Step 420, Epoch 6/30, Batch 20/80
train_loss: 0.0143

Step 425, Epoch 6/30, Batch 25/80
train_loss: 0.0143

Step 430, Epoch 6/30, Batch 30/80
train_loss: 0.0140

Step 435, Epoch 6/30, Batch 35/80
train_loss: 0.0140

Step 440, Epoch 6/30, Batch 40/80
train_loss: 0.0140

Step 445, Epoch 6/30, Batch 45/80
train_loss: 0.0137

Step 450, Epoch 6/30, Batch 50/80
train_loss: 0.0141

Step 455, Epoch 6/30, Batch 55/80
train_loss: 0.0141

Step 460, Epoch 6/30, Batch 60/80
train_loss: 0.0139

Step 465, Epoch 6/30, Batch 65/80
train_loss: 0.0137

Step 470, Epoch 6/30, Batch 70/80
train_loss: 0.0137

Step 475, Epoch 6/30, Batch 75/80
train_loss: 0.0137

Epoch 6/30 completed, Global Step: 479
train_loss: 0.0137, val_loss: 0.0135

---------------------------------------------------------------------------


Step 480, Epoch 7/30, Batch 0/80
train_loss: 0.0137

Step 485, Epoch 7/30, Batch 5/80
train_loss: 0.0136

Step 490, Epoch 7/30, Batch 10/80
train_loss: 0.0167

Step 495, Epoch 7/30, Batch 15/80
train_loss: 0.0139

Step 500, Epoch 7/30, Batch 20/80
train_loss: 0.0142

Step 505, Epoch 7/30, Batch 25/80
train_loss: 0.0138

Step 510, Epoch 7/30, Batch 30/80
train_loss: 0.0135

Step 515, Epoch 7/30, Batch 35/80
train_loss: 0.0133

Step 520, Epoch 7/30, Batch 40/80
train_loss: 0.0133

Step 525, Epoch 7/30, Batch 45/80
train_loss: 0.0129

Step 530, Epoch 7/30, Batch 50/80
train_loss: 0.0129

Step 535, Epoch 7/30, Batch 55/80
train_loss: 0.0130

Step 540, Epoch 7/30, Batch 60/80
train_loss: 0.0131

Step 545, Epoch 7/30, Batch 65/80
train_loss: 0.0131

Step 550, Epoch 7/30, Batch 70/80
train_loss: 0.0129

Step 555, Epoch 7/30, Batch 75/80
train_loss: 0.0126

Epoch 7/30 completed, Global Step: 559
train_loss: 0.0126, val_loss: 0.0125

---------------------------------------------------------------------------


Step 560, Epoch 8/30, Batch 0/80
train_loss: 0.0126

Step 565, Epoch 8/30, Batch 5/80
train_loss: 0.0123

Step 570, Epoch 8/30, Batch 10/80
train_loss: 0.0120

Step 575, Epoch 8/30, Batch 15/80
train_loss: 0.0121

Step 580, Epoch 8/30, Batch 20/80
train_loss: 0.0124

Step 585, Epoch 8/30, Batch 25/80
train_loss: 0.0126

Step 590, Epoch 8/30, Batch 30/80
train_loss: 0.0133

Step 595, Epoch 8/30, Batch 35/80
train_loss: 0.0121

Step 600, Epoch 8/30, Batch 40/80
train_loss: 0.0123

Step 605, Epoch 8/30, Batch 45/80
train_loss: 0.0119

Step 610, Epoch 8/30, Batch 50/80
train_loss: 0.0116

Step 615, Epoch 8/30, Batch 55/80
train_loss: 0.0113

Step 620, Epoch 8/30, Batch 60/80
train_loss: 0.0112

Step 625, Epoch 8/30, Batch 65/80
train_loss: 0.0317

Step 630, Epoch 8/30, Batch 70/80
train_loss: 0.0137

Step 635, Epoch 8/30, Batch 75/80
train_loss: 0.0147

Epoch 8/30 completed, Global Step: 639
train_loss: 0.0147, val_loss: 0.0135

---------------------------------------------------------------------------


Step 640, Epoch 9/30, Batch 0/80
train_loss: 0.0136

Step 645, Epoch 9/30, Batch 5/80
train_loss: 0.0131

Step 650, Epoch 9/30, Batch 10/80
train_loss: 0.0131

Step 655, Epoch 9/30, Batch 15/80
train_loss: 0.0125

Step 660, Epoch 9/30, Batch 20/80
train_loss: 0.0122

Step 665, Epoch 9/30, Batch 25/80
train_loss: 0.0120

Step 670, Epoch 9/30, Batch 30/80
train_loss: 0.0119

Step 675, Epoch 9/30, Batch 35/80
train_loss: 0.0119

Step 680, Epoch 9/30, Batch 40/80
train_loss: 0.0114

Step 685, Epoch 9/30, Batch 45/80
train_loss: 0.0113

Step 690, Epoch 9/30, Batch 50/80
train_loss: 0.0110

Step 695, Epoch 9/30, Batch 55/80
train_loss: 0.0106

Step 700, Epoch 9/30, Batch 60/80
train_loss: 0.0105

Step 705, Epoch 9/30, Batch 65/80
train_loss: 0.0103

Step 710, Epoch 9/30, Batch 70/80
train_loss: 0.0101

Step 715, Epoch 9/30, Batch 75/80
train_loss: 0.0103

Epoch 9/30 completed, Global Step: 719
train_loss: 0.0103, val_loss: 0.0110

---------------------------------------------------------------------------


Step 720, Epoch 10/30, Batch 0/80
train_loss: 0.0111

Step 725, Epoch 10/30, Batch 5/80
train_loss: 0.0110

Step 730, Epoch 10/30, Batch 10/80
train_loss: 0.0103

Step 735, Epoch 10/30, Batch 15/80
train_loss: 0.0099

Step 740, Epoch 10/30, Batch 20/80
train_loss: 0.0098

Step 745, Epoch 10/30, Batch 25/80
train_loss: 0.0095

Step 750, Epoch 10/30, Batch 30/80
train_loss: 0.0095

Step 755, Epoch 10/30, Batch 35/80
train_loss: 0.0091

Step 760, Epoch 10/30, Batch 40/80
train_loss: 0.0093

Step 765, Epoch 10/30, Batch 45/80
train_loss: 0.0091

Step 770, Epoch 10/30, Batch 50/80
train_loss: 0.0090

Step 775, Epoch 10/30, Batch 55/80
train_loss: 0.0087

Step 780, Epoch 10/30, Batch 60/80
train_loss: 0.0111

Step 785, Epoch 10/30, Batch 65/80
train_loss: 0.0088

Step 790, Epoch 10/30, Batch 70/80
train_loss: 0.0096

Step 795, Epoch 10/30, Batch 75/80
train_loss: 0.0087

Epoch 10/30 completed, Global Step: 799
train_loss: 0.0087, val_loss: 0.0085

---------------------------------------------------------------------------


Step 800, Epoch 11/30, Batch 0/80
train_loss: 0.0086

Step 805, Epoch 11/30, Batch 5/80
train_loss: 0.0083

Step 810, Epoch 11/30, Batch 10/80
train_loss: 0.0083

Step 815, Epoch 11/30, Batch 15/80
train_loss: 0.0080

Step 820, Epoch 11/30, Batch 20/80
train_loss: 0.0079

Step 825, Epoch 11/30, Batch 25/80
train_loss: 0.0082

Step 830, Epoch 11/30, Batch 30/80
train_loss: 0.0082

Step 835, Epoch 11/30, Batch 35/80
train_loss: 0.0077

Step 840, Epoch 11/30, Batch 40/80
train_loss: 0.0076

Step 845, Epoch 11/30, Batch 45/80
train_loss: 0.0074

Step 850, Epoch 11/30, Batch 50/80
train_loss: 0.0070

Step 855, Epoch 11/30, Batch 55/80
train_loss: 0.0079

Step 860, Epoch 11/30, Batch 60/80
train_loss: 0.0067

Step 865, Epoch 11/30, Batch 65/80
train_loss: 0.0063

Step 870, Epoch 11/30, Batch 70/80
train_loss: 0.0061

Step 875, Epoch 11/30, Batch 75/80
train_loss: 0.0058

Epoch 11/30 completed, Global Step: 879
train_loss: 0.0058, val_loss: 0.0055

---------------------------------------------------------------------------


Step 880, Epoch 12/30, Batch 0/80
train_loss: 0.0055

Step 885, Epoch 12/30, Batch 5/80
train_loss: 0.0061

Step 890, Epoch 12/30, Batch 10/80
train_loss: 0.0053

Step 895, Epoch 12/30, Batch 15/80
train_loss: 0.0052

Step 900, Epoch 12/30, Batch 20/80
train_loss: 0.0048

Step 905, Epoch 12/30, Batch 25/80
train_loss: 0.0044

Step 910, Epoch 12/30, Batch 30/80
train_loss: 0.0040

Step 915, Epoch 12/30, Batch 35/80
train_loss: 0.0040

Step 920, Epoch 12/30, Batch 40/80
train_loss: 0.0038

Step 925, Epoch 12/30, Batch 45/80
train_loss: 0.0037

Step 930, Epoch 12/30, Batch 50/80
train_loss: 0.0034

Step 935, Epoch 12/30, Batch 55/80
train_loss: 0.0028

Step 940, Epoch 12/30, Batch 60/80
train_loss: 0.0044

Step 945, Epoch 12/30, Batch 65/80
train_loss: 0.0031

Step 950, Epoch 12/30, Batch 70/80
train_loss: 0.0030

Step 955, Epoch 12/30, Batch 75/80
train_loss: 0.0029

Epoch 12/30 completed, Global Step: 959
train_loss: 0.0029, val_loss: 0.0029

---------------------------------------------------------------------------


Step 960, Epoch 13/30, Batch 0/80
train_loss: 0.0031

Step 965, Epoch 13/30, Batch 5/80
train_loss: 0.0025

Step 970, Epoch 13/30, Batch 10/80
train_loss: 0.0025

Step 975, Epoch 13/30, Batch 15/80
train_loss: 0.0027

Step 980, Epoch 13/30, Batch 20/80
train_loss: 0.0022

Step 985, Epoch 13/30, Batch 25/80
train_loss: 0.0024

Step 990, Epoch 13/30, Batch 30/80
train_loss: 0.0021

Step 995, Epoch 13/30, Batch 35/80
train_loss: 0.0028

Step 1000, Epoch 13/30, Batch 40/80
train_loss: 0.0024

Step 1005, Epoch 13/30, Batch 45/80
train_loss: 0.0019

Step 1010, Epoch 13/30, Batch 50/80
train_loss: 0.0023

Step 1015, Epoch 13/30, Batch 55/80
train_loss: 0.0023

Step 1020, Epoch 13/30, Batch 60/80
train_loss: 0.0018

Step 1025, Epoch 13/30, Batch 65/80
train_loss: 0.0023

Step 1030, Epoch 13/30, Batch 70/80
train_loss: 0.0019

Step 1035, Epoch 13/30, Batch 75/80
train_loss: 0.0017

Epoch 13/30 completed, Global Step: 1039
train_loss: 0.0017, val_loss: 0.0022

---------------------------------------------------------------------------


Step 1040, Epoch 14/30, Batch 0/80
train_loss: 0.0022

Step 1045, Epoch 14/30, Batch 5/80
train_loss: 0.0017

Step 1050, Epoch 14/30, Batch 10/80
train_loss: 0.0020

Step 1055, Epoch 14/30, Batch 15/80
train_loss: 0.0016

Step 1060, Epoch 14/30, Batch 20/80
train_loss: 0.0019

Step 1065, Epoch 14/30, Batch 25/80
train_loss: 0.0014

Step 1070, Epoch 14/30, Batch 30/80
train_loss: 0.0015

Step 1075, Epoch 14/30, Batch 35/80
train_loss: 0.0014

Step 1080, Epoch 14/30, Batch 40/80
train_loss: 0.0014

Step 1085, Epoch 14/30, Batch 45/80
train_loss: 0.0014

Step 1090, Epoch 14/30, Batch 50/80
train_loss: 0.0014

Step 1095, Epoch 14/30, Batch 55/80
train_loss: 0.0013

Step 1100, Epoch 14/30, Batch 60/80
train_loss: 0.0016

Step 1105, Epoch 14/30, Batch 65/80
train_loss: 0.0013

Step 1110, Epoch 14/30, Batch 70/80
train_loss: 0.0013

Step 1115, Epoch 14/30, Batch 75/80
train_loss: 0.0012

Epoch 14/30 completed, Global Step: 1119
train_loss: 0.0012, val_loss: 0.0012

---------------------------------------------------------------------------


Step 1120, Epoch 15/30, Batch 0/80
train_loss: 0.0012

Step 1125, Epoch 15/30, Batch 5/80
train_loss: 0.0012

Step 1130, Epoch 15/30, Batch 10/80
train_loss: 0.0014

Step 1135, Epoch 15/30, Batch 15/80
train_loss: 0.0015

Step 1140, Epoch 15/30, Batch 20/80
train_loss: 0.0014

Step 1145, Epoch 15/30, Batch 25/80
train_loss: 0.0013

Step 1150, Epoch 15/30, Batch 30/80
train_loss: 0.0012

Step 1155, Epoch 15/30, Batch 35/80
train_loss: 0.0011

Step 1160, Epoch 15/30, Batch 40/80
train_loss: 0.0011

Step 1165, Epoch 15/30, Batch 45/80
train_loss: 0.0012

Step 1170, Epoch 15/30, Batch 50/80
train_loss: 0.0012

Step 1175, Epoch 15/30, Batch 55/80
train_loss: 0.0015

Step 1180, Epoch 15/30, Batch 60/80
train_loss: 0.0014

Step 1185, Epoch 15/30, Batch 65/80
train_loss: 0.0013

Step 1190, Epoch 15/30, Batch 70/80
train_loss: 0.0013

Step 1195, Epoch 15/30, Batch 75/80
train_loss: 0.0014

Epoch 15/30 completed, Global Step: 1199
train_loss: 0.0014, val_loss: 0.0013

---------------------------------------------------------------------------


Step 1200, Epoch 16/30, Batch 0/80
train_loss: 0.0013

Step 1205, Epoch 16/30, Batch 5/80
train_loss: 0.0014

Step 1210, Epoch 16/30, Batch 10/80
train_loss: 0.0013

Step 1215, Epoch 16/30, Batch 15/80
train_loss: 0.0014

Step 1220, Epoch 16/30, Batch 20/80
train_loss: 0.0010

Step 1225, Epoch 16/30, Batch 25/80
train_loss: 0.0011

Step 1230, Epoch 16/30, Batch 30/80
train_loss: 0.0011

Step 1235, Epoch 16/30, Batch 35/80
train_loss: 0.0010

Step 1240, Epoch 16/30, Batch 40/80
train_loss: 0.0010

Step 1245, Epoch 16/30, Batch 45/80
train_loss: 0.0010

Step 1250, Epoch 16/30, Batch 50/80
train_loss: 0.0009

Step 1255, Epoch 16/30, Batch 55/80
train_loss: 0.0017

Step 1260, Epoch 16/30, Batch 60/80
train_loss: 0.0010

Step 1265, Epoch 16/30, Batch 65/80
train_loss: 0.0010

Step 1270, Epoch 16/30, Batch 70/80
train_loss: 0.0012

Step 1275, Epoch 16/30, Batch 75/80
train_loss: 0.0015

Epoch 16/30 completed, Global Step: 1279
train_loss: 0.0015, val_loss: 0.0014

---------------------------------------------------------------------------


Step 1280, Epoch 17/30, Batch 0/80
train_loss: 0.0014

Step 1285, Epoch 17/30, Batch 5/80
train_loss: 0.0010

Step 1290, Epoch 17/30, Batch 10/80
train_loss: 0.0009

Step 1295, Epoch 17/30, Batch 15/80
train_loss: 0.0010

Step 1300, Epoch 17/30, Batch 20/80
train_loss: 0.0009

Step 1305, Epoch 17/30, Batch 25/80
train_loss: 0.0012

Step 1310, Epoch 17/30, Batch 30/80
train_loss: 0.0010

Step 1315, Epoch 17/30, Batch 35/80
train_loss: 0.0011

Step 1320, Epoch 17/30, Batch 40/80
train_loss: 0.0014

Step 1325, Epoch 17/30, Batch 45/80
train_loss: 0.0010

Step 1330, Epoch 17/30, Batch 50/80
train_loss: 0.0011

Step 1335, Epoch 17/30, Batch 55/80
train_loss: 0.0011

Step 1340, Epoch 17/30, Batch 60/80
train_loss: 0.0009

Step 1345, Epoch 17/30, Batch 65/80
train_loss: 0.0010

Step 1350, Epoch 17/30, Batch 70/80
train_loss: 0.0010

Step 1355, Epoch 17/30, Batch 75/80
train_loss: 0.0010

Epoch 17/30 completed, Global Step: 1359
train_loss: 0.0010, val_loss: 0.0009

---------------------------------------------------------------------------


Step 1360, Epoch 18/30, Batch 0/80
train_loss: 0.0010

Step 1365, Epoch 18/30, Batch 5/80
train_loss: 0.0009

Step 1370, Epoch 18/30, Batch 10/80
train_loss: 0.0009

Step 1375, Epoch 18/30, Batch 15/80
train_loss: 0.0009

Step 1380, Epoch 18/30, Batch 20/80
train_loss: 0.0008

Step 1385, Epoch 18/30, Batch 25/80
train_loss: 0.0008

Step 1390, Epoch 18/30, Batch 30/80
train_loss: 0.0008

Step 1395, Epoch 18/30, Batch 35/80
train_loss: 0.0009

Step 1400, Epoch 18/30, Batch 40/80
train_loss: 0.0009

Step 1405, Epoch 18/30, Batch 45/80
train_loss: 0.0008

Step 1410, Epoch 18/30, Batch 50/80
train_loss: 0.0012

Step 1415, Epoch 18/30, Batch 55/80
train_loss: 0.0008

Step 1420, Epoch 18/30, Batch 60/80
train_loss: 0.0008

Step 1425, Epoch 18/30, Batch 65/80
train_loss: 0.0008

Step 1430, Epoch 18/30, Batch 70/80
train_loss: 0.0008

Step 1435, Epoch 18/30, Batch 75/80
train_loss: 0.0008

Epoch 18/30 completed, Global Step: 1439
train_loss: 0.0008, val_loss: 0.0008

---------------------------------------------------------------------------


Step 1440, Epoch 19/30, Batch 0/80
train_loss: 0.0008

Step 1445, Epoch 19/30, Batch 5/80
train_loss: 0.0011

Step 1450, Epoch 19/30, Batch 10/80
train_loss: 0.0011

Step 1455, Epoch 19/30, Batch 15/80
train_loss: 0.0011

Step 1460, Epoch 19/30, Batch 20/80
train_loss: 0.0008

Step 1465, Epoch 19/30, Batch 25/80
train_loss: 0.0007

Step 1470, Epoch 19/30, Batch 30/80
train_loss: 0.0009

Step 1475, Epoch 19/30, Batch 35/80
train_loss: 0.0009

Step 1480, Epoch 19/30, Batch 40/80
train_loss: 0.0008

Step 1485, Epoch 19/30, Batch 45/80
train_loss: 0.0008

Step 1490, Epoch 19/30, Batch 50/80
train_loss: 0.0007

Step 1495, Epoch 19/30, Batch 55/80
train_loss: 0.0007

Step 1500, Epoch 19/30, Batch 60/80
train_loss: 0.0009

Step 1505, Epoch 19/30, Batch 65/80
train_loss: 0.0007

Step 1510, Epoch 19/30, Batch 70/80
train_loss: 0.0011

Step 1515, Epoch 19/30, Batch 75/80
train_loss: 0.0010

Epoch 19/30 completed, Global Step: 1519
train_loss: 0.0010, val_loss: 0.0007

---------------------------------------------------------------------------


Step 1520, Epoch 20/30, Batch 0/80
train_loss: 0.0007

Step 1525, Epoch 20/30, Batch 5/80
train_loss: 0.0007

Step 1530, Epoch 20/30, Batch 10/80
train_loss: 0.0007

Step 1535, Epoch 20/30, Batch 15/80
train_loss: 0.0009

Step 1540, Epoch 20/30, Batch 20/80
train_loss: 0.0008

Step 1545, Epoch 20/30, Batch 25/80
train_loss: 0.0007

Step 1550, Epoch 20/30, Batch 30/80
train_loss: 0.0007

Step 1555, Epoch 20/30, Batch 35/80
train_loss: 0.0007

Step 1560, Epoch 20/30, Batch 40/80
train_loss: 0.0007

Step 1565, Epoch 20/30, Batch 45/80
train_loss: 0.0007

Step 1570, Epoch 20/30, Batch 50/80
train_loss: 0.0007

Step 1575, Epoch 20/30, Batch 55/80
train_loss: 0.0012

Step 1580, Epoch 20/30, Batch 60/80
train_loss: 0.0008

Step 1585, Epoch 20/30, Batch 65/80
train_loss: 0.0010

Step 1590, Epoch 20/30, Batch 70/80
train_loss: 0.0007

Step 1595, Epoch 20/30, Batch 75/80
train_loss: 0.0010

Epoch 20/30 completed, Global Step: 1599
train_loss: 0.0010, val_loss: 0.0008

---------------------------------------------------------------------------


Step 1600, Epoch 21/30, Batch 0/80
train_loss: 0.0008

Step 1605, Epoch 21/30, Batch 5/80
train_loss: 0.0008

Step 1610, Epoch 21/30, Batch 10/80
train_loss: 0.0007

Step 1615, Epoch 21/30, Batch 15/80
train_loss: 0.0008

Step 1620, Epoch 21/30, Batch 20/80
train_loss: 0.0009

Step 1625, Epoch 21/30, Batch 25/80
train_loss: 0.0008

Step 1630, Epoch 21/30, Batch 30/80
train_loss: 0.0009

Step 1635, Epoch 21/30, Batch 35/80
train_loss: 0.0007

Step 1640, Epoch 21/30, Batch 40/80
train_loss: 0.0007

Step 1645, Epoch 21/30, Batch 45/80
train_loss: 0.0006

Step 1650, Epoch 21/30, Batch 50/80
train_loss: 0.0007

Step 1655, Epoch 21/30, Batch 55/80
train_loss: 0.0006

Step 1660, Epoch 21/30, Batch 60/80
train_loss: 0.0012

Step 1665, Epoch 21/30, Batch 65/80
train_loss: 0.0010

Step 1670, Epoch 21/30, Batch 70/80
train_loss: 0.0006

Step 1675, Epoch 21/30, Batch 75/80
train_loss: 0.0009

Epoch 21/30 completed, Global Step: 1679
train_loss: 0.0009, val_loss: 0.0007

---------------------------------------------------------------------------


Step 1680, Epoch 22/30, Batch 0/80
train_loss: 0.0007

Step 1685, Epoch 22/30, Batch 5/80
train_loss: 0.0008

Step 1690, Epoch 22/30, Batch 10/80
train_loss: 0.0007

Step 1695, Epoch 22/30, Batch 15/80
train_loss: 0.0008

Step 1700, Epoch 22/30, Batch 20/80
train_loss: 0.0006

Step 1705, Epoch 22/30, Batch 25/80
train_loss: 0.0008

Step 1710, Epoch 22/30, Batch 30/80
train_loss: 0.0007

Step 1715, Epoch 22/30, Batch 35/80
train_loss: 0.0006

Step 1720, Epoch 22/30, Batch 40/80
train_loss: 0.0006

Step 1725, Epoch 22/30, Batch 45/80
train_loss: 0.0006

Step 1730, Epoch 22/30, Batch 50/80
train_loss: 0.0006

Step 1735, Epoch 22/30, Batch 55/80
train_loss: 0.0006

Step 1740, Epoch 22/30, Batch 60/80
train_loss: 0.0006

Step 1745, Epoch 22/30, Batch 65/80
train_loss: 0.0006

Step 1750, Epoch 22/30, Batch 70/80
train_loss: 0.0006

Step 1755, Epoch 22/30, Batch 75/80
train_loss: 0.0009

Epoch 22/30 completed, Global Step: 1759
train_loss: 0.0009, val_loss: 0.0006

---------------------------------------------------------------------------


Step 1760, Epoch 23/30, Batch 0/80
train_loss: 0.0005

Step 1765, Epoch 23/30, Batch 5/80
train_loss: 0.0006

Step 1770, Epoch 23/30, Batch 10/80
train_loss: 0.0006

Step 1775, Epoch 23/30, Batch 15/80
train_loss: 0.0006

Step 1780, Epoch 23/30, Batch 20/80
train_loss: 0.0006

Step 1785, Epoch 23/30, Batch 25/80
train_loss: 0.0005

Step 1790, Epoch 23/30, Batch 30/80
train_loss: 0.0005

Step 1795, Epoch 23/30, Batch 35/80
train_loss: 0.0007

Step 1800, Epoch 23/30, Batch 40/80
train_loss: 0.0007

Step 1805, Epoch 23/30, Batch 45/80
train_loss: 0.0007

Step 1810, Epoch 23/30, Batch 50/80
train_loss: 0.0006

Step 1815, Epoch 23/30, Batch 55/80
train_loss: 0.0006

Step 1820, Epoch 23/30, Batch 60/80
train_loss: 0.0005

Step 1825, Epoch 23/30, Batch 65/80
train_loss: 0.0006

Step 1830, Epoch 23/30, Batch 70/80
train_loss: 0.0005

Step 1835, Epoch 23/30, Batch 75/80
train_loss: 0.0007

Epoch 23/30 completed, Global Step: 1839
train_loss: 0.0007, val_loss: 0.0008

---------------------------------------------------------------------------


Step 1840, Epoch 24/30, Batch 0/80
train_loss: 0.0008

Step 1845, Epoch 24/30, Batch 5/80
train_loss: 0.0006

Step 1850, Epoch 24/30, Batch 10/80
train_loss: 0.0006

Step 1855, Epoch 24/30, Batch 15/80
train_loss: 0.0006

Step 1860, Epoch 24/30, Batch 20/80
train_loss: 0.0005

Step 1865, Epoch 24/30, Batch 25/80
train_loss: 0.0005

Step 1870, Epoch 24/30, Batch 30/80
train_loss: 0.0017

Step 1875, Epoch 24/30, Batch 35/80
train_loss: 0.0009

Step 1880, Epoch 24/30, Batch 40/80
train_loss: 0.0007

Step 1885, Epoch 24/30, Batch 45/80
train_loss: 0.0010

Step 1890, Epoch 24/30, Batch 50/80
train_loss: 0.0006

Step 1895, Epoch 24/30, Batch 55/80
train_loss: 0.0007

Step 1900, Epoch 24/30, Batch 60/80
train_loss: 0.0006

Step 1905, Epoch 24/30, Batch 65/80
train_loss: 0.0005

Step 1910, Epoch 24/30, Batch 70/80
train_loss: 0.0005

Step 1915, Epoch 24/30, Batch 75/80
train_loss: 0.0005

Epoch 24/30 completed, Global Step: 1919
train_loss: 0.0005, val_loss: 0.0005

---------------------------------------------------------------------------


Step 1920, Epoch 25/30, Batch 0/80
train_loss: 0.0005

Step 1925, Epoch 25/30, Batch 5/80
train_loss: 0.0005

Step 1930, Epoch 25/30, Batch 10/80
train_loss: 0.0005

Step 1935, Epoch 25/30, Batch 15/80
train_loss: 0.0006

Step 1940, Epoch 25/30, Batch 20/80
train_loss: 0.0008

Step 1945, Epoch 25/30, Batch 25/80
train_loss: 0.0007

Step 1950, Epoch 25/30, Batch 30/80
train_loss: 0.0005

Step 1955, Epoch 25/30, Batch 35/80
train_loss: 0.0005

Step 1960, Epoch 25/30, Batch 40/80
train_loss: 0.0007

Step 1965, Epoch 25/30, Batch 45/80
train_loss: 0.0007

Step 1970, Epoch 25/30, Batch 50/80
train_loss: 0.0005

Step 1975, Epoch 25/30, Batch 55/80
train_loss: 0.0005

Step 1980, Epoch 25/30, Batch 60/80
train_loss: 0.0005

Step 1985, Epoch 25/30, Batch 65/80
train_loss: 0.0006

Step 1990, Epoch 25/30, Batch 70/80
train_loss: 0.0006

Step 1995, Epoch 25/30, Batch 75/80
train_loss: 0.0005

Epoch 25/30 completed, Global Step: 1999
train_loss: 0.0005, val_loss: 0.0005

---------------------------------------------------------------------------


Step 2000, Epoch 26/30, Batch 0/80
train_loss: 0.0005

Step 2005, Epoch 26/30, Batch 5/80
train_loss: 0.0008

Step 2010, Epoch 26/30, Batch 10/80
train_loss: 0.0006

Step 2015, Epoch 26/30, Batch 15/80
train_loss: 0.0005

Step 2020, Epoch 26/30, Batch 20/80
train_loss: 0.0005

Step 2025, Epoch 26/30, Batch 25/80
train_loss: 0.0005

Step 2030, Epoch 26/30, Batch 30/80
train_loss: 0.0005

Step 2035, Epoch 26/30, Batch 35/80
train_loss: 0.0005

Step 2040, Epoch 26/30, Batch 40/80
train_loss: 0.0005

Step 2045, Epoch 26/30, Batch 45/80
train_loss: 0.0004

Step 2050, Epoch 26/30, Batch 50/80
train_loss: 0.0007

Step 2055, Epoch 26/30, Batch 55/80
train_loss: 0.0005

Step 2060, Epoch 26/30, Batch 60/80
train_loss: 0.0008

Step 2065, Epoch 26/30, Batch 65/80
train_loss: 0.0007

Step 2070, Epoch 26/30, Batch 70/80
train_loss: 0.0005

Step 2075, Epoch 26/30, Batch 75/80
train_loss: 0.0004

Epoch 26/30 completed, Global Step: 2079
train_loss: 0.0004, val_loss: 0.0005

---------------------------------------------------------------------------


Step 2080, Epoch 27/30, Batch 0/80
train_loss: 0.0005

Step 2085, Epoch 27/30, Batch 5/80
train_loss: 0.0004

Step 2090, Epoch 27/30, Batch 10/80
train_loss: 0.0005

Step 2095, Epoch 27/30, Batch 15/80
train_loss: 0.0007

Step 2100, Epoch 27/30, Batch 20/80
train_loss: 0.0005

Step 2105, Epoch 27/30, Batch 25/80
train_loss: 0.0005

Step 2110, Epoch 27/30, Batch 30/80
train_loss: 0.0005

Step 2115, Epoch 27/30, Batch 35/80
train_loss: 0.0005

Step 2120, Epoch 27/30, Batch 40/80
train_loss: 0.0004

Step 2125, Epoch 27/30, Batch 45/80
train_loss: 0.0006

Step 2130, Epoch 27/30, Batch 50/80
train_loss: 0.0005

Step 2135, Epoch 27/30, Batch 55/80
train_loss: 0.0004

Step 2140, Epoch 27/30, Batch 60/80
train_loss: 0.0005

Step 2145, Epoch 27/30, Batch 65/80
train_loss: 0.0004

Step 2150, Epoch 27/30, Batch 70/80
train_loss: 0.0004

Step 2155, Epoch 27/30, Batch 75/80
train_loss: 0.0004

Epoch 27/30 completed, Global Step: 2159
train_loss: 0.0004, val_loss: 0.0007

---------------------------------------------------------------------------


Step 2160, Epoch 28/30, Batch 0/80
train_loss: 0.0007

Step 2165, Epoch 28/30, Batch 5/80
train_loss: 0.0006

Step 2170, Epoch 28/30, Batch 10/80
train_loss: 0.0008

Step 2175, Epoch 28/30, Batch 15/80
train_loss: 0.0005

Step 2180, Epoch 28/30, Batch 20/80
train_loss: 0.0005

Step 2185, Epoch 28/30, Batch 25/80
train_loss: 0.0005

Step 2190, Epoch 28/30, Batch 30/80
train_loss: 0.0005

Step 2195, Epoch 28/30, Batch 35/80
train_loss: 0.0004

Step 2200, Epoch 28/30, Batch 40/80
train_loss: 0.0004

Step 2205, Epoch 28/30, Batch 45/80
train_loss: 0.0007

Step 2210, Epoch 28/30, Batch 50/80
train_loss: 0.0005

Step 2215, Epoch 28/30, Batch 55/80
train_loss: 0.0004

Step 2220, Epoch 28/30, Batch 60/80
train_loss: 0.0004

Step 2225, Epoch 28/30, Batch 65/80
train_loss: 0.0004

Step 2230, Epoch 28/30, Batch 70/80
train_loss: 0.0004

Step 2235, Epoch 28/30, Batch 75/80
train_loss: 0.0004

Epoch 28/30 completed, Global Step: 2239
train_loss: 0.0004, val_loss: 0.0005

---------------------------------------------------------------------------


Step 2240, Epoch 29/30, Batch 0/80
train_loss: 0.0005

Step 2245, Epoch 29/30, Batch 5/80
train_loss: 0.0006

Step 2250, Epoch 29/30, Batch 10/80
train_loss: 0.0005

Step 2255, Epoch 29/30, Batch 15/80
train_loss: 0.0006

Step 2260, Epoch 29/30, Batch 20/80
train_loss: 0.0006

Step 2265, Epoch 29/30, Batch 25/80
train_loss: 0.0007

Step 2270, Epoch 29/30, Batch 30/80
train_loss: 0.0005

Step 2275, Epoch 29/30, Batch 35/80
train_loss: 0.0004

Step 2280, Epoch 29/30, Batch 40/80
train_loss: 0.0004

Step 2285, Epoch 29/30, Batch 45/80
train_loss: 0.0004

Step 2290, Epoch 29/30, Batch 50/80
train_loss: 0.0004

Step 2295, Epoch 29/30, Batch 55/80
train_loss: 0.0004

Step 2300, Epoch 29/30, Batch 60/80
train_loss: 0.0005

Step 2305, Epoch 29/30, Batch 65/80
train_loss: 0.0006

Step 2310, Epoch 29/30, Batch 70/80
train_loss: 0.0004

Step 2315, Epoch 29/30, Batch 75/80
train_loss: 0.0004

Epoch 29/30 completed, Global Step: 2319
train_loss: 0.0004, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2320, Epoch 30/30, Batch 0/80
train_loss: 0.0004

Step 2325, Epoch 30/30, Batch 5/80
train_loss: 0.0004

Step 2330, Epoch 30/30, Batch 10/80
train_loss: 0.0005

Step 2335, Epoch 30/30, Batch 15/80
train_loss: 0.0009

Step 2340, Epoch 30/30, Batch 20/80
train_loss: 0.0005

Step 2345, Epoch 30/30, Batch 25/80
train_loss: 0.0004

Step 2350, Epoch 30/30, Batch 30/80
train_loss: 0.0004

Step 2355, Epoch 30/30, Batch 35/80
train_loss: 0.0004

Step 2360, Epoch 30/30, Batch 40/80
train_loss: 0.0005

Step 2365, Epoch 30/30, Batch 45/80
train_loss: 0.0005

Step 2370, Epoch 30/30, Batch 50/80
train_loss: 0.0005

Step 2375, Epoch 30/30, Batch 55/80
train_loss: 0.0004

Step 2380, Epoch 30/30, Batch 60/80
train_loss: 0.0004

Step 2385, Epoch 30/30, Batch 65/80
train_loss: 0.0004

Step 2390, Epoch 30/30, Batch 70/80
train_loss: 0.0004

Step 2395, Epoch 30/30, Batch 75/80
train_loss: 0.0004

Epoch 30/30 completed, Global Step: 2399
train_loss: 0.0004, val_loss: 0.0004

---------------------------------------------------------------------------


Training completed in 743.09 seconds or 12.38 minutes or 0.20641404350598652 hours.
Total training steps: 2399

Training completed for model '[m004_(apv+G)]-gru_dec_1.1'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m004_(apv+G)]-gru_dec_1.1...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.1538' for [m004_(apv+G)]-gru_dec_1.1...

Decoder output plot for rep '1001.1538' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.3965' for [m004_(apv+G)]-gru_dec_1.1...

Decoder output plot for rep '1001.3965' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1


---------------------------------------------------------------------------

TESTING TRAINED DECODER MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1\checkpoints:

['best-model-epoch=28-val_loss=0.0004.ckpt']

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1\test

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Trained Decoder Model Loaded for testing.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                                                                   | 0/? [00:00<?, ?it/s]
Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                                                              | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|                                                 | 0/10 [00:00<?, ?it/s]
Found rep_num = 1001.0001 in batch 0 of epoch 28. Decoder output plot will be made for this data.
Testing DataLoader 0:  10%|####1                                    | 1/10 [00:00<00:01,  5.13it/s]Testing DataLoader 0:  20%|########2                                | 2/10 [00:00<00:01,  6.71it/s]Testing DataLoader 0:  30%|############2                            | 3/10 [00:00<00:00,  7.50it/s]Testing DataLoader 0:  40%|################4                        | 4/10 [00:00<00:00,  8.21it/s]Testing DataLoader 0:  50%|####################5                    | 5/10 [00:00<00:00,  8.74it/s]Testing DataLoader 0:  60%|########################5                | 6/10 [00:00<00:00,  8.71it/s]Testing DataLoader 0:  70%|############################7            | 7/10 [00:00<00:00,  8.69it/s]Testing DataLoader 0:  80%|################################8        | 8/10 [00:00<00:00,  8.62it/s]Testing DataLoader 0:  90%|####################################9    | 9/10 [00:01<00:00,  8.69it/s]Testing DataLoader 0: 100%|########################################| 10/10 [00:01<00:00,  8.59it/s]
Testing completed in 1.17 seconds or 0.02 minutes or 0.00032372368706597225 hours.

test_loss: 0.0004

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.0001' for [m004_(apv+G)]-gru_dec_1.1...

Decoder output plot for rep '1001.0001' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.1\test

Testing DataLoader 0: 100%|########################################| 10/10 [00:03<00:00,  3.15it/s]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │   0.0004263470764271915   │
└───────────────────────────┴───────────────────────────┘

===========================================================================

Decoder model '[m004_(apv+G)]-gru_dec_1.1' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-21 10:13:14
