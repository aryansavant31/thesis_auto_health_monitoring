=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: (m004)-(gru)_decoder_1.1
Start Time: 2025-08-24 19:55:09
End Time: 2025-08-24 19:56:29

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting decoder model training...

'Train' type dataset selected:


ds_subtype selections:

(<ds_subtype_num>) <ds_subtype> : [<augments>]
---------------------------------------------
>> Healthy configs
(1) series_tp_(fs=1000)    : [OG]

>> Unhealthy configs

>> Unknown configs


Node and signal types are set as follows:

(<node_num>) <node_type> : [<signal_types>]
---------------------------------------------
(1) mass_1   : [acc, pos, vel]
(2) mass_2   : [acc, pos, vel]
(3) mass_3   : [acc, pos, vel]
(4) mass_4   : [acc, pos, vel]

Node group name: m004


For ds_type 'OK' and others....

Maximum timesteps across all node types: 100001

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[1000., 1000., 1000.],
 [1000., 1000., 1000.],
 [1000., 1000., 1000.],
 [1000., 1000., 1000.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
---------------------------------------------
Total samples: 1000 
Train: 800/800 [OK=800, NOK=0, UK=0], Test: 100/100 [OK=100, NOK=0, UK=0], Val: 100/100 [OK=100, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 16
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 2
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 2
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Reciever relation matrix:
tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 0., 1.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]]) 
shape: torch.Size([12, 4])

Sender relation matrix:
tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.]]) 
shape: torch.Size([12, 4])

---------------------------------------------------------------------------

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 1
msg_out_size: 64
edge_mlp_config: [[64, 'tanh'], [32, 'tanh'], [16, 'tanh'], [64, None]]
out_mlp_config: [[64, 'tanh'], [32, 'tanh'], [16, 'tanh'], [64, None]]
do_prob: 0
is_batch_norm: True
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: None
feat_configs: []
reduc_config: None
feat_norm: None
n_dims: 3

Decoder run parameters:
-------------------------
skip_first_edge_type: False
pred_steps: 1
is_burn_in: False
burn_in_steps: 1
is_dynamic_graph: False
temp: 1.0
is_hard: True

Edge matrix is created from relation matrices and set to decoder.

---------------------------------------------------------------------------

Decoder Model Initialized with the following configurations:

Decoder Model Summary:
Decoder(
  (edge_mlp_fn): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Tanh()
        (3): Dropout(p=0, inplace=False)
        (4): Linear(in_features=64, out_features=32, bias=True)
        (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): Tanh()
        (7): Dropout(p=0, inplace=False)
        (8): Linear(in_features=32, out_features=16, bias=True)
        (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (10): Tanh()
        (11): Dropout(p=0, inplace=False)
        (12): Linear(in_features=16, out_features=64, bias=True)
        (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=64, bias=True)
    (hidden_u): Linear(in_features=64, out_features=64, bias=True)
    (input_r): Linear(in_features=3, out_features=64, bias=True)
    (hidden_r): Linear(in_features=64, out_features=64, bias=True)
    (input_h): Linear(in_features=3, out_features=64, bias=True)
    (hidden_h): Linear(in_features=64, out_features=64, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Tanh()
      (3): Dropout(p=0, inplace=False)
      (4): Linear(in_features=64, out_features=32, bias=True)
      (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): Tanh()
      (7): Dropout(p=0, inplace=False)
      (8): Linear(in_features=32, out_features=16, bias=True)
      (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): Tanh()
      (11): Dropout(p=0, inplace=False)
      (12): Linear(in_features=16, out_features=64, bias=True)
      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Tanh()
      (3): Dropout(p=0, inplace=False)
      (4): Linear(in_features=64, out_features=32, bias=True)
      (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): Tanh()
      (7): Dropout(p=0, inplace=False)
      (8): Linear(in_features=32, out_features=16, bias=True)
      (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): Tanh()
      (11): Dropout(p=0, inplace=False)
      (12): Linear(in_features=16, out_features=64, bias=True)
      (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (mean_output_layer): Linear(in_features=64, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=64, out_features=3, bias=True)
)

---------------------------------------------------------------------------
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1

---------------------------------------------------------------------------

Initializing input processors for decoder model...

>> Domain transformer initialized for 'time' domain

>> No raw data normalization is applied

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Training: |                                                                         | 0/? [00:00<?, ?it/s]Training:   0%|                                                                    | 0/16 [00:00<?, ?it/s]Epoch 0:   0%|                                                                     | 0/16 [00:00<?, ?it/s]Epoch 0:   6%|###8                                                         | 1/16 [00:00<00:11,  1.32it/s]Epoch 0:   6%|###                                              | 1/16 [00:00<00:11,  1.32it/s, v_num=_1.1]Epoch 0:  12%|######1                                          | 2/16 [00:01<00:09,  1.43it/s, v_num=_1.1]Epoch 0:  12%|######1                                          | 2/16 [00:01<00:09,  1.43it/s, v_num=_1.1]Epoch 0:  19%|#########1                                       | 3/16 [00:02<00:09,  1.42it/s, v_num=_1.1]Epoch 0:  19%|#########1                                       | 3/16 [00:02<00:09,  1.42it/s, v_num=_1.1]Epoch 0:  25%|############2                                    | 4/16 [00:02<00:08,  1.44it/s, v_num=_1.1]Epoch 0:  25%|############2                                    | 4/16 [00:02<00:08,  1.44it/s, v_num=_1.1]Epoch 0:  31%|###############3                                 | 5/16 [00:03<00:07,  1.51it/s, v_num=_1.1]Epoch 0:  31%|###############3                                 | 5/16 [00:03<00:07,  1.51it/s, v_num=_1.1]Epoch 0:  38%|##################3                              | 6/16 [00:03<00:06,  1.52it/s, v_num=_1.1]Epoch 0:  38%|##################3                              | 6/16 [00:03<00:06,  1.52it/s, v_num=_1.1]Epoch 0:  44%|#####################4                           | 7/16 [00:04<00:05,  1.55it/s, v_num=_1.1]Epoch 0:  44%|#####################4                           | 7/16 [00:04<00:05,  1.55it/s, v_num=_1.1]Epoch 0:  50%|########################5                        | 8/16 [00:05<00:05,  1.54it/s, v_num=_1.1]Epoch 0:  50%|########################5                        | 8/16 [00:05<00:05,  1.54it/s, v_num=_1.1]Epoch 0:  56%|###########################5                     | 9/16 [00:05<00:04,  1.53it/s, v_num=_1.1]Epoch 0:  56%|###########################5                     | 9/16 [00:05<00:04,  1.53it/s, v_num=_1.1]Epoch 0:  62%|##############################                  | 10/16 [00:06<00:03,  1.53it/s, v_num=_1.1]Epoch 0:  62%|##############################                  | 10/16 [00:06<00:03,  1.53it/s, v_num=_1.1]Epoch 0:  69%|#################################               | 11/16 [00:07<00:03,  1.54it/s, v_num=_1.1]Epoch 0:  69%|#################################               | 11/16 [00:07<00:03,  1.54it/s, v_num=_1.1]Epoch 0:  75%|####################################            | 12/16 [00:07<00:02,  1.54it/s, v_num=_1.1]Epoch 0:  75%|####################################            | 12/16 [00:07<00:02,  1.54it/s, v_num=_1.1]Epoch 0:  81%|#######################################         | 13/16 [00:08<00:01,  1.55it/s, v_num=_1.1]Epoch 0:  81%|#######################################         | 13/16 [00:08<00:01,  1.55it/s, v_num=_1.1]Epoch 0:  88%|##########################################      | 14/16 [00:09<00:01,  1.55it/s, v_num=_1.1]Epoch 0:  88%|##########################################      | 14/16 [00:09<00:01,  1.55it/s, v_num=_1.1]Epoch 0:  94%|#############################################   | 15/16 [00:09<00:00,  1.57it/s, v_num=_1.1]Epoch 0:  94%|#############################################   | 15/16 [00:09<00:00,  1.57it/s, v_num=_1.1]Epoch 0: 100%|################################################| 16/16 [00:10<00:00,  1.57it/s, v_num=_1.1]Epoch 0: 100%|################################################| 16/16 [00:10<00:00,  1.57it/s, v_num=_1.1]
Validation: |                                                                       | 0/? [00:00<?, ?it/s][A
Validation:   0%|                                                                   | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                                      | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:  50%|#######################                       | 1/2 [00:00<00:00,  3.82it/s][A
Validation DataLoader 0: 100%|##############################################| 2/2 [00:00<00:00,  3.81it/s][A
                                                                                                          [AEpoch 0: 100%|################################| 16/16 [00:10<00:00,  1.49it/s, v_num=_1.1, val_loss=202.0]Epoch 0: 100%|##############| 16/16 [00:10<00:00,  1.49it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]
Epoch 1/5 completed
Epoch 0:   0%|   | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]                    Epoch 1:   0%|   | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:   6%| | 1/16 [00:00<00:10,  1.44it/s, v_num=_1.1, val_loss=202.0, train_loss=Epoch 1:   6%| | 1/16 [00:00<00:10,  1.44it/s, v_num=_1.1, val_loss=202.0, train_loss=Epoch 1:  12%|1| 2/16 [00:01<00:09,  1.50it/s, v_num=_1.1, val_loss=202.0, train_lo   Epoch 1:  12%|1| 2/16 [00:01<00:09,  1.50it/s, v_num=_1.1, val_loss=202.0, train_loEpoch 1:  19%|1| 3/16 [00:02<00:08,  1.46it/s, v_num=_1.1, val_loss=202.0, train_loEpoch 1:  19%|1| 3/16 [00:02<00:08,  1.46it/s, v_num=_1.1, val_loss=202.0, train_loEpoch 1:  25%|###7           | 4/16 [00:02<00:08,  1.39it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  25%|###7           | 4/16 [00:02<00:08,  1.39it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  31%|####6          | 5/16 [00:03<00:08,  1.34it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  31%|####6          | 5/16 [00:03<00:08,  1.34it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  38%|#####6         | 6/16 [00:04<00:07,  1.38it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  38%|#####6         | 6/16 [00:04<00:07,  1.38it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  44%|######5        | 7/16 [00:04<00:06,  1.41it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  44%|######5        | 7/16 [00:04<00:06,  1.41it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  50%|#######5       | 8/16 [00:05<00:05,  1.43it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  50%|#######5       | 8/16 [00:05<00:05,  1.43it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  56%|########4      | 9/16 [00:06<00:04,  1.46it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  56%|########4      | 9/16 [00:06<00:04,  1.46it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  62%|########7     | 10/16 [00:06<00:04,  1.48it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  62%|########7     | 10/16 [00:06<00:04,  1.48it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  69%|#########6    | 11/16 [00:07<00:03,  1.49it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  69%|#########6    | 11/16 [00:07<00:03,  1.49it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  75%|##########5   | 12/16 [00:07<00:02,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  75%|##########5   | 12/16 [00:07<00:02,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  81%|###########3  | 13/16 [00:08<00:01,  1.52it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  81%|###########3  | 13/16 [00:08<00:01,  1.52it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  88%|############2 | 14/16 [00:09<00:01,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  88%|############2 | 14/16 [00:09<00:01,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  94%|#############1| 15/16 [00:09<00:00,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1:  94%|#############1| 15/16 [00:09<00:00,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0]Epoch 1: 100%|#| 16/16 [00:10<00:00,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0              Epoch 1: 100%|#| 16/16 [00:10<00:00,  1.51it/s, v_num=_1.1, val_loss=202.0, train_loss=226.0
Validation: |                                                         | 0/? [00:00<?, ?it/s][A
Validation:   0%|                                                     | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                        | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:  50%|################                | 1/2 [00:00<00:00,  3.81it/s][A
Validation DataLoader 0: 100%|################################################| 2/2 [00:00<00:00,  3.83it/s][A
                                                                                                            [AEpoch 1: 100%|################| 16/16 [00:11<00:00,  1.43it/s, v_num=_1.1, val_loss=150.0, train_loss=226.0]Epoch 1: 100%|################| 16/16 [00:11<00:00,  1.43it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]
Epoch 2/5 completed
Epoch 1:   0%|                         | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:   0%|                         | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:   6%|#                | 1/16 [00:00<00:09,  1.62it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:   6%|#                | 1/16 [00:00<00:09,  1.62it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  12%|#8             | 2/16 [00:01<00:09,  1.52it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]  Epoch 2:  12%|#8             | 2/16 [00:01<00:09,  1.52it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  19%|##8            | 3/16 [00:02<00:08,  1.44it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  19%|##8            | 3/16 [00:02<00:08,  1.44it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  25%|###7           | 4/16 [00:02<00:08,  1.49it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  25%|###7           | 4/16 [00:02<00:08,  1.49it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  31%|####6          | 5/16 [00:03<00:07,  1.51it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  31%|####6          | 5/16 [00:03<00:07,  1.51it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  38%|#####6         | 6/16 [00:03<00:06,  1.54it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  38%|#####6         | 6/16 [00:03<00:06,  1.54it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  44%|######5        | 7/16 [00:04<00:05,  1.57it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  44%|######5        | 7/16 [00:04<00:05,  1.57it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  50%|#######5       | 8/16 [00:05<00:05,  1.58it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  50%|#######5       | 8/16 [00:05<00:05,  1.58it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  56%|########4      | 9/16 [00:05<00:04,  1.60it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  56%|########4      | 9/16 [00:05<00:04,  1.60it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  62%|########7     | 10/16 [00:06<00:03,  1.59it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  62%|########7     | 10/16 [00:06<00:03,  1.59it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  69%|#########6    | 11/16 [00:06<00:03,  1.57it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  69%|#########6    | 11/16 [00:06<00:03,  1.57it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  75%|#########7   | 12/16 [00:07<00:02,  1.56it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0] Epoch 2:  75%|#########7   | 12/16 [00:07<00:02,  1.56it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  81%|##########5  | 13/16 [00:08<00:01,  1.55it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  81%|##########5  | 13/16 [00:08<00:01,  1.55it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  88%|#######8 | 14/16 [00:09<00:01,  1.54it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]    Epoch 2:  88%|#######8 | 14/16 [00:09<00:01,  1.54it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  94%|########4| 15/16 [00:10<00:00,  1.49it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2:  94%|########4| 15/16 [00:10<00:00,  1.49it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]Epoch 2: 100%|#######| 16/16 [00:10<00:00,  1.49it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]  Epoch 2: 100%|#######| 16/16 [00:10<00:00,  1.49it/s, v_num=_1.1, val_loss=150.0, train_loss=167.0]
Validation: |                                                                | 0/? [00:00<?, ?it/s][A
Validation:   0%|                                                            | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                               | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:  50%|###################5                   | 1/2 [00:00<00:00,  4.19it/s][A
Validation DataLoader 0: 100%|#######################################| 2/2 [00:00<00:00,  4.06it/s][A
                                                                                                   [AEpoch 2: 100%|#######| 16/16 [00:11<00:00,  1.42it/s, v_num=_1.1, val_loss=52.00, train_loss=167.0]Epoch 2: 100%|#######| 16/16 [00:11<00:00,  1.42it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]
Epoch 3/5 completed
Epoch 2:   0%|                | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:   0%|                | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:   6%|5       | 1/16 [00:00<00:09,  1.55it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:   6%|5       | 1/16 [00:00<00:09,  1.55it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  12%|#       | 2/16 [00:01<00:08,  1.59it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  12%|#       | 2/16 [00:01<00:08,  1.59it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  19%|#5      | 3/16 [00:01<00:08,  1.62it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  19%|#5      | 3/16 [00:01<00:08,  1.62it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  25%|##      | 4/16 [00:02<00:07,  1.66it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  25%|##      | 4/16 [00:02<00:07,  1.66it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  31%|##5     | 5/16 [00:03<00:06,  1.66it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  31%|##5     | 5/16 [00:03<00:06,  1.66it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  38%|###     | 6/16 [00:03<00:06,  1.63it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  38%|###     | 6/16 [00:03<00:06,  1.63it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  44%|###5    | 7/16 [00:04<00:05,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  44%|###5    | 7/16 [00:04<00:05,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  50%|####    | 8/16 [00:04<00:04,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  50%|####    | 8/16 [00:04<00:04,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  56%|####5   | 9/16 [00:05<00:04,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  56%|####5   | 9/16 [00:05<00:04,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  62%|####3  | 10/16 [00:06<00:03,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  62%|####3  | 10/16 [00:06<00:03,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  69%|####8  | 11/16 [00:06<00:03,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  69%|####8  | 11/16 [00:06<00:03,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  75%|#####2 | 12/16 [00:07<00:02,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  75%|#####2 | 12/16 [00:07<00:02,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  81%|#####6 | 13/16 [00:07<00:01,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  81%|#####6 | 13/16 [00:07<00:01,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  88%|######1| 14/16 [00:08<00:01,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  88%|######1| 14/16 [00:08<00:01,  1.64it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  94%|######5| 15/16 [00:09<00:00,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3:  94%|######5| 15/16 [00:09<00:00,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3: 100%|#######| 16/16 [00:09<00:00,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]Epoch 3: 100%|#######| 16/16 [00:09<00:00,  1.65it/s, v_num=_1.1, val_loss=52.00, train_loss=112.0]
Validation: |                                                                | 0/? [00:00<?, ?it/s][A
Validation:   0%|                                                            | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                               | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:  50%|###################5                   | 1/2 [00:00<00:00,  4.64it/s][A
Validation DataLoader 0: 100%|#######################################| 2/2 [00:00<00:00,  4.69it/s][A
                                                                                                   [AEpoch 3: 100%|#######| 16/16 [00:10<00:00,  1.58it/s, v_num=_1.1, val_loss=1.070, train_loss=112.0]Epoch 3: 100%|#######| 16/16 [00:10<00:00,  1.58it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]
Epoch 4/5 completed
Epoch 3:   0%|                | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:   0%|                | 0/16 [00:00<?, ?it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:   6%|5       | 1/16 [00:00<00:10,  1.49it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:   6%|5       | 1/16 [00:00<00:10,  1.48it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  12%|#       | 2/16 [00:01<00:09,  1.55it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  12%|#       | 2/16 [00:01<00:09,  1.55it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  19%|#5      | 3/16 [00:01<00:08,  1.56it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  19%|#5      | 3/16 [00:01<00:08,  1.56it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  25%|##      | 4/16 [00:02<00:07,  1.59it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  25%|##      | 4/16 [00:02<00:07,  1.59it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  31%|##5     | 5/16 [00:03<00:06,  1.64it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  31%|##5     | 5/16 [00:03<00:06,  1.64it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  38%|###     | 6/16 [00:03<00:06,  1.65it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  38%|###     | 6/16 [00:03<00:06,  1.65it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  44%|###5    | 7/16 [00:04<00:05,  1.66it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  44%|###5    | 7/16 [00:04<00:05,  1.66it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  50%|####    | 8/16 [00:04<00:04,  1.67it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  50%|####    | 8/16 [00:04<00:04,  1.67it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  56%|####5   | 9/16 [00:05<00:04,  1.68it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  56%|####5   | 9/16 [00:05<00:04,  1.68it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  62%|####3  | 10/16 [00:05<00:03,  1.68it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  62%|####3  | 10/16 [00:05<00:03,  1.68it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  69%|####8  | 11/16 [00:06<00:02,  1.70it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  69%|####8  | 11/16 [00:06<00:02,  1.70it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  75%|#####2 | 12/16 [00:07<00:02,  1.70it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  75%|#####2 | 12/16 [00:07<00:02,  1.70it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  81%|#####6 | 13/16 [00:07<00:01,  1.71it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  81%|#####6 | 13/16 [00:07<00:01,  1.71it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  88%|######1| 14/16 [00:08<00:01,  1.72it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  88%|######1| 14/16 [00:08<00:01,  1.72it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  94%|######5| 15/16 [00:08<00:00,  1.71it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4:  94%|######5| 15/16 [00:08<00:00,  1.71it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4: 100%|#######| 16/16 [00:09<00:00,  1.72it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]Epoch 4: 100%|#######| 16/16 [00:09<00:00,  1.72it/s, v_num=_1.1, val_loss=1.070, train_loss=49.60]
Validation: |                                                                | 0/? [00:00<?, ?it/s][A
Validation:   0%|                                                            | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                               | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:  50%|###################5                   | 1/2 [00:00<00:00,  4.22it/s][A
Validation DataLoader 0: 100%|#######################################| 2/2 [00:00<00:00,  4.35it/s][A
                                                                                                   [AEpoch 4: 100%|#######| 16/16 [00:09<00:00,  1.64it/s, v_num=_1.1, val_loss=-57.8, train_loss=49.60]Epoch 4: 100%|#######| 16/16 [00:09<00:00,  1.64it/s, v_num=_1.1, val_loss=-57.8, train_loss=-12.6]
Epoch 5/5 completed
Epoch 4: 100%|#######| 16/16 [00:09<00:00,  1.63it/s, v_num=_1.1, val_loss=-57.8, train_loss=-12.6]

Training completed in 57.35 seconds or 0.96 minutes or 0.015931065943506028 hours.

Training completed for model '(m004)-(gru)_decoder_1.1'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for (m004)-(gru)_decoder_1.1...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.386' for (m004)-(gru)_decoder_1.1...

Decoder output plot for rep '1001.386' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.638' for (m004)-(gru)_decoder_1.1...

Decoder output plot for rep '1001.638' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1


.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1\checkpoints:

['epoch=4-step=80.ckpt']

Edge matrix is created from relation matrices and set to decoder.

Trained Decoder Model Loaded for testing.

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1\test
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                                                                   | 0/? [00:00<?, ?it/s]
Initializing input processors for decoder model...

>> Domain transformer initialized for 'time' domain

>> No raw data normalization is applied

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                                                               | 0/2 [00:00<?, ?it/s]Testing DataLoader 0:   0%|                                                  | 0/2 [00:00<?, ?it/s]Testing DataLoader 0:  50%|#####################                     | 1/2 [00:00<00:00,  5.23it/s]Testing DataLoader 0: 100%|##########################################| 2/2 [00:00<00:00,  5.23it/s]
test_loss: -67.5610

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.603' for (m004)-(gru)_decoder_1.1...

Decoder output plot for rep '1001.603' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\grp=m004\D=gru\(m004)-(gru)_decoder_1.1\test

Testing DataLoader 0: 100%|##########################################| 2/2 [00:02<00:00,  0.87it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_loss         â”‚    -67.56100463867188     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

===========================================================================

Decoder model '(m004)-(gru)_decoder_1.1' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-08-24 19:56:29
