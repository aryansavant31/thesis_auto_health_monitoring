=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m004_(apv+G)]-gru_dec_1.4
Start Time: 2025-09-21 19:00:31
End Time: 2025-09-21 19:23:06

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting decoder model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]

Node group name: m004
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.

Target rep_num 1001.0001 found at index 0. This sample will be included in the test set.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 450/500 [OK=450, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 9
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (4, 4)
     n1   n2   n3   n4
n1  0.0  1.0  0.0  0.0
n2  1.0  0.0  1.0  0.0
n3  0.0  1.0  0.0  1.0
n4  0.0  0.0  1.0  0.0


**Receiver relation matrix** => shape: (12, 4)
      n1   n2   n3   n4
e12  0.0  1.0  0.0  0.0
e13  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0
e21  1.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0
e24  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0
e34  0.0  0.0  0.0  1.0
e41  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0
e43  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (12, 4)
      n1   n2   n3   n4
e12  1.0  0.0  0.0  0.0
e13  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0
e21  0.0  1.0  0.0  0.0
e23  0.0  1.0  0.0  0.0
e24  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0
e32  0.0  0.0  1.0  0.0
e34  0.0  0.0  1.0  0.0
e41  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0
e43  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 1
msg_out_size: 128
edge_mlp_config: [[128, 'tanh'], [128, 'tanh']]
out_mlp_config: [[128, 'relu'], [128, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
n_dims: 3

Decoder run parameters:
-------------------------
skip_first_edge_type: False
pred_steps: 10
is_burn_in: True
final_pred_steps: 50
is_dynamic_graph: False
temp: 1.0
is_hard: True
show_conf_band: False

'[m004_(apv+G)]-gru_dec_1.2' already exists in the log path 'C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2'.
(a) Overwrite exsiting version, (b) create new version, (c) stop training (Choose 'a', 'b' or 'c'):  ['[m004_(apv+G)]-gru_dec_1.1', '[m004_(apv+G)]-gru_dec_1.2', '[m004_(apv+G)]-gru_dec_1.3']
Next decoder folder will be: [m004_(apv+G)]-gru_dec_1.4
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4

---------------------------------------------------------------------------

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Training parameters set to: 
lr=0.001, 
optimizer=adam, 
loss_type=mae

---------------------------------------------------------------------------

Decoder Model Initialized with the following configurations:

Decoder Model Summary:
Decoder(
  (edge_mlp_fn): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=128, out_features=128, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=128, bias=True)
    (hidden_u): Linear(in_features=128, out_features=128, bias=True)
    (input_r): Linear(in_features=3, out_features=128, bias=True)
    (hidden_r): Linear(in_features=128, out_features=128, bias=True)
    (input_h): Linear(in_features=3, out_features=128, bias=True)
    (hidden_h): Linear(in_features=128, out_features=128, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=128, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=128, out_features=3, bias=True)
)

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Step 0, Epoch 1/50, Batch 0/80
train_loss: 1.3917

Step 5, Epoch 1/50, Batch 5/80
train_loss: 0.2747

Step 10, Epoch 1/50, Batch 10/80
train_loss: 0.1309

Step 15, Epoch 1/50, Batch 15/80
train_loss: 0.1213

Step 20, Epoch 1/50, Batch 20/80
train_loss: 0.1176

Step 25, Epoch 1/50, Batch 25/80
train_loss: 0.1024

Step 30, Epoch 1/50, Batch 30/80
train_loss: 0.1139

Step 35, Epoch 1/50, Batch 35/80
train_loss: 0.0995

Step 40, Epoch 1/50, Batch 40/80
train_loss: 0.1007

Step 45, Epoch 1/50, Batch 45/80
train_loss: 0.0964

Step 50, Epoch 1/50, Batch 50/80
train_loss: 0.1116

Step 55, Epoch 1/50, Batch 55/80
train_loss: 0.1062

Step 60, Epoch 1/50, Batch 60/80
train_loss: 0.1026

Step 65, Epoch 1/50, Batch 65/80
train_loss: 0.1016

Step 70, Epoch 1/50, Batch 70/80
train_loss: 0.0955

Step 75, Epoch 1/50, Batch 75/80
train_loss: 0.0936

Epoch 1/50 completed, Global Step: 79
train_loss: 0.0936, val_loss: 0.0945

---------------------------------------------------------------------------


Step 80, Epoch 2/50, Batch 0/80
train_loss: 0.0939

Step 85, Epoch 2/50, Batch 5/80
train_loss: 0.0947

Step 90, Epoch 2/50, Batch 10/80
train_loss: 0.0928

Step 95, Epoch 2/50, Batch 15/80
train_loss: 0.0963

Step 100, Epoch 2/50, Batch 20/80
train_loss: 0.0930

Step 105, Epoch 2/50, Batch 25/80
train_loss: 0.1026

Step 110, Epoch 2/50, Batch 30/80
train_loss: 0.0936

Step 115, Epoch 2/50, Batch 35/80
train_loss: 0.0932

Step 120, Epoch 2/50, Batch 40/80
train_loss: 0.1024

Step 125, Epoch 2/50, Batch 45/80
train_loss: 0.0917

Step 130, Epoch 2/50, Batch 50/80
train_loss: 0.0931

Step 135, Epoch 2/50, Batch 55/80
train_loss: 0.0949

Step 140, Epoch 2/50, Batch 60/80
train_loss: 0.0920

Step 145, Epoch 2/50, Batch 65/80
train_loss: 0.0913

Step 150, Epoch 2/50, Batch 70/80
train_loss: 0.0948

Step 155, Epoch 2/50, Batch 75/80
train_loss: 0.0915

Epoch 2/50 completed, Global Step: 159
train_loss: 0.0915, val_loss: 0.0908

---------------------------------------------------------------------------


Step 160, Epoch 3/50, Batch 0/80
train_loss: 0.0905

Step 165, Epoch 3/50, Batch 5/80
train_loss: 0.0906

Step 170, Epoch 3/50, Batch 10/80
train_loss: 0.0922

Step 175, Epoch 3/50, Batch 15/80
train_loss: 0.0916

Step 180, Epoch 3/50, Batch 20/80
train_loss: 0.0931

Step 185, Epoch 3/50, Batch 25/80
train_loss: 0.0903

Step 190, Epoch 3/50, Batch 30/80
train_loss: 0.0925

Step 195, Epoch 3/50, Batch 35/80
train_loss: 0.0950

Step 200, Epoch 3/50, Batch 40/80
train_loss: 0.0904

Step 205, Epoch 3/50, Batch 45/80
train_loss: 0.0900

Step 210, Epoch 3/50, Batch 50/80
train_loss: 0.0903

Step 215, Epoch 3/50, Batch 55/80
train_loss: 0.0883

Step 220, Epoch 3/50, Batch 60/80
train_loss: 0.0975

Step 225, Epoch 3/50, Batch 65/80
train_loss: 0.0902

Step 230, Epoch 3/50, Batch 70/80
train_loss: 0.0885

Step 235, Epoch 3/50, Batch 75/80
train_loss: 0.0907

Epoch 3/50 completed, Global Step: 239
train_loss: 0.0907, val_loss: 0.0885

---------------------------------------------------------------------------


Step 240, Epoch 4/50, Batch 0/80
train_loss: 0.0880

Step 245, Epoch 4/50, Batch 5/80
train_loss: 0.0878

Step 250, Epoch 4/50, Batch 10/80
train_loss: 0.0866

Step 255, Epoch 4/50, Batch 15/80
train_loss: 0.0892

Step 260, Epoch 4/50, Batch 20/80
train_loss: 0.0892

Step 265, Epoch 4/50, Batch 25/80
train_loss: 0.0868

Step 270, Epoch 4/50, Batch 30/80
train_loss: 0.0867

Step 275, Epoch 4/50, Batch 35/80
train_loss: 0.0877

Step 280, Epoch 4/50, Batch 40/80
train_loss: 0.0871

Step 285, Epoch 4/50, Batch 45/80
train_loss: 0.0905

Step 290, Epoch 4/50, Batch 50/80
train_loss: 0.0858

Step 295, Epoch 4/50, Batch 55/80
train_loss: 0.0851

Step 300, Epoch 4/50, Batch 60/80
train_loss: 0.0882

Step 305, Epoch 4/50, Batch 65/80
train_loss: 0.0836

Step 310, Epoch 4/50, Batch 70/80
train_loss: 0.0865

Step 315, Epoch 4/50, Batch 75/80
train_loss: 0.0837

Epoch 4/50 completed, Global Step: 319
train_loss: 0.0837, val_loss: 0.0887

---------------------------------------------------------------------------


Step 320, Epoch 5/50, Batch 0/80
train_loss: 0.0887

Step 325, Epoch 5/50, Batch 5/80
train_loss: 0.0863

Step 330, Epoch 5/50, Batch 10/80
train_loss: 0.0911

Step 335, Epoch 5/50, Batch 15/80
train_loss: 0.0907

Step 340, Epoch 5/50, Batch 20/80
train_loss: 0.0828

Step 345, Epoch 5/50, Batch 25/80
train_loss: 0.0852

Step 350, Epoch 5/50, Batch 30/80
train_loss: 0.0876

Step 355, Epoch 5/50, Batch 35/80
train_loss: 0.0829

Step 360, Epoch 5/50, Batch 40/80
train_loss: 0.0816

Step 365, Epoch 5/50, Batch 45/80
train_loss: 0.0858

Step 370, Epoch 5/50, Batch 50/80
train_loss: 0.0813

Step 375, Epoch 5/50, Batch 55/80
train_loss: 0.0814

Step 380, Epoch 5/50, Batch 60/80
train_loss: 0.0803

Step 385, Epoch 5/50, Batch 65/80
train_loss: 0.0810

Step 390, Epoch 5/50, Batch 70/80
train_loss: 0.0791

Step 395, Epoch 5/50, Batch 75/80
train_loss: 0.0784

Epoch 5/50 completed, Global Step: 399
train_loss: 0.0784, val_loss: 0.0824

---------------------------------------------------------------------------


Step 400, Epoch 6/50, Batch 0/80
train_loss: 0.0821

Step 405, Epoch 6/50, Batch 5/80
train_loss: 0.0798

Step 410, Epoch 6/50, Batch 10/80
train_loss: 0.0831

Step 415, Epoch 6/50, Batch 15/80
train_loss: 0.0819

Step 420, Epoch 6/50, Batch 20/80
train_loss: 0.0782

Step 425, Epoch 6/50, Batch 25/80
train_loss: 0.0795

Step 430, Epoch 6/50, Batch 30/80
train_loss: 0.0787

Step 435, Epoch 6/50, Batch 35/80
train_loss: 0.0798

Step 440, Epoch 6/50, Batch 40/80
train_loss: 0.0761

Step 445, Epoch 6/50, Batch 45/80
train_loss: 0.0750

Step 450, Epoch 6/50, Batch 50/80
train_loss: 0.0787

Step 455, Epoch 6/50, Batch 55/80
train_loss: 0.0770

Step 460, Epoch 6/50, Batch 60/80
train_loss: 0.0757

Step 465, Epoch 6/50, Batch 65/80
train_loss: 0.0717

Step 470, Epoch 6/50, Batch 70/80
train_loss: 0.0734

Step 475, Epoch 6/50, Batch 75/80
train_loss: 0.0729

Epoch 6/50 completed, Global Step: 479
train_loss: 0.0729, val_loss: 0.0722

---------------------------------------------------------------------------


Step 480, Epoch 7/50, Batch 0/80
train_loss: 0.0724

Step 485, Epoch 7/50, Batch 5/80
train_loss: 0.0710

Step 490, Epoch 7/50, Batch 10/80
train_loss: 0.0702

Step 495, Epoch 7/50, Batch 15/80
train_loss: 0.0745

Step 500, Epoch 7/50, Batch 20/80
train_loss: 0.0713

Step 505, Epoch 7/50, Batch 25/80
train_loss: 0.0727

Step 510, Epoch 7/50, Batch 30/80
train_loss: 0.0680

Step 515, Epoch 7/50, Batch 35/80
train_loss: 0.0695

Step 520, Epoch 7/50, Batch 40/80
train_loss: 0.0697

Step 525, Epoch 7/50, Batch 45/80
train_loss: 0.0668

Step 530, Epoch 7/50, Batch 50/80
train_loss: 0.0661

Step 535, Epoch 7/50, Batch 55/80
train_loss: 0.0673

Step 540, Epoch 7/50, Batch 60/80
train_loss: 0.0659

Step 545, Epoch 7/50, Batch 65/80
train_loss: 0.0698

Step 550, Epoch 7/50, Batch 70/80
train_loss: 0.0691

Step 555, Epoch 7/50, Batch 75/80
train_loss: 0.0741

Epoch 7/50 completed, Global Step: 559
train_loss: 0.0741, val_loss: 0.0751

---------------------------------------------------------------------------


Step 560, Epoch 8/50, Batch 0/80
train_loss: 0.0751

Step 565, Epoch 8/50, Batch 5/80
train_loss: 0.0720

Step 570, Epoch 8/50, Batch 10/80
train_loss: 0.0716

Step 575, Epoch 8/50, Batch 15/80
train_loss: 0.0649

Step 580, Epoch 8/50, Batch 20/80
train_loss: 0.0640

Step 585, Epoch 8/50, Batch 25/80
train_loss: 0.0659

Step 590, Epoch 8/50, Batch 30/80
train_loss: 0.0669

Step 595, Epoch 8/50, Batch 35/80
train_loss: 0.0666

Step 600, Epoch 8/50, Batch 40/80
train_loss: 0.0628

Step 605, Epoch 8/50, Batch 45/80
train_loss: 0.0652

Step 610, Epoch 8/50, Batch 50/80
train_loss: 0.0636

Step 615, Epoch 8/50, Batch 55/80
train_loss: 0.0645

Step 620, Epoch 8/50, Batch 60/80
train_loss: 0.0627

Step 625, Epoch 8/50, Batch 65/80
train_loss: 0.0615

Step 630, Epoch 8/50, Batch 70/80
train_loss: 0.0659

Step 635, Epoch 8/50, Batch 75/80
train_loss: 0.0728

Epoch 8/50 completed, Global Step: 639
train_loss: 0.0728, val_loss: 0.0646

---------------------------------------------------------------------------


Step 640, Epoch 9/50, Batch 0/80
train_loss: 0.0645

Step 645, Epoch 9/50, Batch 5/80
train_loss: 0.0718

Step 650, Epoch 9/50, Batch 10/80
train_loss: 0.0660

Step 655, Epoch 9/50, Batch 15/80
train_loss: 0.0664

Step 660, Epoch 9/50, Batch 20/80
train_loss: 0.0657

Step 665, Epoch 9/50, Batch 25/80
train_loss: 0.0640

Step 670, Epoch 9/50, Batch 30/80
train_loss: 0.0638

Step 675, Epoch 9/50, Batch 35/80
train_loss: 0.0610

Step 680, Epoch 9/50, Batch 40/80
train_loss: 0.0604

Step 685, Epoch 9/50, Batch 45/80
train_loss: 0.0631

Step 690, Epoch 9/50, Batch 50/80
train_loss: 0.0607

Step 695, Epoch 9/50, Batch 55/80
train_loss: 0.0615

Step 700, Epoch 9/50, Batch 60/80
train_loss: 0.0724

Step 705, Epoch 9/50, Batch 65/80
train_loss: 0.0609

Step 710, Epoch 9/50, Batch 70/80
train_loss: 0.0580

Step 715, Epoch 9/50, Batch 75/80
train_loss: 0.0654

Epoch 9/50 completed, Global Step: 719
train_loss: 0.0654, val_loss: 0.0592

---------------------------------------------------------------------------


Step 720, Epoch 10/50, Batch 0/80
train_loss: 0.0596

Step 725, Epoch 10/50, Batch 5/80
train_loss: 0.0590

Step 730, Epoch 10/50, Batch 10/80
train_loss: 0.0659

Step 735, Epoch 10/50, Batch 15/80
train_loss: 0.0648

Step 740, Epoch 10/50, Batch 20/80
train_loss: 0.0610

Step 745, Epoch 10/50, Batch 25/80
train_loss: 0.0583

Step 750, Epoch 10/50, Batch 30/80
train_loss: 0.0581

Step 755, Epoch 10/50, Batch 35/80
train_loss: 0.0562

Step 760, Epoch 10/50, Batch 40/80
train_loss: 0.0542

Step 765, Epoch 10/50, Batch 45/80
train_loss: 0.0544

Step 770, Epoch 10/50, Batch 50/80
train_loss: 0.0549

Step 775, Epoch 10/50, Batch 55/80
train_loss: 0.0534

Step 780, Epoch 10/50, Batch 60/80
train_loss: 0.0532

Step 785, Epoch 10/50, Batch 65/80
train_loss: 0.0530

Step 790, Epoch 10/50, Batch 70/80
train_loss: 0.0600

Step 795, Epoch 10/50, Batch 75/80
train_loss: 0.0566

Epoch 10/50 completed, Global Step: 799
train_loss: 0.0566, val_loss: 0.0581

---------------------------------------------------------------------------


Step 800, Epoch 11/50, Batch 0/80
train_loss: 0.0577

Step 805, Epoch 11/50, Batch 5/80
train_loss: 0.0541

Step 810, Epoch 11/50, Batch 10/80
train_loss: 0.0579

Step 815, Epoch 11/50, Batch 15/80
train_loss: 0.0543

Step 820, Epoch 11/50, Batch 20/80
train_loss: 0.0518

Step 825, Epoch 11/50, Batch 25/80
train_loss: 0.0524

Step 830, Epoch 11/50, Batch 30/80
train_loss: 0.0533

Step 835, Epoch 11/50, Batch 35/80
train_loss: 0.0517

Step 840, Epoch 11/50, Batch 40/80
train_loss: 0.0504

Step 845, Epoch 11/50, Batch 45/80
train_loss: 0.0517

Step 850, Epoch 11/50, Batch 50/80
train_loss: 0.0536

Step 855, Epoch 11/50, Batch 55/80
train_loss: 0.0511

Step 860, Epoch 11/50, Batch 60/80
train_loss: 0.0511

Step 865, Epoch 11/50, Batch 65/80
train_loss: 0.0485

Step 870, Epoch 11/50, Batch 70/80
train_loss: 0.0498

Step 875, Epoch 11/50, Batch 75/80
train_loss: 0.0527

Epoch 11/50 completed, Global Step: 879
train_loss: 0.0527, val_loss: 0.0490

---------------------------------------------------------------------------


Step 880, Epoch 12/50, Batch 0/80
train_loss: 0.0491

Step 885, Epoch 12/50, Batch 5/80
train_loss: 0.0518

Step 890, Epoch 12/50, Batch 10/80
train_loss: 0.0501

Step 895, Epoch 12/50, Batch 15/80
train_loss: 0.0562

Step 900, Epoch 12/50, Batch 20/80
train_loss: 0.0515

Step 905, Epoch 12/50, Batch 25/80
train_loss: 0.0500

Step 910, Epoch 12/50, Batch 30/80
train_loss: 0.0505

Step 915, Epoch 12/50, Batch 35/80
train_loss: 0.0544

Step 920, Epoch 12/50, Batch 40/80
train_loss: 0.0530

Step 925, Epoch 12/50, Batch 45/80
train_loss: 0.0505

Step 930, Epoch 12/50, Batch 50/80
train_loss: 0.0490

Step 935, Epoch 12/50, Batch 55/80
train_loss: 0.0497

Step 940, Epoch 12/50, Batch 60/80
train_loss: 0.0481

Step 945, Epoch 12/50, Batch 65/80
train_loss: 0.0479

Step 950, Epoch 12/50, Batch 70/80
train_loss: 0.0485

Step 955, Epoch 12/50, Batch 75/80
train_loss: 0.0480

Epoch 12/50 completed, Global Step: 959
train_loss: 0.0480, val_loss: 0.0493

---------------------------------------------------------------------------


Step 960, Epoch 13/50, Batch 0/80
train_loss: 0.0491

Step 965, Epoch 13/50, Batch 5/80
train_loss: 0.0480

Step 970, Epoch 13/50, Batch 10/80
train_loss: 0.0470

Step 975, Epoch 13/50, Batch 15/80
train_loss: 0.0466

Step 980, Epoch 13/50, Batch 20/80
train_loss: 0.0471

Step 985, Epoch 13/50, Batch 25/80
train_loss: 0.0475

Step 990, Epoch 13/50, Batch 30/80
train_loss: 0.0472

Step 995, Epoch 13/50, Batch 35/80
train_loss: 0.0488

Step 1000, Epoch 13/50, Batch 40/80
train_loss: 0.0470

Step 1005, Epoch 13/50, Batch 45/80
train_loss: 0.0463

Step 1010, Epoch 13/50, Batch 50/80
train_loss: 0.0460

Step 1015, Epoch 13/50, Batch 55/80
train_loss: 0.0486

Step 1020, Epoch 13/50, Batch 60/80
train_loss: 0.0459

Step 1025, Epoch 13/50, Batch 65/80
train_loss: 0.0481

Step 1030, Epoch 13/50, Batch 70/80
train_loss: 0.0495

Step 1035, Epoch 13/50, Batch 75/80
train_loss: 0.0455

Epoch 13/50 completed, Global Step: 1039
train_loss: 0.0455, val_loss: 0.0492

---------------------------------------------------------------------------


Step 1040, Epoch 14/50, Batch 0/80
train_loss: 0.0488

Step 1045, Epoch 14/50, Batch 5/80
train_loss: 0.0506

Step 1050, Epoch 14/50, Batch 10/80
train_loss: 0.0468

Step 1055, Epoch 14/50, Batch 15/80
train_loss: 0.0458

Step 1060, Epoch 14/50, Batch 20/80
train_loss: 0.0516

Step 1065, Epoch 14/50, Batch 25/80
train_loss: 0.0537

Step 1070, Epoch 14/50, Batch 30/80
train_loss: 0.0472

Step 1075, Epoch 14/50, Batch 35/80
train_loss: 0.0503

Step 1080, Epoch 14/50, Batch 40/80
train_loss: 0.0465

Step 1085, Epoch 14/50, Batch 45/80
train_loss: 0.0504

Step 1090, Epoch 14/50, Batch 50/80
train_loss: 0.0485

Step 1095, Epoch 14/50, Batch 55/80
train_loss: 0.0476

Step 1100, Epoch 14/50, Batch 60/80
train_loss: 0.0441

Step 1105, Epoch 14/50, Batch 65/80
train_loss: 0.0451

Step 1110, Epoch 14/50, Batch 70/80
train_loss: 0.0452

Step 1115, Epoch 14/50, Batch 75/80
train_loss: 0.0437

Epoch 14/50 completed, Global Step: 1119
train_loss: 0.0437, val_loss: 0.0431

---------------------------------------------------------------------------


Step 1120, Epoch 15/50, Batch 0/80
train_loss: 0.0430

Step 1125, Epoch 15/50, Batch 5/80
train_loss: 0.0455

Step 1130, Epoch 15/50, Batch 10/80
train_loss: 0.0425

Step 1135, Epoch 15/50, Batch 15/80
train_loss: 0.0441

Step 1140, Epoch 15/50, Batch 20/80
train_loss: 0.0465

Step 1145, Epoch 15/50, Batch 25/80
train_loss: 0.0448

Step 1150, Epoch 15/50, Batch 30/80
train_loss: 0.0434

Step 1155, Epoch 15/50, Batch 35/80
train_loss: 0.0424

Step 1160, Epoch 15/50, Batch 40/80
train_loss: 0.0487

Step 1165, Epoch 15/50, Batch 45/80
train_loss: 0.0445

Step 1170, Epoch 15/50, Batch 50/80
train_loss: 0.0442

Step 1175, Epoch 15/50, Batch 55/80
train_loss: 0.0444

Step 1180, Epoch 15/50, Batch 60/80
train_loss: 0.0415

Step 1185, Epoch 15/50, Batch 65/80
train_loss: 0.0419

Step 1190, Epoch 15/50, Batch 70/80
train_loss: 0.0412

Step 1195, Epoch 15/50, Batch 75/80
train_loss: 0.0413

Epoch 15/50 completed, Global Step: 1199
train_loss: 0.0413, val_loss: 0.0408

---------------------------------------------------------------------------


Step 1200, Epoch 16/50, Batch 0/80
train_loss: 0.0408

Step 1205, Epoch 16/50, Batch 5/80
train_loss: 0.0392

Step 1210, Epoch 16/50, Batch 10/80
train_loss: 0.0397

Step 1215, Epoch 16/50, Batch 15/80
train_loss: 0.0385

Step 1220, Epoch 16/50, Batch 20/80
train_loss: 0.0393

Step 1225, Epoch 16/50, Batch 25/80
train_loss: 0.0382

Step 1230, Epoch 16/50, Batch 30/80
train_loss: 0.0378

Step 1235, Epoch 16/50, Batch 35/80
train_loss: 0.0435

Step 1240, Epoch 16/50, Batch 40/80
train_loss: 0.0419

Step 1245, Epoch 16/50, Batch 45/80
train_loss: 0.0398

Step 1250, Epoch 16/50, Batch 50/80
train_loss: 0.0481

Step 1255, Epoch 16/50, Batch 55/80
train_loss: 0.0489

Step 1260, Epoch 16/50, Batch 60/80
train_loss: 0.0435

Step 1265, Epoch 16/50, Batch 65/80
train_loss: 0.0413

Step 1270, Epoch 16/50, Batch 70/80
train_loss: 0.0368

Step 1275, Epoch 16/50, Batch 75/80
train_loss: 0.0349

Epoch 16/50 completed, Global Step: 1279
train_loss: 0.0349, val_loss: 0.0289

---------------------------------------------------------------------------


Step 1280, Epoch 17/50, Batch 0/80
train_loss: 0.0290

Step 1285, Epoch 17/50, Batch 5/80
train_loss: 0.0240

Step 1290, Epoch 17/50, Batch 10/80
train_loss: 0.0352

Step 1295, Epoch 17/50, Batch 15/80
train_loss: 0.0279

Step 1300, Epoch 17/50, Batch 20/80
train_loss: 0.0269

Step 1305, Epoch 17/50, Batch 25/80
train_loss: 0.0234

Step 1310, Epoch 17/50, Batch 30/80
train_loss: 0.0215

Step 1315, Epoch 17/50, Batch 35/80
train_loss: 0.0249

Step 1320, Epoch 17/50, Batch 40/80
train_loss: 0.0184

Step 1325, Epoch 17/50, Batch 45/80
train_loss: 0.0235

Step 1330, Epoch 17/50, Batch 50/80
train_loss: 0.0170

Step 1335, Epoch 17/50, Batch 55/80
train_loss: 0.0196

Step 1340, Epoch 17/50, Batch 60/80
train_loss: 0.0175

Step 1345, Epoch 17/50, Batch 65/80
train_loss: 0.0156

Step 1350, Epoch 17/50, Batch 70/80
train_loss: 0.0162

Step 1355, Epoch 17/50, Batch 75/80
train_loss: 0.0155

Epoch 17/50 completed, Global Step: 1359
train_loss: 0.0155, val_loss: 0.0144

---------------------------------------------------------------------------


Step 1360, Epoch 18/50, Batch 0/80
train_loss: 0.0145

Step 1365, Epoch 18/50, Batch 5/80
train_loss: 0.0180

Step 1370, Epoch 18/50, Batch 10/80
train_loss: 0.0180

Step 1375, Epoch 18/50, Batch 15/80
train_loss: 0.0142

Step 1380, Epoch 18/50, Batch 20/80
train_loss: 0.0162

Step 1385, Epoch 18/50, Batch 25/80
train_loss: 0.0147

Step 1390, Epoch 18/50, Batch 30/80
train_loss: 0.0142

Step 1395, Epoch 18/50, Batch 35/80
train_loss: 0.0135

Step 1400, Epoch 18/50, Batch 40/80
train_loss: 0.0174

Step 1405, Epoch 18/50, Batch 45/80
train_loss: 0.0133

Step 1410, Epoch 18/50, Batch 50/80
train_loss: 0.0230

Step 1415, Epoch 18/50, Batch 55/80
train_loss: 0.0145

Step 1420, Epoch 18/50, Batch 60/80
train_loss: 0.0141

Step 1425, Epoch 18/50, Batch 65/80
train_loss: 0.0154

Step 1430, Epoch 18/50, Batch 70/80
train_loss: 0.0160

Step 1435, Epoch 18/50, Batch 75/80
train_loss: 0.0159

Epoch 18/50 completed, Global Step: 1439
train_loss: 0.0159, val_loss: 0.0165

---------------------------------------------------------------------------


Step 1440, Epoch 19/50, Batch 0/80
train_loss: 0.0171

Step 1445, Epoch 19/50, Batch 5/80
train_loss: 0.0155

Step 1450, Epoch 19/50, Batch 10/80
train_loss: 0.0164

Step 1455, Epoch 19/50, Batch 15/80
train_loss: 0.0206

Step 1460, Epoch 19/50, Batch 20/80
train_loss: 0.0171

Step 1465, Epoch 19/50, Batch 25/80
train_loss: 0.0162

Step 1470, Epoch 19/50, Batch 30/80
train_loss: 0.0191

Step 1475, Epoch 19/50, Batch 35/80
train_loss: 0.0220

Step 1480, Epoch 19/50, Batch 40/80
train_loss: 0.0222

Step 1485, Epoch 19/50, Batch 45/80
train_loss: 0.0193

Step 1490, Epoch 19/50, Batch 50/80
train_loss: 0.0197

Step 1495, Epoch 19/50, Batch 55/80
train_loss: 0.0179

Step 1500, Epoch 19/50, Batch 60/80
train_loss: 0.0160

Step 1505, Epoch 19/50, Batch 65/80
train_loss: 0.0131

Step 1510, Epoch 19/50, Batch 70/80
train_loss: 0.0117

Step 1515, Epoch 19/50, Batch 75/80
train_loss: 0.0157

Epoch 19/50 completed, Global Step: 1519
train_loss: 0.0157, val_loss: 0.0133

---------------------------------------------------------------------------


Step 1520, Epoch 20/50, Batch 0/80
train_loss: 0.0133

Step 1525, Epoch 20/50, Batch 5/80
train_loss: 0.0130

Step 1530, Epoch 20/50, Batch 10/80
train_loss: 0.0157

Step 1535, Epoch 20/50, Batch 15/80
train_loss: 0.0162

Step 1540, Epoch 20/50, Batch 20/80
train_loss: 0.0163

Step 1545, Epoch 20/50, Batch 25/80
train_loss: 0.0201

Step 1550, Epoch 20/50, Batch 30/80
train_loss: 0.0218

Step 1555, Epoch 20/50, Batch 35/80
train_loss: 0.0149

Step 1560, Epoch 20/50, Batch 40/80
train_loss: 0.0140

Step 1565, Epoch 20/50, Batch 45/80
train_loss: 0.0110

Step 1570, Epoch 20/50, Batch 50/80
train_loss: 0.0103

Step 1575, Epoch 20/50, Batch 55/80
train_loss: 0.0115

Step 1580, Epoch 20/50, Batch 60/80
train_loss: 0.0095

Step 1585, Epoch 20/50, Batch 65/80
train_loss: 0.0112

Step 1590, Epoch 20/50, Batch 70/80
train_loss: 0.0100

Step 1595, Epoch 20/50, Batch 75/80
train_loss: 0.0185

Epoch 20/50 completed, Global Step: 1599
train_loss: 0.0185, val_loss: 0.0150

---------------------------------------------------------------------------


Step 1600, Epoch 21/50, Batch 0/80
train_loss: 0.0152

Step 1605, Epoch 21/50, Batch 5/80
train_loss: 0.0166

Step 1610, Epoch 21/50, Batch 10/80
train_loss: 0.0175

Step 1615, Epoch 21/50, Batch 15/80
train_loss: 0.0163

Step 1620, Epoch 21/50, Batch 20/80
train_loss: 0.0124

Step 1625, Epoch 21/50, Batch 25/80
train_loss: 0.0107

Step 1630, Epoch 21/50, Batch 30/80
train_loss: 0.0115

Step 1635, Epoch 21/50, Batch 35/80
train_loss: 0.0094

Step 1640, Epoch 21/50, Batch 40/80
train_loss: 0.0099

Step 1645, Epoch 21/50, Batch 45/80
train_loss: 0.0095

Step 1650, Epoch 21/50, Batch 50/80
train_loss: 0.0119

Step 1655, Epoch 21/50, Batch 55/80
train_loss: 0.0132

Step 1660, Epoch 21/50, Batch 60/80
train_loss: 0.0142

Step 1665, Epoch 21/50, Batch 65/80
train_loss: 0.0131

Step 1670, Epoch 21/50, Batch 70/80
train_loss: 0.0108

Step 1675, Epoch 21/50, Batch 75/80
train_loss: 0.0097

Epoch 21/50 completed, Global Step: 1679
train_loss: 0.0097, val_loss: 0.0097

---------------------------------------------------------------------------


Step 1680, Epoch 22/50, Batch 0/80
train_loss: 0.0096

Step 1685, Epoch 22/50, Batch 5/80
train_loss: 0.0093

Step 1690, Epoch 22/50, Batch 10/80
train_loss: 0.0093

Step 1695, Epoch 22/50, Batch 15/80
train_loss: 0.0096

Step 1700, Epoch 22/50, Batch 20/80
train_loss: 0.0129

Step 1705, Epoch 22/50, Batch 25/80
train_loss: 0.0108

Step 1710, Epoch 22/50, Batch 30/80
train_loss: 0.0113

Step 1715, Epoch 22/50, Batch 35/80
train_loss: 0.0113

Step 1720, Epoch 22/50, Batch 40/80
train_loss: 0.0115

Step 1725, Epoch 22/50, Batch 45/80
train_loss: 0.0130

Step 1730, Epoch 22/50, Batch 50/80
train_loss: 0.0114

Step 1735, Epoch 22/50, Batch 55/80
train_loss: 0.0101

Step 1740, Epoch 22/50, Batch 60/80
train_loss: 0.0104

Step 1745, Epoch 22/50, Batch 65/80
train_loss: 0.0080

Step 1750, Epoch 22/50, Batch 70/80
train_loss: 0.0083

Step 1755, Epoch 22/50, Batch 75/80
train_loss: 0.0134

Epoch 22/50 completed, Global Step: 1759
train_loss: 0.0134, val_loss: 0.0159

---------------------------------------------------------------------------


Step 1760, Epoch 23/50, Batch 0/80
train_loss: 0.0157

Step 1765, Epoch 23/50, Batch 5/80
train_loss: 0.0114

Step 1770, Epoch 23/50, Batch 10/80
train_loss: 0.0124

Step 1775, Epoch 23/50, Batch 15/80
train_loss: 0.0092

Step 1780, Epoch 23/50, Batch 20/80
train_loss: 0.0093

Step 1785, Epoch 23/50, Batch 25/80
train_loss: 0.0110

Step 1790, Epoch 23/50, Batch 30/80
train_loss: 0.0082

Step 1795, Epoch 23/50, Batch 35/80
train_loss: 0.0095

Step 1800, Epoch 23/50, Batch 40/80
train_loss: 0.0073

Step 1805, Epoch 23/50, Batch 45/80
train_loss: 0.0095

Step 1810, Epoch 23/50, Batch 50/80
train_loss: 0.0093

Step 1815, Epoch 23/50, Batch 55/80
train_loss: 0.0099

Step 1820, Epoch 23/50, Batch 60/80
train_loss: 0.0097

Step 1825, Epoch 23/50, Batch 65/80
train_loss: 0.0154

Step 1830, Epoch 23/50, Batch 70/80
train_loss: 0.0104

Step 1835, Epoch 23/50, Batch 75/80
train_loss: 0.0129

Epoch 23/50 completed, Global Step: 1839
train_loss: 0.0129, val_loss: 0.0101

---------------------------------------------------------------------------


Step 1840, Epoch 24/50, Batch 0/80
train_loss: 0.0103

Step 1845, Epoch 24/50, Batch 5/80
train_loss: 0.0093

Step 1850, Epoch 24/50, Batch 10/80
train_loss: 0.0118

Step 1855, Epoch 24/50, Batch 15/80
train_loss: 0.0149

Step 1860, Epoch 24/50, Batch 20/80
train_loss: 0.0146

Step 1865, Epoch 24/50, Batch 25/80
train_loss: 0.0157

Step 1870, Epoch 24/50, Batch 30/80
train_loss: 0.0116

Step 1875, Epoch 24/50, Batch 35/80
train_loss: 0.0117

Step 1880, Epoch 24/50, Batch 40/80
train_loss: 0.0120

Step 1885, Epoch 24/50, Batch 45/80
train_loss: 0.0114

Step 1890, Epoch 24/50, Batch 50/80
train_loss: 0.0107

Step 1895, Epoch 24/50, Batch 55/80
train_loss: 0.0109

Step 1900, Epoch 24/50, Batch 60/80
train_loss: 0.0125

Step 1905, Epoch 24/50, Batch 65/80
train_loss: 0.0133

Step 1910, Epoch 24/50, Batch 70/80
train_loss: 0.0136

Step 1915, Epoch 24/50, Batch 75/80
train_loss: 0.0151

Epoch 24/50 completed, Global Step: 1919
train_loss: 0.0151, val_loss: 0.0126

---------------------------------------------------------------------------


Step 1920, Epoch 25/50, Batch 0/80
train_loss: 0.0127

Step 1925, Epoch 25/50, Batch 5/80
train_loss: 0.0163

Step 1930, Epoch 25/50, Batch 10/80
train_loss: 0.0116

Step 1935, Epoch 25/50, Batch 15/80
train_loss: 0.0104

Step 1940, Epoch 25/50, Batch 20/80
train_loss: 0.0089

Step 1945, Epoch 25/50, Batch 25/80
train_loss: 0.0073

Step 1950, Epoch 25/50, Batch 30/80
train_loss: 0.0070

Step 1955, Epoch 25/50, Batch 35/80
train_loss: 0.0072

Step 1960, Epoch 25/50, Batch 40/80
train_loss: 0.0092

Step 1965, Epoch 25/50, Batch 45/80
train_loss: 0.0066

Step 1970, Epoch 25/50, Batch 50/80
train_loss: 0.0076

Step 1975, Epoch 25/50, Batch 55/80
train_loss: 0.0086

Step 1980, Epoch 25/50, Batch 60/80
train_loss: 0.0102

Step 1985, Epoch 25/50, Batch 65/80
train_loss: 0.0106

Step 1990, Epoch 25/50, Batch 70/80
train_loss: 0.0095

Step 1995, Epoch 25/50, Batch 75/80
train_loss: 0.0079

Epoch 25/50 completed, Global Step: 1999
train_loss: 0.0079, val_loss: 0.0070

---------------------------------------------------------------------------


Step 2000, Epoch 26/50, Batch 0/80
train_loss: 0.0068

Step 2005, Epoch 26/50, Batch 5/80
train_loss: 0.0078

Step 2010, Epoch 26/50, Batch 10/80
train_loss: 0.0114

Step 2015, Epoch 26/50, Batch 15/80
train_loss: 0.0078

Step 2020, Epoch 26/50, Batch 20/80
train_loss: 0.0098

Step 2025, Epoch 26/50, Batch 25/80
train_loss: 0.0085

Step 2030, Epoch 26/50, Batch 30/80
train_loss: 0.0105

Step 2035, Epoch 26/50, Batch 35/80
train_loss: 0.0071

Step 2040, Epoch 26/50, Batch 40/80
train_loss: 0.0094

Step 2045, Epoch 26/50, Batch 45/80
train_loss: 0.0077

Step 2050, Epoch 26/50, Batch 50/80
train_loss: 0.0070

Step 2055, Epoch 26/50, Batch 55/80
train_loss: 0.0076

Step 2060, Epoch 26/50, Batch 60/80
train_loss: 0.0070

Step 2065, Epoch 26/50, Batch 65/80
train_loss: 0.0081

Step 2070, Epoch 26/50, Batch 70/80
train_loss: 0.0079

Step 2075, Epoch 26/50, Batch 75/80
train_loss: 0.0078

Epoch 26/50 completed, Global Step: 2079
train_loss: 0.0078, val_loss: 0.0070

---------------------------------------------------------------------------


Step 2080, Epoch 27/50, Batch 0/80
train_loss: 0.0070

Step 2085, Epoch 27/50, Batch 5/80
train_loss: 0.0074

Step 2090, Epoch 27/50, Batch 10/80
train_loss: 0.0071

Step 2095, Epoch 27/50, Batch 15/80
train_loss: 0.0064

Step 2100, Epoch 27/50, Batch 20/80
train_loss: 0.0083

Step 2105, Epoch 27/50, Batch 25/80
train_loss: 0.0076

Step 2110, Epoch 27/50, Batch 30/80
train_loss: 0.0092

Step 2115, Epoch 27/50, Batch 35/80
train_loss: 0.0074

Step 2120, Epoch 27/50, Batch 40/80
train_loss: 0.0071

Step 2125, Epoch 27/50, Batch 45/80
train_loss: 0.0071

Step 2130, Epoch 27/50, Batch 50/80
train_loss: 0.0079

Step 2135, Epoch 27/50, Batch 55/80
train_loss: 0.0079

Step 2140, Epoch 27/50, Batch 60/80
train_loss: 0.0068

Step 2145, Epoch 27/50, Batch 65/80
train_loss: 0.0090

Step 2150, Epoch 27/50, Batch 70/80
train_loss: 0.0068

Step 2155, Epoch 27/50, Batch 75/80
train_loss: 0.0128

Epoch 27/50 completed, Global Step: 2159
train_loss: 0.0128, val_loss: 0.0102

---------------------------------------------------------------------------


Step 2160, Epoch 28/50, Batch 0/80
train_loss: 0.0101

Step 2165, Epoch 28/50, Batch 5/80
train_loss: 0.0109

Step 2170, Epoch 28/50, Batch 10/80
train_loss: 0.0102

Step 2175, Epoch 28/50, Batch 15/80
train_loss: 0.0095

Step 2180, Epoch 28/50, Batch 20/80
train_loss: 0.0071

Step 2185, Epoch 28/50, Batch 25/80
train_loss: 0.0079

Step 2190, Epoch 28/50, Batch 30/80
train_loss: 0.0115

Step 2195, Epoch 28/50, Batch 35/80
train_loss: 0.0121

Step 2200, Epoch 28/50, Batch 40/80
train_loss: 0.0085

Step 2205, Epoch 28/50, Batch 45/80
train_loss: 0.0086

Step 2210, Epoch 28/50, Batch 50/80
train_loss: 0.0072

Step 2215, Epoch 28/50, Batch 55/80
train_loss: 0.0078

Step 2220, Epoch 28/50, Batch 60/80
train_loss: 0.0083

Step 2225, Epoch 28/50, Batch 65/80
train_loss: 0.0073

Step 2230, Epoch 28/50, Batch 70/80
train_loss: 0.0073

Step 2235, Epoch 28/50, Batch 75/80
train_loss: 0.0070

Epoch 28/50 completed, Global Step: 2239
train_loss: 0.0070, val_loss: 0.0082

---------------------------------------------------------------------------


Step 2240, Epoch 29/50, Batch 0/80
train_loss: 0.0080

Step 2245, Epoch 29/50, Batch 5/80
train_loss: 0.0066

Step 2250, Epoch 29/50, Batch 10/80
train_loss: 0.0072

Step 2255, Epoch 29/50, Batch 15/80
train_loss: 0.0074

Step 2260, Epoch 29/50, Batch 20/80
train_loss: 0.0068

Step 2265, Epoch 29/50, Batch 25/80
train_loss: 0.0075

Step 2270, Epoch 29/50, Batch 30/80
train_loss: 0.0066

Step 2275, Epoch 29/50, Batch 35/80
train_loss: 0.0074

Step 2280, Epoch 29/50, Batch 40/80
train_loss: 0.0098

Step 2285, Epoch 29/50, Batch 45/80
train_loss: 0.0147

Step 2290, Epoch 29/50, Batch 50/80
train_loss: 0.0128

Step 2295, Epoch 29/50, Batch 55/80
train_loss: 0.0166

Step 2300, Epoch 29/50, Batch 60/80
train_loss: 0.0112

Step 2305, Epoch 29/50, Batch 65/80
train_loss: 0.0106

Step 2310, Epoch 29/50, Batch 70/80
train_loss: 0.0099

Step 2315, Epoch 29/50, Batch 75/80
train_loss: 0.0137

Epoch 29/50 completed, Global Step: 2319
train_loss: 0.0137, val_loss: 0.0125

---------------------------------------------------------------------------


Step 2320, Epoch 30/50, Batch 0/80
train_loss: 0.0124

Step 2325, Epoch 30/50, Batch 5/80
train_loss: 0.0126

Step 2330, Epoch 30/50, Batch 10/80
train_loss: 0.0100

Step 2335, Epoch 30/50, Batch 15/80
train_loss: 0.0112

Step 2340, Epoch 30/50, Batch 20/80
train_loss: 0.0118

Step 2345, Epoch 30/50, Batch 25/80
train_loss: 0.0115

Step 2350, Epoch 30/50, Batch 30/80
train_loss: 0.0096

Step 2355, Epoch 30/50, Batch 35/80
train_loss: 0.0112

Step 2360, Epoch 30/50, Batch 40/80
train_loss: 0.0080

Step 2365, Epoch 30/50, Batch 45/80
train_loss: 0.0100

Step 2370, Epoch 30/50, Batch 50/80
train_loss: 0.0097

Step 2375, Epoch 30/50, Batch 55/80
train_loss: 0.0101

Step 2380, Epoch 30/50, Batch 60/80
train_loss: 0.0083

Step 2385, Epoch 30/50, Batch 65/80
train_loss: 0.0094

Step 2390, Epoch 30/50, Batch 70/80
train_loss: 0.0078

Step 2395, Epoch 30/50, Batch 75/80
train_loss: 0.0104

Epoch 30/50 completed, Global Step: 2399
train_loss: 0.0104, val_loss: 0.0064

---------------------------------------------------------------------------


Step 2400, Epoch 31/50, Batch 0/80
train_loss: 0.0063

Step 2405, Epoch 31/50, Batch 5/80
train_loss: 0.0065

Step 2410, Epoch 31/50, Batch 10/80
train_loss: 0.0072

Step 2415, Epoch 31/50, Batch 15/80
train_loss: 0.0061

Step 2420, Epoch 31/50, Batch 20/80
train_loss: 0.0059

Step 2425, Epoch 31/50, Batch 25/80
train_loss: 0.0063

Step 2430, Epoch 31/50, Batch 30/80
train_loss: 0.0068

Step 2435, Epoch 31/50, Batch 35/80
train_loss: 0.0064

Step 2440, Epoch 31/50, Batch 40/80
train_loss: 0.0058

Step 2445, Epoch 31/50, Batch 45/80
train_loss: 0.0072

Step 2450, Epoch 31/50, Batch 50/80
train_loss: 0.0063

Step 2455, Epoch 31/50, Batch 55/80
train_loss: 0.0073

Step 2460, Epoch 31/50, Batch 60/80
train_loss: 0.0053

Step 2465, Epoch 31/50, Batch 65/80
train_loss: 0.0055

Step 2470, Epoch 31/50, Batch 70/80
train_loss: 0.0085

Step 2475, Epoch 31/50, Batch 75/80
train_loss: 0.0075

Epoch 31/50 completed, Global Step: 2479
train_loss: 0.0075, val_loss: 0.0081

---------------------------------------------------------------------------


Step 2480, Epoch 32/50, Batch 0/80
train_loss: 0.0081

Step 2485, Epoch 32/50, Batch 5/80
train_loss: 0.0090

Step 2490, Epoch 32/50, Batch 10/80
train_loss: 0.0128

Step 2495, Epoch 32/50, Batch 15/80
train_loss: 0.0097

Step 2500, Epoch 32/50, Batch 20/80
train_loss: 0.0083

Step 2505, Epoch 32/50, Batch 25/80
train_loss: 0.0098

Step 2510, Epoch 32/50, Batch 30/80
train_loss: 0.0123

Step 2515, Epoch 32/50, Batch 35/80
train_loss: 0.0101

Step 2520, Epoch 32/50, Batch 40/80
train_loss: 0.0094

Step 2525, Epoch 32/50, Batch 45/80
train_loss: 0.0089

Step 2530, Epoch 32/50, Batch 50/80
train_loss: 0.0111

Step 2535, Epoch 32/50, Batch 55/80
train_loss: 0.0086

Step 2540, Epoch 32/50, Batch 60/80
train_loss: 0.0109

Step 2545, Epoch 32/50, Batch 65/80
train_loss: 0.0091

Step 2550, Epoch 32/50, Batch 70/80
train_loss: 0.0082

Step 2555, Epoch 32/50, Batch 75/80
train_loss: 0.0092

Epoch 32/50 completed, Global Step: 2559
train_loss: 0.0092, val_loss: 0.0104

---------------------------------------------------------------------------


Step 2560, Epoch 33/50, Batch 0/80
train_loss: 0.0104

Step 2565, Epoch 33/50, Batch 5/80
train_loss: 0.0094

Step 2570, Epoch 33/50, Batch 10/80
train_loss: 0.0071

Step 2575, Epoch 33/50, Batch 15/80
train_loss: 0.0059

Step 2580, Epoch 33/50, Batch 20/80
train_loss: 0.0063

Step 2585, Epoch 33/50, Batch 25/80
train_loss: 0.0057

Step 2590, Epoch 33/50, Batch 30/80
train_loss: 0.0064

Step 2595, Epoch 33/50, Batch 35/80
train_loss: 0.0074

Step 2600, Epoch 33/50, Batch 40/80
train_loss: 0.0066

Step 2605, Epoch 33/50, Batch 45/80
train_loss: 0.0075

Step 2610, Epoch 33/50, Batch 50/80
train_loss: 0.0092

Step 2615, Epoch 33/50, Batch 55/80
train_loss: 0.0127

Step 2620, Epoch 33/50, Batch 60/80
train_loss: 0.0080

Step 2625, Epoch 33/50, Batch 65/80
train_loss: 0.0077

Step 2630, Epoch 33/50, Batch 70/80
train_loss: 0.0091

Step 2635, Epoch 33/50, Batch 75/80
train_loss: 0.0101

Epoch 33/50 completed, Global Step: 2639
train_loss: 0.0101, val_loss: 0.0093

---------------------------------------------------------------------------


Step 2640, Epoch 34/50, Batch 0/80
train_loss: 0.0094

Step 2645, Epoch 34/50, Batch 5/80
train_loss: 0.0069

Step 2650, Epoch 34/50, Batch 10/80
train_loss: 0.0082

Step 2655, Epoch 34/50, Batch 15/80
train_loss: 0.0076

Step 2660, Epoch 34/50, Batch 20/80
train_loss: 0.0093

Step 2665, Epoch 34/50, Batch 25/80
train_loss: 0.0081

Step 2670, Epoch 34/50, Batch 30/80
train_loss: 0.0086

Step 2675, Epoch 34/50, Batch 35/80
train_loss: 0.0077

Step 2680, Epoch 34/50, Batch 40/80
train_loss: 0.0094

Step 2685, Epoch 34/50, Batch 45/80
train_loss: 0.0072

Step 2690, Epoch 34/50, Batch 50/80
train_loss: 0.0060

Step 2695, Epoch 34/50, Batch 55/80
train_loss: 0.0075

Step 2700, Epoch 34/50, Batch 60/80
train_loss: 0.0065

Step 2705, Epoch 34/50, Batch 65/80
train_loss: 0.0094

Step 2710, Epoch 34/50, Batch 70/80
train_loss: 0.0083

Step 2715, Epoch 34/50, Batch 75/80
train_loss: 0.0103

Epoch 34/50 completed, Global Step: 2719
train_loss: 0.0103, val_loss: 0.0097

---------------------------------------------------------------------------


Step 2720, Epoch 35/50, Batch 0/80
train_loss: 0.0096

Step 2725, Epoch 35/50, Batch 5/80
train_loss: 0.0100

Step 2730, Epoch 35/50, Batch 10/80
train_loss: 0.0081

Step 2735, Epoch 35/50, Batch 15/80
train_loss: 0.0084

Step 2740, Epoch 35/50, Batch 20/80
train_loss: 0.0082

Step 2745, Epoch 35/50, Batch 25/80
train_loss: 0.0089

Step 2750, Epoch 35/50, Batch 30/80
train_loss: 0.0076

Step 2755, Epoch 35/50, Batch 35/80
train_loss: 0.0089

Step 2760, Epoch 35/50, Batch 40/80
train_loss: 0.0077

Step 2765, Epoch 35/50, Batch 45/80
train_loss: 0.0082

Step 2770, Epoch 35/50, Batch 50/80
train_loss: 0.0089

Step 2775, Epoch 35/50, Batch 55/80
train_loss: 0.0091

Step 2780, Epoch 35/50, Batch 60/80
train_loss: 0.0079

Step 2785, Epoch 35/50, Batch 65/80
train_loss: 0.0075

Step 2790, Epoch 35/50, Batch 70/80
train_loss: 0.0064

Step 2795, Epoch 35/50, Batch 75/80
train_loss: 0.0080

Epoch 35/50 completed, Global Step: 2799
train_loss: 0.0080, val_loss: 0.0089

---------------------------------------------------------------------------


Step 2800, Epoch 36/50, Batch 0/80
train_loss: 0.0087

Step 2805, Epoch 36/50, Batch 5/80
train_loss: 0.0079

Step 2810, Epoch 36/50, Batch 10/80
train_loss: 0.0076

Step 2815, Epoch 36/50, Batch 15/80
train_loss: 0.0070

Step 2820, Epoch 36/50, Batch 20/80
train_loss: 0.0082

Step 2825, Epoch 36/50, Batch 25/80
train_loss: 0.0080

Step 2830, Epoch 36/50, Batch 30/80
train_loss: 0.0112

Step 2835, Epoch 36/50, Batch 35/80
train_loss: 0.0096

Step 2840, Epoch 36/50, Batch 40/80
train_loss: 0.0070

Step 2845, Epoch 36/50, Batch 45/80
train_loss: 0.0070

Step 2850, Epoch 36/50, Batch 50/80
train_loss: 0.0070

Step 2855, Epoch 36/50, Batch 55/80
train_loss: 0.0062

Step 2860, Epoch 36/50, Batch 60/80
train_loss: 0.0072

Step 2865, Epoch 36/50, Batch 65/80
train_loss: 0.0110

Step 2870, Epoch 36/50, Batch 70/80
train_loss: 0.0083

Step 2875, Epoch 36/50, Batch 75/80
train_loss: 0.0072

Epoch 36/50 completed, Global Step: 2879
train_loss: 0.0072, val_loss: 0.0113

---------------------------------------------------------------------------


Step 2880, Epoch 37/50, Batch 0/80
train_loss: 0.0113

Step 2885, Epoch 37/50, Batch 5/80
train_loss: 0.0103

Step 2890, Epoch 37/50, Batch 10/80
train_loss: 0.0070

Step 2895, Epoch 37/50, Batch 15/80
train_loss: 0.0089

Step 2900, Epoch 37/50, Batch 20/80
train_loss: 0.0073

Step 2905, Epoch 37/50, Batch 25/80
train_loss: 0.0070

Step 2910, Epoch 37/50, Batch 30/80
train_loss: 0.0063

Step 2915, Epoch 37/50, Batch 35/80
train_loss: 0.0053

Step 2920, Epoch 37/50, Batch 40/80
train_loss: 0.0057

Step 2925, Epoch 37/50, Batch 45/80
train_loss: 0.0057

Step 2930, Epoch 37/50, Batch 50/80
train_loss: 0.0055

Step 2935, Epoch 37/50, Batch 55/80
train_loss: 0.0071

Step 2940, Epoch 37/50, Batch 60/80
train_loss: 0.0056

Step 2945, Epoch 37/50, Batch 65/80
train_loss: 0.0061

Step 2950, Epoch 37/50, Batch 70/80
train_loss: 0.0061

Step 2955, Epoch 37/50, Batch 75/80
train_loss: 0.0060

Epoch 37/50 completed, Global Step: 2959
train_loss: 0.0060, val_loss: 0.0053

---------------------------------------------------------------------------


Step 2960, Epoch 38/50, Batch 0/80
train_loss: 0.0052

Step 2965, Epoch 38/50, Batch 5/80
train_loss: 0.0067

Step 2970, Epoch 38/50, Batch 10/80
train_loss: 0.0080

Step 2975, Epoch 38/50, Batch 15/80
train_loss: 0.0066

Step 2980, Epoch 38/50, Batch 20/80
train_loss: 0.0063

Step 2985, Epoch 38/50, Batch 25/80
train_loss: 0.0090

Step 2990, Epoch 38/50, Batch 30/80
train_loss: 0.0093

Step 2995, Epoch 38/50, Batch 35/80
train_loss: 0.0071

Step 3000, Epoch 38/50, Batch 40/80
train_loss: 0.0076

Step 3005, Epoch 38/50, Batch 45/80
train_loss: 0.0079

Step 3010, Epoch 38/50, Batch 50/80
train_loss: 0.0096

Step 3015, Epoch 38/50, Batch 55/80
train_loss: 0.0088

Step 3020, Epoch 38/50, Batch 60/80
train_loss: 0.0067

Step 3025, Epoch 38/50, Batch 65/80
train_loss: 0.0076

Step 3030, Epoch 38/50, Batch 70/80
train_loss: 0.0104

Step 3035, Epoch 38/50, Batch 75/80
train_loss: 0.0084

Epoch 38/50 completed, Global Step: 3039
train_loss: 0.0084, val_loss: 0.0100

---------------------------------------------------------------------------


Step 3040, Epoch 39/50, Batch 0/80
train_loss: 0.0101

Step 3045, Epoch 39/50, Batch 5/80
train_loss: 0.0076

Step 3050, Epoch 39/50, Batch 10/80
train_loss: 0.0096

Step 3055, Epoch 39/50, Batch 15/80
train_loss: 0.0074

Step 3060, Epoch 39/50, Batch 20/80
train_loss: 0.0068

Step 3065, Epoch 39/50, Batch 25/80
train_loss: 0.0076

Step 3070, Epoch 39/50, Batch 30/80
train_loss: 0.0077

Step 3075, Epoch 39/50, Batch 35/80
train_loss: 0.0076

Step 3080, Epoch 39/50, Batch 40/80
train_loss: 0.0094

Step 3085, Epoch 39/50, Batch 45/80
train_loss: 0.0089

Step 3090, Epoch 39/50, Batch 50/80
train_loss: 0.0084

Step 3095, Epoch 39/50, Batch 55/80
train_loss: 0.0098

Step 3100, Epoch 39/50, Batch 60/80
train_loss: 0.0082

Step 3105, Epoch 39/50, Batch 65/80
train_loss: 0.0115

Step 3110, Epoch 39/50, Batch 70/80
train_loss: 0.0085

Step 3115, Epoch 39/50, Batch 75/80
train_loss: 0.0078

Epoch 39/50 completed, Global Step: 3119
train_loss: 0.0078, val_loss: 0.0070

---------------------------------------------------------------------------


Step 3120, Epoch 40/50, Batch 0/80
train_loss: 0.0073

Step 3125, Epoch 40/50, Batch 5/80
train_loss: 0.0076

Step 3130, Epoch 40/50, Batch 10/80
train_loss: 0.0071

Step 3135, Epoch 40/50, Batch 15/80
train_loss: 0.0065

Step 3140, Epoch 40/50, Batch 20/80
train_loss: 0.0057

Step 3145, Epoch 40/50, Batch 25/80
train_loss: 0.0050

Step 3150, Epoch 40/50, Batch 30/80
train_loss: 0.0060

Step 3155, Epoch 40/50, Batch 35/80
train_loss: 0.0052

Step 3160, Epoch 40/50, Batch 40/80
train_loss: 0.0072

Step 3165, Epoch 40/50, Batch 45/80
train_loss: 0.0073

Step 3170, Epoch 40/50, Batch 50/80
train_loss: 0.0056

Step 3175, Epoch 40/50, Batch 55/80
train_loss: 0.0057

Step 3180, Epoch 40/50, Batch 60/80
train_loss: 0.0063

Step 3185, Epoch 40/50, Batch 65/80
train_loss: 0.0059

Step 3190, Epoch 40/50, Batch 70/80
train_loss: 0.0049

Step 3195, Epoch 40/50, Batch 75/80
train_loss: 0.0057

Epoch 40/50 completed, Global Step: 3199
train_loss: 0.0057, val_loss: 0.0062

---------------------------------------------------------------------------


Step 3200, Epoch 41/50, Batch 0/80
train_loss: 0.0063

Step 3205, Epoch 41/50, Batch 5/80
train_loss: 0.0059

Step 3210, Epoch 41/50, Batch 10/80
train_loss: 0.0058

Step 3215, Epoch 41/50, Batch 15/80
train_loss: 0.0065

Step 3220, Epoch 41/50, Batch 20/80
train_loss: 0.0046

Step 3225, Epoch 41/50, Batch 25/80
train_loss: 0.0059

Step 3230, Epoch 41/50, Batch 30/80
train_loss: 0.0052

Step 3235, Epoch 41/50, Batch 35/80
train_loss: 0.0056

Step 3240, Epoch 41/50, Batch 40/80
train_loss: 0.0074

Step 3245, Epoch 41/50, Batch 45/80
train_loss: 0.0064

Step 3250, Epoch 41/50, Batch 50/80
train_loss: 0.0082

Step 3255, Epoch 41/50, Batch 55/80
train_loss: 0.0085

Step 3260, Epoch 41/50, Batch 60/80
train_loss: 0.0077

Step 3265, Epoch 41/50, Batch 65/80
train_loss: 0.0091

Step 3270, Epoch 41/50, Batch 70/80
train_loss: 0.0109

Step 3275, Epoch 41/50, Batch 75/80
train_loss: 0.0090

Epoch 41/50 completed, Global Step: 3279
train_loss: 0.0090, val_loss: 0.0104

---------------------------------------------------------------------------


Step 3280, Epoch 42/50, Batch 0/80
train_loss: 0.0102

Step 3285, Epoch 42/50, Batch 5/80
train_loss: 0.0080

Step 3290, Epoch 42/50, Batch 10/80
train_loss: 0.0085

Step 3295, Epoch 42/50, Batch 15/80
train_loss: 0.0078

Step 3300, Epoch 42/50, Batch 20/80
train_loss: 0.0090

Step 3305, Epoch 42/50, Batch 25/80
train_loss: 0.0078

Step 3310, Epoch 42/50, Batch 30/80
train_loss: 0.0089

Step 3315, Epoch 42/50, Batch 35/80
train_loss: 0.0082

Step 3320, Epoch 42/50, Batch 40/80
train_loss: 0.0086

Step 3325, Epoch 42/50, Batch 45/80
train_loss: 0.0092

Step 3330, Epoch 42/50, Batch 50/80
train_loss: 0.0078

Step 3335, Epoch 42/50, Batch 55/80
train_loss: 0.0074

Step 3340, Epoch 42/50, Batch 60/80
train_loss: 0.0079

Step 3345, Epoch 42/50, Batch 65/80
train_loss: 0.0089

Step 3350, Epoch 42/50, Batch 70/80
train_loss: 0.0082

Step 3355, Epoch 42/50, Batch 75/80
train_loss: 0.0065

Epoch 42/50 completed, Global Step: 3359
train_loss: 0.0065, val_loss: 0.0070

---------------------------------------------------------------------------


Step 3360, Epoch 43/50, Batch 0/80
train_loss: 0.0068

Step 3365, Epoch 43/50, Batch 5/80
train_loss: 0.0074

Step 3370, Epoch 43/50, Batch 10/80
train_loss: 0.0080

Step 3375, Epoch 43/50, Batch 15/80
train_loss: 0.0075

Step 3380, Epoch 43/50, Batch 20/80
train_loss: 0.0095

Step 3385, Epoch 43/50, Batch 25/80
train_loss: 0.0080

Step 3390, Epoch 43/50, Batch 30/80
train_loss: 0.0098

Step 3395, Epoch 43/50, Batch 35/80
train_loss: 0.0090

Step 3400, Epoch 43/50, Batch 40/80
train_loss: 0.0080

Step 3405, Epoch 43/50, Batch 45/80
train_loss: 0.0064

Step 3410, Epoch 43/50, Batch 50/80
train_loss: 0.0076

Step 3415, Epoch 43/50, Batch 55/80
train_loss: 0.0077

Step 3420, Epoch 43/50, Batch 60/80
train_loss: 0.0066

Step 3425, Epoch 43/50, Batch 65/80
train_loss: 0.0064

Step 3430, Epoch 43/50, Batch 70/80
train_loss: 0.0051

Step 3435, Epoch 43/50, Batch 75/80
train_loss: 0.0048

Epoch 43/50 completed, Global Step: 3439
train_loss: 0.0048, val_loss: 0.0054

---------------------------------------------------------------------------


Step 3440, Epoch 44/50, Batch 0/80
train_loss: 0.0051

Step 3445, Epoch 44/50, Batch 5/80
train_loss: 0.0055

Step 3450, Epoch 44/50, Batch 10/80
train_loss: 0.0049

Step 3455, Epoch 44/50, Batch 15/80
train_loss: 0.0064

Step 3460, Epoch 44/50, Batch 20/80
train_loss: 0.0076

Step 3465, Epoch 44/50, Batch 25/80
train_loss: 0.0068

Step 3470, Epoch 44/50, Batch 30/80
train_loss: 0.0090

Step 3475, Epoch 44/50, Batch 35/80
train_loss: 0.0074

Step 3480, Epoch 44/50, Batch 40/80
train_loss: 0.0077

Step 3485, Epoch 44/50, Batch 45/80
train_loss: 0.0074

Step 3490, Epoch 44/50, Batch 50/80
train_loss: 0.0076

Step 3495, Epoch 44/50, Batch 55/80
train_loss: 0.0076

Step 3500, Epoch 44/50, Batch 60/80
train_loss: 0.0078

Step 3505, Epoch 44/50, Batch 65/80
train_loss: 0.0073

Step 3510, Epoch 44/50, Batch 70/80
train_loss: 0.0080

Step 3515, Epoch 44/50, Batch 75/80
train_loss: 0.0086

Epoch 44/50 completed, Global Step: 3519
train_loss: 0.0086, val_loss: 0.0086

---------------------------------------------------------------------------


Step 3520, Epoch 45/50, Batch 0/80
train_loss: 0.0086

Step 3525, Epoch 45/50, Batch 5/80
train_loss: 0.0069

Step 3530, Epoch 45/50, Batch 10/80
train_loss: 0.0079

Step 3535, Epoch 45/50, Batch 15/80
train_loss: 0.0067

Step 3540, Epoch 45/50, Batch 20/80
train_loss: 0.0072

Step 3545, Epoch 45/50, Batch 25/80
train_loss: 0.0068

Step 3550, Epoch 45/50, Batch 30/80
train_loss: 0.0076

Step 3555, Epoch 45/50, Batch 35/80
train_loss: 0.0070

Step 3560, Epoch 45/50, Batch 40/80
train_loss: 0.0082

Step 3565, Epoch 45/50, Batch 45/80
train_loss: 0.0074

Step 3570, Epoch 45/50, Batch 50/80
train_loss: 0.0067

Step 3575, Epoch 45/50, Batch 55/80
train_loss: 0.0074

Step 3580, Epoch 45/50, Batch 60/80
train_loss: 0.0059

Step 3585, Epoch 45/50, Batch 65/80
train_loss: 0.0069

Step 3590, Epoch 45/50, Batch 70/80
train_loss: 0.0067

Step 3595, Epoch 45/50, Batch 75/80
train_loss: 0.0061

Epoch 45/50 completed, Global Step: 3599
train_loss: 0.0061, val_loss: 0.0072

---------------------------------------------------------------------------


Step 3600, Epoch 46/50, Batch 0/80
train_loss: 0.0073

Step 3605, Epoch 46/50, Batch 5/80
train_loss: 0.0065

Step 3610, Epoch 46/50, Batch 10/80
train_loss: 0.0067

Step 3615, Epoch 46/50, Batch 15/80
train_loss: 0.0066

Step 3620, Epoch 46/50, Batch 20/80
train_loss: 0.0093

Step 3625, Epoch 46/50, Batch 25/80
train_loss: 0.0099

Step 3630, Epoch 46/50, Batch 30/80
train_loss: 0.0095

Step 3635, Epoch 46/50, Batch 35/80
train_loss: 0.0079

Step 3640, Epoch 46/50, Batch 40/80
train_loss: 0.0068

Step 3645, Epoch 46/50, Batch 45/80
train_loss: 0.0074

Step 3650, Epoch 46/50, Batch 50/80
train_loss: 0.0059

Step 3655, Epoch 46/50, Batch 55/80
train_loss: 0.0070

Step 3660, Epoch 46/50, Batch 60/80
train_loss: 0.0058

Step 3665, Epoch 46/50, Batch 65/80
train_loss: 0.0060

Step 3670, Epoch 46/50, Batch 70/80
train_loss: 0.0055

Step 3675, Epoch 46/50, Batch 75/80
train_loss: 0.0066

Epoch 46/50 completed, Global Step: 3679
train_loss: 0.0066, val_loss: 0.0055

---------------------------------------------------------------------------


Step 3680, Epoch 47/50, Batch 0/80
train_loss: 0.0052

Step 3685, Epoch 47/50, Batch 5/80
train_loss: 0.0045

Step 3690, Epoch 47/50, Batch 10/80
train_loss: 0.0041

Step 3695, Epoch 47/50, Batch 15/80
train_loss: 0.0051

Step 3700, Epoch 47/50, Batch 20/80
train_loss: 0.0050

Step 3705, Epoch 47/50, Batch 25/80
train_loss: 0.0043

Step 3710, Epoch 47/50, Batch 30/80
train_loss: 0.0037

Step 3715, Epoch 47/50, Batch 35/80
train_loss: 0.0052

Step 3720, Epoch 47/50, Batch 40/80
train_loss: 0.0063

Step 3725, Epoch 47/50, Batch 45/80
train_loss: 0.0055

Step 3730, Epoch 47/50, Batch 50/80
train_loss: 0.0053

Step 3735, Epoch 47/50, Batch 55/80
train_loss: 0.0082

Step 3740, Epoch 47/50, Batch 60/80
train_loss: 0.0081

Step 3745, Epoch 47/50, Batch 65/80
train_loss: 0.0066

Step 3750, Epoch 47/50, Batch 70/80
train_loss: 0.0082

Step 3755, Epoch 47/50, Batch 75/80
train_loss: 0.0065

Epoch 47/50 completed, Global Step: 3759
train_loss: 0.0065, val_loss: 0.0062

---------------------------------------------------------------------------


Step 3760, Epoch 48/50, Batch 0/80
train_loss: 0.0062

Step 3765, Epoch 48/50, Batch 5/80
train_loss: 0.0073

Step 3770, Epoch 48/50, Batch 10/80
train_loss: 0.0076

Step 3775, Epoch 48/50, Batch 15/80
train_loss: 0.0081

Step 3780, Epoch 48/50, Batch 20/80
train_loss: 0.0079

Step 3785, Epoch 48/50, Batch 25/80
train_loss: 0.0070

Step 3790, Epoch 48/50, Batch 30/80
train_loss: 0.0073

Step 3795, Epoch 48/50, Batch 35/80
train_loss: 0.0059

Step 3800, Epoch 48/50, Batch 40/80
train_loss: 0.0055

Step 3805, Epoch 48/50, Batch 45/80
train_loss: 0.0066

Step 3810, Epoch 48/50, Batch 50/80
train_loss: 0.0050

Step 3815, Epoch 48/50, Batch 55/80
train_loss: 0.0088

Step 3820, Epoch 48/50, Batch 60/80
train_loss: 0.0066

Step 3825, Epoch 48/50, Batch 65/80
train_loss: 0.0094

Step 3830, Epoch 48/50, Batch 70/80
train_loss: 0.0069

Step 3835, Epoch 48/50, Batch 75/80
train_loss: 0.0072

Epoch 48/50 completed, Global Step: 3839
train_loss: 0.0072, val_loss: 0.0079

---------------------------------------------------------------------------


Step 3840, Epoch 49/50, Batch 0/80
train_loss: 0.0079

Step 3845, Epoch 49/50, Batch 5/80
train_loss: 0.0070

Step 3850, Epoch 49/50, Batch 10/80
train_loss: 0.0083

Step 3855, Epoch 49/50, Batch 15/80
train_loss: 0.0069

Step 3860, Epoch 49/50, Batch 20/80
train_loss: 0.0068

Step 3865, Epoch 49/50, Batch 25/80
train_loss: 0.0075

Step 3870, Epoch 49/50, Batch 30/80
train_loss: 0.0065

Step 3875, Epoch 49/50, Batch 35/80
train_loss: 0.0072

Step 3880, Epoch 49/50, Batch 40/80
train_loss: 0.0056

Step 3885, Epoch 49/50, Batch 45/80
train_loss: 0.0052

Step 3890, Epoch 49/50, Batch 50/80
train_loss: 0.0076

Step 3895, Epoch 49/50, Batch 55/80
train_loss: 0.0065

Step 3900, Epoch 49/50, Batch 60/80
train_loss: 0.0053

Step 3905, Epoch 49/50, Batch 65/80
train_loss: 0.0083

Step 3910, Epoch 49/50, Batch 70/80
train_loss: 0.0060

Step 3915, Epoch 49/50, Batch 75/80
train_loss: 0.0054

Epoch 49/50 completed, Global Step: 3919
train_loss: 0.0054, val_loss: 0.0068

---------------------------------------------------------------------------


Step 3920, Epoch 50/50, Batch 0/80
train_loss: 0.0069

Step 3925, Epoch 50/50, Batch 5/80
train_loss: 0.0065

Step 3930, Epoch 50/50, Batch 10/80
train_loss: 0.0076

Step 3935, Epoch 50/50, Batch 15/80
train_loss: 0.0063

Step 3940, Epoch 50/50, Batch 20/80
train_loss: 0.0069

Step 3945, Epoch 50/50, Batch 25/80
train_loss: 0.0063

Step 3950, Epoch 50/50, Batch 30/80
train_loss: 0.0080

Step 3955, Epoch 50/50, Batch 35/80
train_loss: 0.0047

Step 3960, Epoch 50/50, Batch 40/80
train_loss: 0.0061

Step 3965, Epoch 50/50, Batch 45/80
train_loss: 0.0055

Step 3970, Epoch 50/50, Batch 50/80
train_loss: 0.0068

Step 3975, Epoch 50/50, Batch 55/80
train_loss: 0.0065

Step 3980, Epoch 50/50, Batch 60/80
train_loss: 0.0080

Step 3985, Epoch 50/50, Batch 65/80
train_loss: 0.0054

Step 3990, Epoch 50/50, Batch 70/80
train_loss: 0.0051

Step 3995, Epoch 50/50, Batch 75/80
train_loss: 0.0049

Epoch 50/50 completed, Global Step: 3999
train_loss: 0.0049, val_loss: 0.0085

---------------------------------------------------------------------------


Training completed in 1324.96 seconds or 22.08 minutes or 0.368043752974934 hours.
Total training steps: 3999

Training completed for model '[m004_(apv+G)]-gru_dec_1.4'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m004_(apv+G)]-gru_dec_1.4...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.4022' for [m004_(apv+G)]-gru_dec_1.4...

Decoder output plot for rep '1001.4022' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.2555' for [m004_(apv+G)]-gru_dec_1.4...

Decoder output plot for rep '1001.2555' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4


---------------------------------------------------------------------------

TESTING TRAINED DECODER MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4\checkpoints:

['best-model-epoch=36-val_loss=0.0053.ckpt']

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4\test

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Trained Decoder Model Loaded for testing.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                                                                          | 0/? [00:00<?, ?it/s]
Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                                                                     | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|                                                        | 0/10 [00:00<?, ?it/s]
Found rep_num = 1001.0001 in batch 0 of epoch 36. Decoder output plot will be made for this data.
Testing DataLoader 0:  10%|####8                                           | 1/10 [00:00<00:02,  3.89it/s]Testing DataLoader 0:  20%|#########6                                      | 2/10 [00:00<00:01,  5.11it/s]Testing DataLoader 0:  30%|##############4                                 | 3/10 [00:00<00:01,  5.68it/s]Testing DataLoader 0:  40%|###################2                            | 4/10 [00:00<00:00,  6.15it/s]Testing DataLoader 0:  50%|########################                        | 5/10 [00:00<00:00,  6.47it/s]Testing DataLoader 0:  60%|############################8                   | 6/10 [00:00<00:00,  6.72it/s]Testing DataLoader 0:  70%|#################################5              | 7/10 [00:01<00:00,  6.96it/s]Testing DataLoader 0:  80%|######################################4         | 8/10 [00:01<00:00,  7.20it/s]Testing DataLoader 0:  90%|###########################################2    | 9/10 [00:01<00:00,  7.39it/s]Testing DataLoader 0: 100%|###############################################| 10/10 [00:01<00:00,  7.46it/s]
Testing completed in 1.34 seconds or 0.02 minutes or 0.000373255279329088 hours.

test_loss: 0.0057

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.0001' for [m004_(apv+G)]-gru_dec_1.4...

Decoder output plot for rep '1001.0001' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.4\test

Testing DataLoader 0: 100%|###############################################| 10/10 [00:03<00:00,  3.00it/s]

        Test metric               DataLoader 0        

         test_loss            0.005725277122110128    


===========================================================================

Decoder model '[m004_(apv+G)]-gru_dec_1.4' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-21 19:23:06
