=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m004_(apv+G)]-gru_dec_1.5
Start Time: 2025-09-21 19:24:16
End Time: 2025-09-21 19:46:34

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting decoder model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]

Node group name: m004
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.

Target rep_num 1001.0001 found at index 0. This sample will be included in the test set.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 450/500 [OK=450, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 9
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (4, 4)
     n1   n2   n3   n4
n1  0.0  1.0  1.0  1.0
n2  1.0  0.0  1.0  1.0
n3  1.0  1.0  0.0  1.0
n4  1.0  1.0  1.0  0.0


**Receiver relation matrix** => shape: (12, 4)
      n1   n2   n3   n4
e12  0.0  1.0  0.0  0.0
e13  0.0  0.0  1.0  0.0
e14  0.0  0.0  0.0  1.0
e21  1.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0
e24  0.0  0.0  0.0  1.0
e31  1.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0
e34  0.0  0.0  0.0  1.0
e41  1.0  0.0  0.0  0.0
e42  0.0  1.0  0.0  0.0
e43  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (12, 4)
      n1   n2   n3   n4
e12  1.0  0.0  0.0  0.0
e13  1.0  0.0  0.0  0.0
e14  1.0  0.0  0.0  0.0
e21  0.0  1.0  0.0  0.0
e23  0.0  1.0  0.0  0.0
e24  0.0  1.0  0.0  0.0
e31  0.0  0.0  1.0  0.0
e32  0.0  0.0  1.0  0.0
e34  0.0  0.0  1.0  0.0
e41  0.0  0.0  0.0  1.0
e42  0.0  0.0  0.0  1.0
e43  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 1
msg_out_size: 128
edge_mlp_config: [[128, 'tanh'], [128, 'tanh']]
out_mlp_config: [[128, 'relu'], [128, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
n_dims: 3

Decoder run parameters:
-------------------------
skip_first_edge_type: False
pred_steps: 10
is_burn_in: True
final_pred_steps: 50
is_dynamic_graph: False
temp: 1.0
is_hard: True
show_conf_band: False

'[m004_(apv+G)]-gru_dec_1.2' already exists in the log path 'C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2'.
(a) Overwrite exsiting version, (b) create new version, (c) stop training (Choose 'a', 'b' or 'c'):  ['[m004_(apv+G)]-gru_dec_1.1', '[m004_(apv+G)]-gru_dec_1.2', '[m004_(apv+G)]-gru_dec_1.3', '[m004_(apv+G)]-gru_dec_1.4']
Next decoder folder will be: [m004_(apv+G)]-gru_dec_1.5
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5

---------------------------------------------------------------------------

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Training parameters set to: 
lr=0.001, 
optimizer=adam, 
loss_type=mae

---------------------------------------------------------------------------

Decoder Model Initialized with the following configurations:

Decoder Model Summary:
Decoder(
  (edge_mlp_fn): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=128, out_features=128, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=128, bias=True)
    (hidden_u): Linear(in_features=128, out_features=128, bias=True)
    (input_r): Linear(in_features=3, out_features=128, bias=True)
    (hidden_r): Linear(in_features=128, out_features=128, bias=True)
    (input_h): Linear(in_features=3, out_features=128, bias=True)
    (hidden_h): Linear(in_features=128, out_features=128, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=128, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=128, out_features=3, bias=True)
)

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Step 0, Epoch 1/50, Batch 0/80
train_loss: 0.6772

Step 5, Epoch 1/50, Batch 5/80
train_loss: 0.2595

Step 10, Epoch 1/50, Batch 10/80
train_loss: 0.1310

Step 15, Epoch 1/50, Batch 15/80
train_loss: 0.1377

Step 20, Epoch 1/50, Batch 20/80
train_loss: 0.1407

Step 25, Epoch 1/50, Batch 25/80
train_loss: 0.1288

Step 30, Epoch 1/50, Batch 30/80
train_loss: 0.1207

Step 35, Epoch 1/50, Batch 35/80
train_loss: 0.1082

Step 40, Epoch 1/50, Batch 40/80
train_loss: 0.1093

Step 45, Epoch 1/50, Batch 45/80
train_loss: 0.1015

Step 50, Epoch 1/50, Batch 50/80
train_loss: 0.1009

Step 55, Epoch 1/50, Batch 55/80
train_loss: 0.1021

Step 60, Epoch 1/50, Batch 60/80
train_loss: 0.1114

Step 65, Epoch 1/50, Batch 65/80
train_loss: 0.1025

Step 70, Epoch 1/50, Batch 70/80
train_loss: 0.1006

Step 75, Epoch 1/50, Batch 75/80
train_loss: 0.1019

Epoch 1/50 completed, Global Step: 79
train_loss: 0.1019, val_loss: 0.0959

---------------------------------------------------------------------------


Step 80, Epoch 2/50, Batch 0/80
train_loss: 0.0967

Step 85, Epoch 2/50, Batch 5/80
train_loss: 0.1002

Step 90, Epoch 2/50, Batch 10/80
train_loss: 0.0946

Step 95, Epoch 2/50, Batch 15/80
train_loss: 0.0938

Step 100, Epoch 2/50, Batch 20/80
train_loss: 0.0963

Step 105, Epoch 2/50, Batch 25/80
train_loss: 0.0969

Step 110, Epoch 2/50, Batch 30/80
train_loss: 0.0933

Step 115, Epoch 2/50, Batch 35/80
train_loss: 0.1020

Step 120, Epoch 2/50, Batch 40/80
train_loss: 0.0973

Step 125, Epoch 2/50, Batch 45/80
train_loss: 0.0973

Step 130, Epoch 2/50, Batch 50/80
train_loss: 0.0936

Step 135, Epoch 2/50, Batch 55/80
train_loss: 0.0909

Step 140, Epoch 2/50, Batch 60/80
train_loss: 0.0912

Step 145, Epoch 2/50, Batch 65/80
train_loss: 0.0916

Step 150, Epoch 2/50, Batch 70/80
train_loss: 0.0953

Step 155, Epoch 2/50, Batch 75/80
train_loss: 0.0907

Epoch 2/50 completed, Global Step: 159
train_loss: 0.0907, val_loss: 0.0896

---------------------------------------------------------------------------


Step 160, Epoch 3/50, Batch 0/80
train_loss: 0.0903

Step 165, Epoch 3/50, Batch 5/80
train_loss: 0.0927

Step 170, Epoch 3/50, Batch 10/80
train_loss: 0.0915

Step 175, Epoch 3/50, Batch 15/80
train_loss: 0.0895

Step 180, Epoch 3/50, Batch 20/80
train_loss: 0.0902

Step 185, Epoch 3/50, Batch 25/80
train_loss: 0.0878

Step 190, Epoch 3/50, Batch 30/80
train_loss: 0.0867

Step 195, Epoch 3/50, Batch 35/80
train_loss: 0.0847

Step 200, Epoch 3/50, Batch 40/80
train_loss: 0.0861

Step 205, Epoch 3/50, Batch 45/80
train_loss: 0.0898

Step 210, Epoch 3/50, Batch 50/80
train_loss: 0.0831

Step 215, Epoch 3/50, Batch 55/80
train_loss: 0.0880

Step 220, Epoch 3/50, Batch 60/80
train_loss: 0.0849

Step 225, Epoch 3/50, Batch 65/80
train_loss: 0.0823

Step 230, Epoch 3/50, Batch 70/80
train_loss: 0.0817

Step 235, Epoch 3/50, Batch 75/80
train_loss: 0.0813

Epoch 3/50 completed, Global Step: 239
train_loss: 0.0813, val_loss: 0.0819

---------------------------------------------------------------------------


Step 240, Epoch 4/50, Batch 0/80
train_loss: 0.0829

Step 245, Epoch 4/50, Batch 5/80
train_loss: 0.0820

Step 250, Epoch 4/50, Batch 10/80
train_loss: 0.0841

Step 255, Epoch 4/50, Batch 15/80
train_loss: 0.0827

Step 260, Epoch 4/50, Batch 20/80
train_loss: 0.0836

Step 265, Epoch 4/50, Batch 25/80
train_loss: 0.0816

Step 270, Epoch 4/50, Batch 30/80
train_loss: 0.0815

Step 275, Epoch 4/50, Batch 35/80
train_loss: 0.0802

Step 280, Epoch 4/50, Batch 40/80
train_loss: 0.0794

Step 285, Epoch 4/50, Batch 45/80
train_loss: 0.0778

Step 290, Epoch 4/50, Batch 50/80
train_loss: 0.0787

Step 295, Epoch 4/50, Batch 55/80
train_loss: 0.0856

Step 300, Epoch 4/50, Batch 60/80
train_loss: 0.0911

Step 305, Epoch 4/50, Batch 65/80
train_loss: 0.0844

Step 310, Epoch 4/50, Batch 70/80
train_loss: 0.0826

Step 315, Epoch 4/50, Batch 75/80
train_loss: 0.0808

Epoch 4/50 completed, Global Step: 319
train_loss: 0.0808, val_loss: 0.0792

---------------------------------------------------------------------------


Step 320, Epoch 5/50, Batch 0/80
train_loss: 0.0797

Step 325, Epoch 5/50, Batch 5/80
train_loss: 0.0778

Step 330, Epoch 5/50, Batch 10/80
train_loss: 0.0783

Step 335, Epoch 5/50, Batch 15/80
train_loss: 0.0789

Step 340, Epoch 5/50, Batch 20/80
train_loss: 0.0771

Step 345, Epoch 5/50, Batch 25/80
train_loss: 0.0794

Step 350, Epoch 5/50, Batch 30/80
train_loss: 0.0774

Step 355, Epoch 5/50, Batch 35/80
train_loss: 0.0781

Step 360, Epoch 5/50, Batch 40/80
train_loss: 0.0774

Step 365, Epoch 5/50, Batch 45/80
train_loss: 0.0781

Step 370, Epoch 5/50, Batch 50/80
train_loss: 0.0795

Step 375, Epoch 5/50, Batch 55/80
train_loss: 0.0761

Step 380, Epoch 5/50, Batch 60/80
train_loss: 0.0774

Step 385, Epoch 5/50, Batch 65/80
train_loss: 0.0760

Step 390, Epoch 5/50, Batch 70/80
train_loss: 0.0766

Step 395, Epoch 5/50, Batch 75/80
train_loss: 0.0764

Epoch 5/50 completed, Global Step: 399
train_loss: 0.0764, val_loss: 0.0777

---------------------------------------------------------------------------


Step 400, Epoch 6/50, Batch 0/80
train_loss: 0.0766

Step 405, Epoch 6/50, Batch 5/80
train_loss: 0.0788

Step 410, Epoch 6/50, Batch 10/80
train_loss: 0.0752

Step 415, Epoch 6/50, Batch 15/80
train_loss: 0.0808

Step 420, Epoch 6/50, Batch 20/80
train_loss: 0.0807

Step 425, Epoch 6/50, Batch 25/80
train_loss: 0.0792

Step 430, Epoch 6/50, Batch 30/80
train_loss: 0.0780

Step 435, Epoch 6/50, Batch 35/80
train_loss: 0.0768

Step 440, Epoch 6/50, Batch 40/80
train_loss: 0.0751

Step 445, Epoch 6/50, Batch 45/80
train_loss: 0.0753

Step 450, Epoch 6/50, Batch 50/80
train_loss: 0.0735

Step 455, Epoch 6/50, Batch 55/80
train_loss: 0.0770

Step 460, Epoch 6/50, Batch 60/80
train_loss: 0.0751

Step 465, Epoch 6/50, Batch 65/80
train_loss: 0.0736

Step 470, Epoch 6/50, Batch 70/80
train_loss: 0.0733

Step 475, Epoch 6/50, Batch 75/80
train_loss: 0.0784

Epoch 6/50 completed, Global Step: 479
train_loss: 0.0784, val_loss: 0.0731

---------------------------------------------------------------------------


Step 480, Epoch 7/50, Batch 0/80
train_loss: 0.0740

Step 485, Epoch 7/50, Batch 5/80
train_loss: 0.0741

Step 490, Epoch 7/50, Batch 10/80
train_loss: 0.0757

Step 495, Epoch 7/50, Batch 15/80
train_loss: 0.0728

Step 500, Epoch 7/50, Batch 20/80
train_loss: 0.0731

Step 505, Epoch 7/50, Batch 25/80
train_loss: 0.0727

Step 510, Epoch 7/50, Batch 30/80
train_loss: 0.0729

Step 515, Epoch 7/50, Batch 35/80
train_loss: 0.0753

Step 520, Epoch 7/50, Batch 40/80
train_loss: 0.0761

Step 525, Epoch 7/50, Batch 45/80
train_loss: 0.0777

Step 530, Epoch 7/50, Batch 50/80
train_loss: 0.0731

Step 535, Epoch 7/50, Batch 55/80
train_loss: 0.0714

Step 540, Epoch 7/50, Batch 60/80
train_loss: 0.0723

Step 545, Epoch 7/50, Batch 65/80
train_loss: 0.0731

Step 550, Epoch 7/50, Batch 70/80
train_loss: 0.0712

Step 555, Epoch 7/50, Batch 75/80
train_loss: 0.0686

Epoch 7/50 completed, Global Step: 559
train_loss: 0.0686, val_loss: 0.0699

---------------------------------------------------------------------------


Step 560, Epoch 8/50, Batch 0/80
train_loss: 0.0710

Step 565, Epoch 8/50, Batch 5/80
train_loss: 0.0682

Step 570, Epoch 8/50, Batch 10/80
train_loss: 0.0679

Step 575, Epoch 8/50, Batch 15/80
train_loss: 0.0666

Step 580, Epoch 8/50, Batch 20/80
train_loss: 0.0679

Step 585, Epoch 8/50, Batch 25/80
train_loss: 0.0786

Step 590, Epoch 8/50, Batch 30/80
train_loss: 0.0699

Step 595, Epoch 8/50, Batch 35/80
train_loss: 0.0693

Step 600, Epoch 8/50, Batch 40/80
train_loss: 0.0662

Step 605, Epoch 8/50, Batch 45/80
train_loss: 0.0647

Step 610, Epoch 8/50, Batch 50/80
train_loss: 0.0627

Step 615, Epoch 8/50, Batch 55/80
train_loss: 0.0617

Step 620, Epoch 8/50, Batch 60/80
train_loss: 0.0667

Step 625, Epoch 8/50, Batch 65/80
train_loss: 0.0626

Step 630, Epoch 8/50, Batch 70/80
train_loss: 0.0634

Step 635, Epoch 8/50, Batch 75/80
train_loss: 0.0654

Epoch 8/50 completed, Global Step: 639
train_loss: 0.0654, val_loss: 0.0616

---------------------------------------------------------------------------


Step 640, Epoch 9/50, Batch 0/80
train_loss: 0.0627

Step 645, Epoch 9/50, Batch 5/80
train_loss: 0.0617

Step 650, Epoch 9/50, Batch 10/80
train_loss: 0.0682

Step 655, Epoch 9/50, Batch 15/80
train_loss: 0.0633

Step 660, Epoch 9/50, Batch 20/80
train_loss: 0.0525

Step 665, Epoch 9/50, Batch 25/80
train_loss: 0.0584

Step 670, Epoch 9/50, Batch 30/80
train_loss: 0.0496

Step 675, Epoch 9/50, Batch 35/80
train_loss: 0.0431

Step 680, Epoch 9/50, Batch 40/80
train_loss: 0.0474

Step 685, Epoch 9/50, Batch 45/80
train_loss: 0.0426

Step 690, Epoch 9/50, Batch 50/80
train_loss: 0.0440

Step 695, Epoch 9/50, Batch 55/80
train_loss: 0.0412

Step 700, Epoch 9/50, Batch 60/80
train_loss: 0.0391

Step 705, Epoch 9/50, Batch 65/80
train_loss: 0.0382

Step 710, Epoch 9/50, Batch 70/80
train_loss: 0.0381

Step 715, Epoch 9/50, Batch 75/80
train_loss: 0.0413

Epoch 9/50 completed, Global Step: 719
train_loss: 0.0413, val_loss: 0.0377

---------------------------------------------------------------------------


Step 720, Epoch 10/50, Batch 0/80
train_loss: 0.0381

Step 725, Epoch 10/50, Batch 5/80
train_loss: 0.0406

Step 730, Epoch 10/50, Batch 10/80
train_loss: 0.0391

Step 735, Epoch 10/50, Batch 15/80
train_loss: 0.0407

Step 740, Epoch 10/50, Batch 20/80
train_loss: 0.0400

Step 745, Epoch 10/50, Batch 25/80
train_loss: 0.0376

Step 750, Epoch 10/50, Batch 30/80
train_loss: 0.0389

Step 755, Epoch 10/50, Batch 35/80
train_loss: 0.0406

Step 760, Epoch 10/50, Batch 40/80
train_loss: 0.0371

Step 765, Epoch 10/50, Batch 45/80
train_loss: 0.0341

Step 770, Epoch 10/50, Batch 50/80
train_loss: 0.0393

Step 775, Epoch 10/50, Batch 55/80
train_loss: 0.0389

Step 780, Epoch 10/50, Batch 60/80
train_loss: 0.0355

Step 785, Epoch 10/50, Batch 65/80
train_loss: 0.0385

Step 790, Epoch 10/50, Batch 70/80
train_loss: 0.0395

Step 795, Epoch 10/50, Batch 75/80
train_loss: 0.0350

Epoch 10/50 completed, Global Step: 799
train_loss: 0.0350, val_loss: 0.0323

---------------------------------------------------------------------------


Step 800, Epoch 11/50, Batch 0/80
train_loss: 0.0333

Step 805, Epoch 11/50, Batch 5/80
train_loss: 0.0386

Step 810, Epoch 11/50, Batch 10/80
train_loss: 0.0386

Step 815, Epoch 11/50, Batch 15/80
train_loss: 0.0330

Step 820, Epoch 11/50, Batch 20/80
train_loss: 0.0393

Step 825, Epoch 11/50, Batch 25/80
train_loss: 0.0381

Step 830, Epoch 11/50, Batch 30/80
train_loss: 0.0293

Step 835, Epoch 11/50, Batch 35/80
train_loss: 0.0291

Step 840, Epoch 11/50, Batch 40/80
train_loss: 0.0296

Step 845, Epoch 11/50, Batch 45/80
train_loss: 0.0291

Step 850, Epoch 11/50, Batch 50/80
train_loss: 0.0284

Step 855, Epoch 11/50, Batch 55/80
train_loss: 0.0280

Step 860, Epoch 11/50, Batch 60/80
train_loss: 0.0290

Step 865, Epoch 11/50, Batch 65/80
train_loss: 0.0307

Step 870, Epoch 11/50, Batch 70/80
train_loss: 0.0386

Step 875, Epoch 11/50, Batch 75/80
train_loss: 0.0314

Epoch 11/50 completed, Global Step: 879
train_loss: 0.0314, val_loss: 0.0312

---------------------------------------------------------------------------


Step 880, Epoch 12/50, Batch 0/80
train_loss: 0.0326

Step 885, Epoch 12/50, Batch 5/80
train_loss: 0.0296

Step 890, Epoch 12/50, Batch 10/80
train_loss: 0.0265

Step 895, Epoch 12/50, Batch 15/80
train_loss: 0.0276

Step 900, Epoch 12/50, Batch 20/80
train_loss: 0.0260

Step 905, Epoch 12/50, Batch 25/80
train_loss: 0.0282

Step 910, Epoch 12/50, Batch 30/80
train_loss: 0.0265

Step 915, Epoch 12/50, Batch 35/80
train_loss: 0.0260

Step 920, Epoch 12/50, Batch 40/80
train_loss: 0.0267

Step 925, Epoch 12/50, Batch 45/80
train_loss: 0.0273

Step 930, Epoch 12/50, Batch 50/80
train_loss: 0.0258

Step 935, Epoch 12/50, Batch 55/80
train_loss: 0.0316

Step 940, Epoch 12/50, Batch 60/80
train_loss: 0.0274

Step 945, Epoch 12/50, Batch 65/80
train_loss: 0.0275

Step 950, Epoch 12/50, Batch 70/80
train_loss: 0.0258

Step 955, Epoch 12/50, Batch 75/80
train_loss: 0.0278

Epoch 12/50 completed, Global Step: 959
train_loss: 0.0278, val_loss: 0.0270

---------------------------------------------------------------------------


Step 960, Epoch 13/50, Batch 0/80
train_loss: 0.0282

Step 965, Epoch 13/50, Batch 5/80
train_loss: 0.0264

Step 970, Epoch 13/50, Batch 10/80
train_loss: 0.0272

Step 975, Epoch 13/50, Batch 15/80
train_loss: 0.0249

Step 980, Epoch 13/50, Batch 20/80
train_loss: 0.0292

Step 985, Epoch 13/50, Batch 25/80
train_loss: 0.0260

Step 990, Epoch 13/50, Batch 30/80
train_loss: 0.0270

Step 995, Epoch 13/50, Batch 35/80
train_loss: 0.0256

Step 1000, Epoch 13/50, Batch 40/80
train_loss: 0.0269

Step 1005, Epoch 13/50, Batch 45/80
train_loss: 0.0253

Step 1010, Epoch 13/50, Batch 50/80
train_loss: 0.0254

Step 1015, Epoch 13/50, Batch 55/80
train_loss: 0.0256

Step 1020, Epoch 13/50, Batch 60/80
train_loss: 0.0252

Step 1025, Epoch 13/50, Batch 65/80
train_loss: 0.0263

Step 1030, Epoch 13/50, Batch 70/80
train_loss: 0.0323

Step 1035, Epoch 13/50, Batch 75/80
train_loss: 0.0276

Epoch 13/50 completed, Global Step: 1039
train_loss: 0.0276, val_loss: 0.0272

---------------------------------------------------------------------------


Step 1040, Epoch 14/50, Batch 0/80
train_loss: 0.0274

Step 1045, Epoch 14/50, Batch 5/80
train_loss: 0.0258

Step 1050, Epoch 14/50, Batch 10/80
train_loss: 0.0240

Step 1055, Epoch 14/50, Batch 15/80
train_loss: 0.0278

Step 1060, Epoch 14/50, Batch 20/80
train_loss: 0.0259

Step 1065, Epoch 14/50, Batch 25/80
train_loss: 0.0239

Step 1070, Epoch 14/50, Batch 30/80
train_loss: 0.0275

Step 1075, Epoch 14/50, Batch 35/80
train_loss: 0.0234

Step 1080, Epoch 14/50, Batch 40/80
train_loss: 0.0242

Step 1085, Epoch 14/50, Batch 45/80
train_loss: 0.0251

Step 1090, Epoch 14/50, Batch 50/80
train_loss: 0.0257

Step 1095, Epoch 14/50, Batch 55/80
train_loss: 0.0283

Step 1100, Epoch 14/50, Batch 60/80
train_loss: 0.0249

Step 1105, Epoch 14/50, Batch 65/80
train_loss: 0.0241

Step 1110, Epoch 14/50, Batch 70/80
train_loss: 0.0273

Step 1115, Epoch 14/50, Batch 75/80
train_loss: 0.0248

Epoch 14/50 completed, Global Step: 1119
train_loss: 0.0248, val_loss: 0.0249

---------------------------------------------------------------------------


Step 1120, Epoch 15/50, Batch 0/80
train_loss: 0.0257

Step 1125, Epoch 15/50, Batch 5/80
train_loss: 0.0236

Step 1130, Epoch 15/50, Batch 10/80
train_loss: 0.0257

Step 1135, Epoch 15/50, Batch 15/80
train_loss: 0.0276

Step 1140, Epoch 15/50, Batch 20/80
train_loss: 0.0273

Step 1145, Epoch 15/50, Batch 25/80
train_loss: 0.0233

Step 1150, Epoch 15/50, Batch 30/80
train_loss: 0.0252

Step 1155, Epoch 15/50, Batch 35/80
train_loss: 0.0273

Step 1160, Epoch 15/50, Batch 40/80
train_loss: 0.0240

Step 1165, Epoch 15/50, Batch 45/80
train_loss: 0.0220

Step 1170, Epoch 15/50, Batch 50/80
train_loss: 0.0216

Step 1175, Epoch 15/50, Batch 55/80
train_loss: 0.0244

Step 1180, Epoch 15/50, Batch 60/80
train_loss: 0.0246

Step 1185, Epoch 15/50, Batch 65/80
train_loss: 0.0237

Step 1190, Epoch 15/50, Batch 70/80
train_loss: 0.0240

Step 1195, Epoch 15/50, Batch 75/80
train_loss: 0.0250

Epoch 15/50 completed, Global Step: 1199
train_loss: 0.0250, val_loss: 0.0228

---------------------------------------------------------------------------


Step 1200, Epoch 16/50, Batch 0/80
train_loss: 0.0241

Step 1205, Epoch 16/50, Batch 5/80
train_loss: 0.0230

Step 1210, Epoch 16/50, Batch 10/80
train_loss: 0.0229

Step 1215, Epoch 16/50, Batch 15/80
train_loss: 0.0239

Step 1220, Epoch 16/50, Batch 20/80
train_loss: 0.0231

Step 1225, Epoch 16/50, Batch 25/80
train_loss: 0.0264

Step 1230, Epoch 16/50, Batch 30/80
train_loss: 0.0214

Step 1235, Epoch 16/50, Batch 35/80
train_loss: 0.0228

Step 1240, Epoch 16/50, Batch 40/80
train_loss: 0.0219

Step 1245, Epoch 16/50, Batch 45/80
train_loss: 0.0248

Step 1250, Epoch 16/50, Batch 50/80
train_loss: 0.0314

Step 1255, Epoch 16/50, Batch 55/80
train_loss: 0.0326

Step 1260, Epoch 16/50, Batch 60/80
train_loss: 0.0264

Step 1265, Epoch 16/50, Batch 65/80
train_loss: 0.0247

Step 1270, Epoch 16/50, Batch 70/80
train_loss: 0.0245

Step 1275, Epoch 16/50, Batch 75/80
train_loss: 0.0236

Epoch 16/50 completed, Global Step: 1279
train_loss: 0.0236, val_loss: 0.0240

---------------------------------------------------------------------------


Step 1280, Epoch 17/50, Batch 0/80
train_loss: 0.0242

Step 1285, Epoch 17/50, Batch 5/80
train_loss: 0.0236

Step 1290, Epoch 17/50, Batch 10/80
train_loss: 0.0247

Step 1295, Epoch 17/50, Batch 15/80
train_loss: 0.0247

Step 1300, Epoch 17/50, Batch 20/80
train_loss: 0.0258

Step 1305, Epoch 17/50, Batch 25/80
train_loss: 0.0234

Step 1310, Epoch 17/50, Batch 30/80
train_loss: 0.0248

Step 1315, Epoch 17/50, Batch 35/80
train_loss: 0.0242

Step 1320, Epoch 17/50, Batch 40/80
train_loss: 0.0242

Step 1325, Epoch 17/50, Batch 45/80
train_loss: 0.0228

Step 1330, Epoch 17/50, Batch 50/80
train_loss: 0.0233

Step 1335, Epoch 17/50, Batch 55/80
train_loss: 0.0220

Step 1340, Epoch 17/50, Batch 60/80
train_loss: 0.0223

Step 1345, Epoch 17/50, Batch 65/80
train_loss: 0.0211

Step 1350, Epoch 17/50, Batch 70/80
train_loss: 0.0237

Step 1355, Epoch 17/50, Batch 75/80
train_loss: 0.0238

Epoch 17/50 completed, Global Step: 1359
train_loss: 0.0238, val_loss: 0.0239

---------------------------------------------------------------------------


Step 1360, Epoch 18/50, Batch 0/80
train_loss: 0.0242

Step 1365, Epoch 18/50, Batch 5/80
train_loss: 0.0210

Step 1370, Epoch 18/50, Batch 10/80
train_loss: 0.0213

Step 1375, Epoch 18/50, Batch 15/80
train_loss: 0.0227

Step 1380, Epoch 18/50, Batch 20/80
train_loss: 0.0227

Step 1385, Epoch 18/50, Batch 25/80
train_loss: 0.0218

Step 1390, Epoch 18/50, Batch 30/80
train_loss: 0.0223

Step 1395, Epoch 18/50, Batch 35/80
train_loss: 0.0216

Step 1400, Epoch 18/50, Batch 40/80
train_loss: 0.0222

Step 1405, Epoch 18/50, Batch 45/80
train_loss: 0.0216

Step 1410, Epoch 18/50, Batch 50/80
train_loss: 0.0217

Step 1415, Epoch 18/50, Batch 55/80
train_loss: 0.0228

Step 1420, Epoch 18/50, Batch 60/80
train_loss: 0.0212

Step 1425, Epoch 18/50, Batch 65/80
train_loss: 0.0220

Step 1430, Epoch 18/50, Batch 70/80
train_loss: 0.0224

Step 1435, Epoch 18/50, Batch 75/80
train_loss: 0.0210

Epoch 18/50 completed, Global Step: 1439
train_loss: 0.0210, val_loss: 0.0208

---------------------------------------------------------------------------


Step 1440, Epoch 19/50, Batch 0/80
train_loss: 0.0207

Step 1445, Epoch 19/50, Batch 5/80
train_loss: 0.0218

Step 1450, Epoch 19/50, Batch 10/80
train_loss: 0.0243

Step 1455, Epoch 19/50, Batch 15/80
train_loss: 0.0227

Step 1460, Epoch 19/50, Batch 20/80
train_loss: 0.0208

Step 1465, Epoch 19/50, Batch 25/80
train_loss: 0.0222

Step 1470, Epoch 19/50, Batch 30/80
train_loss: 0.0222

Step 1475, Epoch 19/50, Batch 35/80
train_loss: 0.0220

Step 1480, Epoch 19/50, Batch 40/80
train_loss: 0.0214

Step 1485, Epoch 19/50, Batch 45/80
train_loss: 0.0212

Step 1490, Epoch 19/50, Batch 50/80
train_loss: 0.0204

Step 1495, Epoch 19/50, Batch 55/80
train_loss: 0.0214

Step 1500, Epoch 19/50, Batch 60/80
train_loss: 0.0211

Step 1505, Epoch 19/50, Batch 65/80
train_loss: 0.0231

Step 1510, Epoch 19/50, Batch 70/80
train_loss: 0.0226

Step 1515, Epoch 19/50, Batch 75/80
train_loss: 0.0222

Epoch 19/50 completed, Global Step: 1519
train_loss: 0.0222, val_loss: 0.0218

---------------------------------------------------------------------------


Step 1520, Epoch 20/50, Batch 0/80
train_loss: 0.0227

Step 1525, Epoch 20/50, Batch 5/80
train_loss: 0.0199

Step 1530, Epoch 20/50, Batch 10/80
train_loss: 0.0201

Step 1535, Epoch 20/50, Batch 15/80
train_loss: 0.0228

Step 1540, Epoch 20/50, Batch 20/80
train_loss: 0.0207

Step 1545, Epoch 20/50, Batch 25/80
train_loss: 0.0228

Step 1550, Epoch 20/50, Batch 30/80
train_loss: 0.0215

Step 1555, Epoch 20/50, Batch 35/80
train_loss: 0.0217

Step 1560, Epoch 20/50, Batch 40/80
train_loss: 0.0205

Step 1565, Epoch 20/50, Batch 45/80
train_loss: 0.0211

Step 1570, Epoch 20/50, Batch 50/80
train_loss: 0.0209

Step 1575, Epoch 20/50, Batch 55/80
train_loss: 0.0210

Step 1580, Epoch 20/50, Batch 60/80
train_loss: 0.0195

Step 1585, Epoch 20/50, Batch 65/80
train_loss: 0.0210

Step 1590, Epoch 20/50, Batch 70/80
train_loss: 0.0217

Step 1595, Epoch 20/50, Batch 75/80
train_loss: 0.0205

Epoch 20/50 completed, Global Step: 1599
train_loss: 0.0205, val_loss: 0.0233

---------------------------------------------------------------------------


Step 1600, Epoch 21/50, Batch 0/80
train_loss: 0.0225

Step 1605, Epoch 21/50, Batch 5/80
train_loss: 0.0198

Step 1610, Epoch 21/50, Batch 10/80
train_loss: 0.0225

Step 1615, Epoch 21/50, Batch 15/80
train_loss: 0.0307

Step 1620, Epoch 21/50, Batch 20/80
train_loss: 0.0227

Step 1625, Epoch 21/50, Batch 25/80
train_loss: 0.0255

Step 1630, Epoch 21/50, Batch 30/80
train_loss: 0.0229

Step 1635, Epoch 21/50, Batch 35/80
train_loss: 0.0249

Step 1640, Epoch 21/50, Batch 40/80
train_loss: 0.0228

Step 1645, Epoch 21/50, Batch 45/80
train_loss: 0.0264

Step 1650, Epoch 21/50, Batch 50/80
train_loss: 0.0234

Step 1655, Epoch 21/50, Batch 55/80
train_loss: 0.0238

Step 1660, Epoch 21/50, Batch 60/80
train_loss: 0.0205

Step 1665, Epoch 21/50, Batch 65/80
train_loss: 0.0198

Step 1670, Epoch 21/50, Batch 70/80
train_loss: 0.0200

Step 1675, Epoch 21/50, Batch 75/80
train_loss: 0.0221

Epoch 21/50 completed, Global Step: 1679
train_loss: 0.0221, val_loss: 0.0265

---------------------------------------------------------------------------


Step 1680, Epoch 22/50, Batch 0/80
train_loss: 0.0262

Step 1685, Epoch 22/50, Batch 5/80
train_loss: 0.0226

Step 1690, Epoch 22/50, Batch 10/80
train_loss: 0.0204

Step 1695, Epoch 22/50, Batch 15/80
train_loss: 0.0192

Step 1700, Epoch 22/50, Batch 20/80
train_loss: 0.0204

Step 1705, Epoch 22/50, Batch 25/80
train_loss: 0.0195

Step 1710, Epoch 22/50, Batch 30/80
train_loss: 0.0205

Step 1715, Epoch 22/50, Batch 35/80
train_loss: 0.0187

Step 1720, Epoch 22/50, Batch 40/80
train_loss: 0.0188

Step 1725, Epoch 22/50, Batch 45/80
train_loss: 0.0207

Step 1730, Epoch 22/50, Batch 50/80
train_loss: 0.0216

Step 1735, Epoch 22/50, Batch 55/80
train_loss: 0.0202

Step 1740, Epoch 22/50, Batch 60/80
train_loss: 0.0197

Step 1745, Epoch 22/50, Batch 65/80
train_loss: 0.0191

Step 1750, Epoch 22/50, Batch 70/80
train_loss: 0.0206

Step 1755, Epoch 22/50, Batch 75/80
train_loss: 0.0201

Epoch 22/50 completed, Global Step: 1759
train_loss: 0.0201, val_loss: 0.0191

---------------------------------------------------------------------------


Step 1760, Epoch 23/50, Batch 0/80
train_loss: 0.0191

Step 1765, Epoch 23/50, Batch 5/80
train_loss: 0.0204

Step 1770, Epoch 23/50, Batch 10/80
train_loss: 0.0202

Step 1775, Epoch 23/50, Batch 15/80
train_loss: 0.0208

Step 1780, Epoch 23/50, Batch 20/80
train_loss: 0.0187

Step 1785, Epoch 23/50, Batch 25/80
train_loss: 0.0187

Step 1790, Epoch 23/50, Batch 30/80
train_loss: 0.0176

Step 1795, Epoch 23/50, Batch 35/80
train_loss: 0.0194

Step 1800, Epoch 23/50, Batch 40/80
train_loss: 0.0178

Step 1805, Epoch 23/50, Batch 45/80
train_loss: 0.0190

Step 1810, Epoch 23/50, Batch 50/80
train_loss: 0.0203

Step 1815, Epoch 23/50, Batch 55/80
train_loss: 0.0184

Step 1820, Epoch 23/50, Batch 60/80
train_loss: 0.0164

Step 1825, Epoch 23/50, Batch 65/80
train_loss: 0.0172

Step 1830, Epoch 23/50, Batch 70/80
train_loss: 0.0192

Step 1835, Epoch 23/50, Batch 75/80
train_loss: 0.0233

Epoch 23/50 completed, Global Step: 1839
train_loss: 0.0233, val_loss: 0.0177

---------------------------------------------------------------------------


Step 1840, Epoch 24/50, Batch 0/80
train_loss: 0.0186

Step 1845, Epoch 24/50, Batch 5/80
train_loss: 0.0185

Step 1850, Epoch 24/50, Batch 10/80
train_loss: 0.0262

Step 1855, Epoch 24/50, Batch 15/80
train_loss: 0.0289

Step 1860, Epoch 24/50, Batch 20/80
train_loss: 0.0284

Step 1865, Epoch 24/50, Batch 25/80
train_loss: 0.0255

Step 1870, Epoch 24/50, Batch 30/80
train_loss: 0.0206

Step 1875, Epoch 24/50, Batch 35/80
train_loss: 0.0195

Step 1880, Epoch 24/50, Batch 40/80
train_loss: 0.0170

Step 1885, Epoch 24/50, Batch 45/80
train_loss: 0.0179

Step 1890, Epoch 24/50, Batch 50/80
train_loss: 0.0178

Step 1895, Epoch 24/50, Batch 55/80
train_loss: 0.0180

Step 1900, Epoch 24/50, Batch 60/80
train_loss: 0.0195

Step 1905, Epoch 24/50, Batch 65/80
train_loss: 0.0166

Step 1910, Epoch 24/50, Batch 70/80
train_loss: 0.0190

Step 1915, Epoch 24/50, Batch 75/80
train_loss: 0.0201

Epoch 24/50 completed, Global Step: 1919
train_loss: 0.0201, val_loss: 0.0162

---------------------------------------------------------------------------


Step 1920, Epoch 25/50, Batch 0/80
train_loss: 0.0164

Step 1925, Epoch 25/50, Batch 5/80
train_loss: 0.0176

Step 1930, Epoch 25/50, Batch 10/80
train_loss: 0.0170

Step 1935, Epoch 25/50, Batch 15/80
train_loss: 0.0158

Step 1940, Epoch 25/50, Batch 20/80
train_loss: 0.0187

Step 1945, Epoch 25/50, Batch 25/80
train_loss: 0.0207

Step 1950, Epoch 25/50, Batch 30/80
train_loss: 0.0216

Step 1955, Epoch 25/50, Batch 35/80
train_loss: 0.0192

Step 1960, Epoch 25/50, Batch 40/80
train_loss: 0.0202

Step 1965, Epoch 25/50, Batch 45/80
train_loss: 0.0184

Step 1970, Epoch 25/50, Batch 50/80
train_loss: 0.0182

Step 1975, Epoch 25/50, Batch 55/80
train_loss: 0.0184

Step 1980, Epoch 25/50, Batch 60/80
train_loss: 0.0163

Step 1985, Epoch 25/50, Batch 65/80
train_loss: 0.0237

Step 1990, Epoch 25/50, Batch 70/80
train_loss: 0.0202

Step 1995, Epoch 25/50, Batch 75/80
train_loss: 0.0196

Epoch 25/50 completed, Global Step: 1999
train_loss: 0.0196, val_loss: 0.0195

---------------------------------------------------------------------------


Step 2000, Epoch 26/50, Batch 0/80
train_loss: 0.0200

Step 2005, Epoch 26/50, Batch 5/80
train_loss: 0.0171

Step 2010, Epoch 26/50, Batch 10/80
train_loss: 0.0155

Step 2015, Epoch 26/50, Batch 15/80
train_loss: 0.0170

Step 2020, Epoch 26/50, Batch 20/80
train_loss: 0.0157

Step 2025, Epoch 26/50, Batch 25/80
train_loss: 0.0151

Step 2030, Epoch 26/50, Batch 30/80
train_loss: 0.0164

Step 2035, Epoch 26/50, Batch 35/80
train_loss: 0.0156

Step 2040, Epoch 26/50, Batch 40/80
train_loss: 0.0156

Step 2045, Epoch 26/50, Batch 45/80
train_loss: 0.0169

Step 2050, Epoch 26/50, Batch 50/80
train_loss: 0.0137

Step 2055, Epoch 26/50, Batch 55/80
train_loss: 0.0152

Step 2060, Epoch 26/50, Batch 60/80
train_loss: 0.0167

Step 2065, Epoch 26/50, Batch 65/80
train_loss: 0.0188

Step 2070, Epoch 26/50, Batch 70/80
train_loss: 0.0142

Step 2075, Epoch 26/50, Batch 75/80
train_loss: 0.0136

Epoch 26/50 completed, Global Step: 2079
train_loss: 0.0136, val_loss: 0.0149

---------------------------------------------------------------------------


Step 2080, Epoch 27/50, Batch 0/80
train_loss: 0.0143

Step 2085, Epoch 27/50, Batch 5/80
train_loss: 0.0139

Step 2090, Epoch 27/50, Batch 10/80
train_loss: 0.0162

Step 2095, Epoch 27/50, Batch 15/80
train_loss: 0.0162

Step 2100, Epoch 27/50, Batch 20/80
train_loss: 0.0123

Step 2105, Epoch 27/50, Batch 25/80
train_loss: 0.0138

Step 2110, Epoch 27/50, Batch 30/80
train_loss: 0.0166

Step 2115, Epoch 27/50, Batch 35/80
train_loss: 0.0179

Step 2120, Epoch 27/50, Batch 40/80
train_loss: 0.0173

Step 2125, Epoch 27/50, Batch 45/80
train_loss: 0.0163

Step 2130, Epoch 27/50, Batch 50/80
train_loss: 0.0170

Step 2135, Epoch 27/50, Batch 55/80
train_loss: 0.0181

Step 2140, Epoch 27/50, Batch 60/80
train_loss: 0.0169

Step 2145, Epoch 27/50, Batch 65/80
train_loss: 0.0142

Step 2150, Epoch 27/50, Batch 70/80
train_loss: 0.0159

Step 2155, Epoch 27/50, Batch 75/80
train_loss: 0.0133

Epoch 27/50 completed, Global Step: 2159
train_loss: 0.0133, val_loss: 0.0141

---------------------------------------------------------------------------


Step 2160, Epoch 28/50, Batch 0/80
train_loss: 0.0148

Step 2165, Epoch 28/50, Batch 5/80
train_loss: 0.0165

Step 2170, Epoch 28/50, Batch 10/80
train_loss: 0.0153

Step 2175, Epoch 28/50, Batch 15/80
train_loss: 0.0188

Step 2180, Epoch 28/50, Batch 20/80
train_loss: 0.0201

Step 2185, Epoch 28/50, Batch 25/80
train_loss: 0.0144

Step 2190, Epoch 28/50, Batch 30/80
train_loss: 0.0159

Step 2195, Epoch 28/50, Batch 35/80
train_loss: 0.0157

Step 2200, Epoch 28/50, Batch 40/80
train_loss: 0.0179

Step 2205, Epoch 28/50, Batch 45/80
train_loss: 0.0150

Step 2210, Epoch 28/50, Batch 50/80
train_loss: 0.0159

Step 2215, Epoch 28/50, Batch 55/80
train_loss: 0.0124

Step 2220, Epoch 28/50, Batch 60/80
train_loss: 0.0133

Step 2225, Epoch 28/50, Batch 65/80
train_loss: 0.0143

Step 2230, Epoch 28/50, Batch 70/80
train_loss: 0.0118

Step 2235, Epoch 28/50, Batch 75/80
train_loss: 0.0127

Epoch 28/50 completed, Global Step: 2239
train_loss: 0.0127, val_loss: 0.0116

---------------------------------------------------------------------------


Step 2240, Epoch 29/50, Batch 0/80
train_loss: 0.0116

Step 2245, Epoch 29/50, Batch 5/80
train_loss: 0.0118

Step 2250, Epoch 29/50, Batch 10/80
train_loss: 0.0122

Step 2255, Epoch 29/50, Batch 15/80
train_loss: 0.0127

Step 2260, Epoch 29/50, Batch 20/80
train_loss: 0.0122

Step 2265, Epoch 29/50, Batch 25/80
train_loss: 0.0119

Step 2270, Epoch 29/50, Batch 30/80
train_loss: 0.0110

Step 2275, Epoch 29/50, Batch 35/80
train_loss: 0.0122

Step 2280, Epoch 29/50, Batch 40/80
train_loss: 0.0132

Step 2285, Epoch 29/50, Batch 45/80
train_loss: 0.0179

Step 2290, Epoch 29/50, Batch 50/80
train_loss: 0.0180

Step 2295, Epoch 29/50, Batch 55/80
train_loss: 0.0121

Step 2300, Epoch 29/50, Batch 60/80
train_loss: 0.0118

Step 2305, Epoch 29/50, Batch 65/80
train_loss: 0.0120

Step 2310, Epoch 29/50, Batch 70/80
train_loss: 0.0119

Step 2315, Epoch 29/50, Batch 75/80
train_loss: 0.0113

Epoch 29/50 completed, Global Step: 2319
train_loss: 0.0113, val_loss: 0.0124

---------------------------------------------------------------------------


Step 2320, Epoch 30/50, Batch 0/80
train_loss: 0.0123

Step 2325, Epoch 30/50, Batch 5/80
train_loss: 0.0119

Step 2330, Epoch 30/50, Batch 10/80
train_loss: 0.0126

Step 2335, Epoch 30/50, Batch 15/80
train_loss: 0.0114

Step 2340, Epoch 30/50, Batch 20/80
train_loss: 0.0113

Step 2345, Epoch 30/50, Batch 25/80
train_loss: 0.0118

Step 2350, Epoch 30/50, Batch 30/80
train_loss: 0.0110

Step 2355, Epoch 30/50, Batch 35/80
train_loss: 0.0130

Step 2360, Epoch 30/50, Batch 40/80
train_loss: 0.0129

Step 2365, Epoch 30/50, Batch 45/80
train_loss: 0.0106

Step 2370, Epoch 30/50, Batch 50/80
train_loss: 0.0155

Step 2375, Epoch 30/50, Batch 55/80
train_loss: 0.0117

Step 2380, Epoch 30/50, Batch 60/80
train_loss: 0.0158

Step 2385, Epoch 30/50, Batch 65/80
train_loss: 0.0128

Step 2390, Epoch 30/50, Batch 70/80
train_loss: 0.0122

Step 2395, Epoch 30/50, Batch 75/80
train_loss: 0.0100

Epoch 30/50 completed, Global Step: 2399
train_loss: 0.0100, val_loss: 0.0118

---------------------------------------------------------------------------


Step 2400, Epoch 31/50, Batch 0/80
train_loss: 0.0120

Step 2405, Epoch 31/50, Batch 5/80
train_loss: 0.0114

Step 2410, Epoch 31/50, Batch 10/80
train_loss: 0.0108

Step 2415, Epoch 31/50, Batch 15/80
train_loss: 0.0100

Step 2420, Epoch 31/50, Batch 20/80
train_loss: 0.0105

Step 2425, Epoch 31/50, Batch 25/80
train_loss: 0.0116

Step 2430, Epoch 31/50, Batch 30/80
train_loss: 0.0107

Step 2435, Epoch 31/50, Batch 35/80
train_loss: 0.0104

Step 2440, Epoch 31/50, Batch 40/80
train_loss: 0.0101

Step 2445, Epoch 31/50, Batch 45/80
train_loss: 0.0136

Step 2450, Epoch 31/50, Batch 50/80
train_loss: 0.0115

Step 2455, Epoch 31/50, Batch 55/80
train_loss: 0.0109

Step 2460, Epoch 31/50, Batch 60/80
train_loss: 0.0115

Step 2465, Epoch 31/50, Batch 65/80
train_loss: 0.0114

Step 2470, Epoch 31/50, Batch 70/80
train_loss: 0.0123

Step 2475, Epoch 31/50, Batch 75/80
train_loss: 0.0112

Epoch 31/50 completed, Global Step: 2479
train_loss: 0.0112, val_loss: 0.0105

---------------------------------------------------------------------------


Step 2480, Epoch 32/50, Batch 0/80
train_loss: 0.0105

Step 2485, Epoch 32/50, Batch 5/80
train_loss: 0.0123

Step 2490, Epoch 32/50, Batch 10/80
train_loss: 0.0100

Step 2495, Epoch 32/50, Batch 15/80
train_loss: 0.0130

Step 2500, Epoch 32/50, Batch 20/80
train_loss: 0.0146

Step 2505, Epoch 32/50, Batch 25/80
train_loss: 0.0157

Step 2510, Epoch 32/50, Batch 30/80
train_loss: 0.0123

Step 2515, Epoch 32/50, Batch 35/80
train_loss: 0.0133

Step 2520, Epoch 32/50, Batch 40/80
train_loss: 0.0142

Step 2525, Epoch 32/50, Batch 45/80
train_loss: 0.0122

Step 2530, Epoch 32/50, Batch 50/80
train_loss: 0.0152

Step 2535, Epoch 32/50, Batch 55/80
train_loss: 0.0130

Step 2540, Epoch 32/50, Batch 60/80
train_loss: 0.0147

Step 2545, Epoch 32/50, Batch 65/80
train_loss: 0.0123

Step 2550, Epoch 32/50, Batch 70/80
train_loss: 0.0145

Step 2555, Epoch 32/50, Batch 75/80
train_loss: 0.0126

Epoch 32/50 completed, Global Step: 2559
train_loss: 0.0126, val_loss: 0.0132

---------------------------------------------------------------------------


Step 2560, Epoch 33/50, Batch 0/80
train_loss: 0.0131

Step 2565, Epoch 33/50, Batch 5/80
train_loss: 0.0142

Step 2570, Epoch 33/50, Batch 10/80
train_loss: 0.0189

Step 2575, Epoch 33/50, Batch 15/80
train_loss: 0.0163

Step 2580, Epoch 33/50, Batch 20/80
train_loss: 0.0139

Step 2585, Epoch 33/50, Batch 25/80
train_loss: 0.0115

Step 2590, Epoch 33/50, Batch 30/80
train_loss: 0.0117

Step 2595, Epoch 33/50, Batch 35/80
train_loss: 0.0128

Step 2600, Epoch 33/50, Batch 40/80
train_loss: 0.0111

Step 2605, Epoch 33/50, Batch 45/80
train_loss: 0.0113

Step 2610, Epoch 33/50, Batch 50/80
train_loss: 0.0096

Step 2615, Epoch 33/50, Batch 55/80
train_loss: 0.0117

Step 2620, Epoch 33/50, Batch 60/80
train_loss: 0.0092

Step 2625, Epoch 33/50, Batch 65/80
train_loss: 0.0106

Step 2630, Epoch 33/50, Batch 70/80
train_loss: 0.0101

Step 2635, Epoch 33/50, Batch 75/80
train_loss: 0.0130

Epoch 33/50 completed, Global Step: 2639
train_loss: 0.0130, val_loss: 0.0131

---------------------------------------------------------------------------


Step 2640, Epoch 34/50, Batch 0/80
train_loss: 0.0127

Step 2645, Epoch 34/50, Batch 5/80
train_loss: 0.0124

Step 2650, Epoch 34/50, Batch 10/80
train_loss: 0.0096

Step 2655, Epoch 34/50, Batch 15/80
train_loss: 0.0109

Step 2660, Epoch 34/50, Batch 20/80
train_loss: 0.0130

Step 2665, Epoch 34/50, Batch 25/80
train_loss: 0.0166

Step 2670, Epoch 34/50, Batch 30/80
train_loss: 0.0169

Step 2675, Epoch 34/50, Batch 35/80
train_loss: 0.0137

Step 2680, Epoch 34/50, Batch 40/80
train_loss: 0.0133

Step 2685, Epoch 34/50, Batch 45/80
train_loss: 0.0111

Step 2690, Epoch 34/50, Batch 50/80
train_loss: 0.0106

Step 2695, Epoch 34/50, Batch 55/80
train_loss: 0.0112

Step 2700, Epoch 34/50, Batch 60/80
train_loss: 0.0087

Step 2705, Epoch 34/50, Batch 65/80
train_loss: 0.0110

Step 2710, Epoch 34/50, Batch 70/80
train_loss: 0.0106

Step 2715, Epoch 34/50, Batch 75/80
train_loss: 0.0102

Epoch 34/50 completed, Global Step: 2719
train_loss: 0.0102, val_loss: 0.0095

---------------------------------------------------------------------------


Step 2720, Epoch 35/50, Batch 0/80
train_loss: 0.0096

Step 2725, Epoch 35/50, Batch 5/80
train_loss: 0.0098

Step 2730, Epoch 35/50, Batch 10/80
train_loss: 0.0099

Step 2735, Epoch 35/50, Batch 15/80
train_loss: 0.0107

Step 2740, Epoch 35/50, Batch 20/80
train_loss: 0.0109

Step 2745, Epoch 35/50, Batch 25/80
train_loss: 0.0089

Step 2750, Epoch 35/50, Batch 30/80
train_loss: 0.0095

Step 2755, Epoch 35/50, Batch 35/80
train_loss: 0.0095

Step 2760, Epoch 35/50, Batch 40/80
train_loss: 0.0101

Step 2765, Epoch 35/50, Batch 45/80
train_loss: 0.0132

Step 2770, Epoch 35/50, Batch 50/80
train_loss: 0.0097

Step 2775, Epoch 35/50, Batch 55/80
train_loss: 0.0095

Step 2780, Epoch 35/50, Batch 60/80
train_loss: 0.0094

Step 2785, Epoch 35/50, Batch 65/80
train_loss: 0.0088

Step 2790, Epoch 35/50, Batch 70/80
train_loss: 0.0101

Step 2795, Epoch 35/50, Batch 75/80
train_loss: 0.0095

Epoch 35/50 completed, Global Step: 2799
train_loss: 0.0095, val_loss: 0.0117

---------------------------------------------------------------------------


Step 2800, Epoch 36/50, Batch 0/80
train_loss: 0.0117

Step 2805, Epoch 36/50, Batch 5/80
train_loss: 0.0091

Step 2810, Epoch 36/50, Batch 10/80
train_loss: 0.0087

Step 2815, Epoch 36/50, Batch 15/80
train_loss: 0.0085

Step 2820, Epoch 36/50, Batch 20/80
train_loss: 0.0113

Step 2825, Epoch 36/50, Batch 25/80
train_loss: 0.0130

Step 2830, Epoch 36/50, Batch 30/80
train_loss: 0.0129

Step 2835, Epoch 36/50, Batch 35/80
train_loss: 0.0121

Step 2840, Epoch 36/50, Batch 40/80
train_loss: 0.0115

Step 2845, Epoch 36/50, Batch 45/80
train_loss: 0.0124

Step 2850, Epoch 36/50, Batch 50/80
train_loss: 0.0130

Step 2855, Epoch 36/50, Batch 55/80
train_loss: 0.0124

Step 2860, Epoch 36/50, Batch 60/80
train_loss: 0.0133

Step 2865, Epoch 36/50, Batch 65/80
train_loss: 0.0096

Step 2870, Epoch 36/50, Batch 70/80
train_loss: 0.0096

Step 2875, Epoch 36/50, Batch 75/80
train_loss: 0.0102

Epoch 36/50 completed, Global Step: 2879
train_loss: 0.0102, val_loss: 0.0098

---------------------------------------------------------------------------


Step 2880, Epoch 37/50, Batch 0/80
train_loss: 0.0103

Step 2885, Epoch 37/50, Batch 5/80
train_loss: 0.0100

Step 2890, Epoch 37/50, Batch 10/80
train_loss: 0.0128

Step 2895, Epoch 37/50, Batch 15/80
train_loss: 0.0143

Step 2900, Epoch 37/50, Batch 20/80
train_loss: 0.0164

Step 2905, Epoch 37/50, Batch 25/80
train_loss: 0.0149

Step 2910, Epoch 37/50, Batch 30/80
train_loss: 0.0133

Step 2915, Epoch 37/50, Batch 35/80
train_loss: 0.0120

Step 2920, Epoch 37/50, Batch 40/80
train_loss: 0.0140

Step 2925, Epoch 37/50, Batch 45/80
train_loss: 0.0109

Step 2930, Epoch 37/50, Batch 50/80
train_loss: 0.0140

Step 2935, Epoch 37/50, Batch 55/80
train_loss: 0.0153

Step 2940, Epoch 37/50, Batch 60/80
train_loss: 0.0116

Step 2945, Epoch 37/50, Batch 65/80
train_loss: 0.0093

Step 2950, Epoch 37/50, Batch 70/80
train_loss: 0.0099

Step 2955, Epoch 37/50, Batch 75/80
train_loss: 0.0093

Epoch 37/50 completed, Global Step: 2959
train_loss: 0.0093, val_loss: 0.0120

---------------------------------------------------------------------------


Step 2960, Epoch 38/50, Batch 0/80
train_loss: 0.0119

Step 2965, Epoch 38/50, Batch 5/80
train_loss: 0.0093

Step 2970, Epoch 38/50, Batch 10/80
train_loss: 0.0081

Step 2975, Epoch 38/50, Batch 15/80
train_loss: 0.0104

Step 2980, Epoch 38/50, Batch 20/80
train_loss: 0.0118

Step 2985, Epoch 38/50, Batch 25/80
train_loss: 0.0115

Step 2990, Epoch 38/50, Batch 30/80
train_loss: 0.0117

Step 2995, Epoch 38/50, Batch 35/80
train_loss: 0.0110

Step 3000, Epoch 38/50, Batch 40/80
train_loss: 0.0118

Step 3005, Epoch 38/50, Batch 45/80
train_loss: 0.0103

Step 3010, Epoch 38/50, Batch 50/80
train_loss: 0.0121

Step 3015, Epoch 38/50, Batch 55/80
train_loss: 0.0117

Step 3020, Epoch 38/50, Batch 60/80
train_loss: 0.0133

Step 3025, Epoch 38/50, Batch 65/80
train_loss: 0.0111

Step 3030, Epoch 38/50, Batch 70/80
train_loss: 0.0105

Step 3035, Epoch 38/50, Batch 75/80
train_loss: 0.0125

Epoch 38/50 completed, Global Step: 3039
train_loss: 0.0125, val_loss: 0.0130

---------------------------------------------------------------------------


Step 3040, Epoch 39/50, Batch 0/80
train_loss: 0.0129

Step 3045, Epoch 39/50, Batch 5/80
train_loss: 0.0114

Step 3050, Epoch 39/50, Batch 10/80
train_loss: 0.0123

Step 3055, Epoch 39/50, Batch 15/80
train_loss: 0.0105

Step 3060, Epoch 39/50, Batch 20/80
train_loss: 0.0101

Step 3065, Epoch 39/50, Batch 25/80
train_loss: 0.0136

Step 3070, Epoch 39/50, Batch 30/80
train_loss: 0.0110

Step 3075, Epoch 39/50, Batch 35/80
train_loss: 0.0140

Step 3080, Epoch 39/50, Batch 40/80
train_loss: 0.0148

Step 3085, Epoch 39/50, Batch 45/80
train_loss: 0.0131

Step 3090, Epoch 39/50, Batch 50/80
train_loss: 0.0133

Step 3095, Epoch 39/50, Batch 55/80
train_loss: 0.0117

Step 3100, Epoch 39/50, Batch 60/80
train_loss: 0.0103

Step 3105, Epoch 39/50, Batch 65/80
train_loss: 0.0104

Step 3110, Epoch 39/50, Batch 70/80
train_loss: 0.0092

Step 3115, Epoch 39/50, Batch 75/80
train_loss: 0.0102

Epoch 39/50 completed, Global Step: 3119
train_loss: 0.0102, val_loss: 0.0116

---------------------------------------------------------------------------


Step 3120, Epoch 40/50, Batch 0/80
train_loss: 0.0117

Step 3125, Epoch 40/50, Batch 5/80
train_loss: 0.0100

Step 3130, Epoch 40/50, Batch 10/80
train_loss: 0.0110

Step 3135, Epoch 40/50, Batch 15/80
train_loss: 0.0099

Step 3140, Epoch 40/50, Batch 20/80
train_loss: 0.0098

Step 3145, Epoch 40/50, Batch 25/80
train_loss: 0.0094

Step 3150, Epoch 40/50, Batch 30/80
train_loss: 0.0114

Step 3155, Epoch 40/50, Batch 35/80
train_loss: 0.0095

Step 3160, Epoch 40/50, Batch 40/80
train_loss: 0.0101

Step 3165, Epoch 40/50, Batch 45/80
train_loss: 0.0102

Step 3170, Epoch 40/50, Batch 50/80
train_loss: 0.0101

Step 3175, Epoch 40/50, Batch 55/80
train_loss: 0.0084

Step 3180, Epoch 40/50, Batch 60/80
train_loss: 0.0102

Step 3185, Epoch 40/50, Batch 65/80
train_loss: 0.0094

Step 3190, Epoch 40/50, Batch 70/80
train_loss: 0.0098

Step 3195, Epoch 40/50, Batch 75/80
train_loss: 0.0082

Epoch 40/50 completed, Global Step: 3199
train_loss: 0.0082, val_loss: 0.0133

---------------------------------------------------------------------------


Step 3200, Epoch 41/50, Batch 0/80
train_loss: 0.0132

Step 3205, Epoch 41/50, Batch 5/80
train_loss: 0.0107

Step 3210, Epoch 41/50, Batch 10/80
train_loss: 0.0124

Step 3215, Epoch 41/50, Batch 15/80
train_loss: 0.0098

Step 3220, Epoch 41/50, Batch 20/80
train_loss: 0.0098

Step 3225, Epoch 41/50, Batch 25/80
train_loss: 0.0103

Step 3230, Epoch 41/50, Batch 30/80
train_loss: 0.0100

Step 3235, Epoch 41/50, Batch 35/80
train_loss: 0.0107

Step 3240, Epoch 41/50, Batch 40/80
train_loss: 0.0097

Step 3245, Epoch 41/50, Batch 45/80
train_loss: 0.0096

Step 3250, Epoch 41/50, Batch 50/80
train_loss: 0.0088

Step 3255, Epoch 41/50, Batch 55/80
train_loss: 0.0081

Step 3260, Epoch 41/50, Batch 60/80
train_loss: 0.0074

Step 3265, Epoch 41/50, Batch 65/80
train_loss: 0.0077

Step 3270, Epoch 41/50, Batch 70/80
train_loss: 0.0089

Step 3275, Epoch 41/50, Batch 75/80
train_loss: 0.0080

Epoch 41/50 completed, Global Step: 3279
train_loss: 0.0080, val_loss: 0.0081

---------------------------------------------------------------------------


Step 3280, Epoch 42/50, Batch 0/80
train_loss: 0.0082

Step 3285, Epoch 42/50, Batch 5/80
train_loss: 0.0086

Step 3290, Epoch 42/50, Batch 10/80
train_loss: 0.0077

Step 3295, Epoch 42/50, Batch 15/80
train_loss: 0.0088

Step 3300, Epoch 42/50, Batch 20/80
train_loss: 0.0090

Step 3305, Epoch 42/50, Batch 25/80
train_loss: 0.0089

Step 3310, Epoch 42/50, Batch 30/80
train_loss: 0.0098

Step 3315, Epoch 42/50, Batch 35/80
train_loss: 0.0091

Step 3320, Epoch 42/50, Batch 40/80
train_loss: 0.0080

Step 3325, Epoch 42/50, Batch 45/80
train_loss: 0.0090

Step 3330, Epoch 42/50, Batch 50/80
train_loss: 0.0096

Step 3335, Epoch 42/50, Batch 55/80
train_loss: 0.0127

Step 3340, Epoch 42/50, Batch 60/80
train_loss: 0.0083

Step 3345, Epoch 42/50, Batch 65/80
train_loss: 0.0091

Step 3350, Epoch 42/50, Batch 70/80
train_loss: 0.0089

Step 3355, Epoch 42/50, Batch 75/80
train_loss: 0.0092

Epoch 42/50 completed, Global Step: 3359
train_loss: 0.0092, val_loss: 0.0087

---------------------------------------------------------------------------


Step 3360, Epoch 43/50, Batch 0/80
train_loss: 0.0091

Step 3365, Epoch 43/50, Batch 5/80
train_loss: 0.0099

Step 3370, Epoch 43/50, Batch 10/80
train_loss: 0.0082

Step 3375, Epoch 43/50, Batch 15/80
train_loss: 0.0080

Step 3380, Epoch 43/50, Batch 20/80
train_loss: 0.0108

Step 3385, Epoch 43/50, Batch 25/80
train_loss: 0.0090

Step 3390, Epoch 43/50, Batch 30/80
train_loss: 0.0087

Step 3395, Epoch 43/50, Batch 35/80
train_loss: 0.0090

Step 3400, Epoch 43/50, Batch 40/80
train_loss: 0.0082

Step 3405, Epoch 43/50, Batch 45/80
train_loss: 0.0091

Step 3410, Epoch 43/50, Batch 50/80
train_loss: 0.0072

Step 3415, Epoch 43/50, Batch 55/80
train_loss: 0.0087

Step 3420, Epoch 43/50, Batch 60/80
train_loss: 0.0113

Step 3425, Epoch 43/50, Batch 65/80
train_loss: 0.0085

Step 3430, Epoch 43/50, Batch 70/80
train_loss: 0.0096

Step 3435, Epoch 43/50, Batch 75/80
train_loss: 0.0108

Epoch 43/50 completed, Global Step: 3439
train_loss: 0.0108, val_loss: 0.0116

---------------------------------------------------------------------------


Step 3440, Epoch 44/50, Batch 0/80
train_loss: 0.0119

Step 3445, Epoch 44/50, Batch 5/80
train_loss: 0.0095

Step 3450, Epoch 44/50, Batch 10/80
train_loss: 0.0118

Step 3455, Epoch 44/50, Batch 15/80
train_loss: 0.0091

Step 3460, Epoch 44/50, Batch 20/80
train_loss: 0.0103

Step 3465, Epoch 44/50, Batch 25/80
train_loss: 0.0098

Step 3470, Epoch 44/50, Batch 30/80
train_loss: 0.0110

Step 3475, Epoch 44/50, Batch 35/80
train_loss: 0.0092

Step 3480, Epoch 44/50, Batch 40/80
train_loss: 0.0103

Step 3485, Epoch 44/50, Batch 45/80
train_loss: 0.0103

Step 3490, Epoch 44/50, Batch 50/80
train_loss: 0.0109

Step 3495, Epoch 44/50, Batch 55/80
train_loss: 0.0097

Step 3500, Epoch 44/50, Batch 60/80
train_loss: 0.0104

Step 3505, Epoch 44/50, Batch 65/80
train_loss: 0.0096

Step 3510, Epoch 44/50, Batch 70/80
train_loss: 0.0127

Step 3515, Epoch 44/50, Batch 75/80
train_loss: 0.0086

Epoch 44/50 completed, Global Step: 3519
train_loss: 0.0086, val_loss: 0.0099

---------------------------------------------------------------------------


Step 3520, Epoch 45/50, Batch 0/80
train_loss: 0.0099

Step 3525, Epoch 45/50, Batch 5/80
train_loss: 0.0096

Step 3530, Epoch 45/50, Batch 10/80
train_loss: 0.0087

Step 3535, Epoch 45/50, Batch 15/80
train_loss: 0.0081

Step 3540, Epoch 45/50, Batch 20/80
train_loss: 0.0089

Step 3545, Epoch 45/50, Batch 25/80
train_loss: 0.0118

Step 3550, Epoch 45/50, Batch 30/80
train_loss: 0.0105

Step 3555, Epoch 45/50, Batch 35/80
train_loss: 0.0111

Step 3560, Epoch 45/50, Batch 40/80
train_loss: 0.0100

Step 3565, Epoch 45/50, Batch 45/80
train_loss: 0.0085

Step 3570, Epoch 45/50, Batch 50/80
train_loss: 0.0080

Step 3575, Epoch 45/50, Batch 55/80
train_loss: 0.0079

Step 3580, Epoch 45/50, Batch 60/80
train_loss: 0.0083

Step 3585, Epoch 45/50, Batch 65/80
train_loss: 0.0073

Step 3590, Epoch 45/50, Batch 70/80
train_loss: 0.0088

Step 3595, Epoch 45/50, Batch 75/80
train_loss: 0.0089

Epoch 45/50 completed, Global Step: 3599
train_loss: 0.0089, val_loss: 0.0080

---------------------------------------------------------------------------


Step 3600, Epoch 46/50, Batch 0/80
train_loss: 0.0078

Step 3605, Epoch 46/50, Batch 5/80
train_loss: 0.0072

Step 3610, Epoch 46/50, Batch 10/80
train_loss: 0.0078

Step 3615, Epoch 46/50, Batch 15/80
train_loss: 0.0079

Step 3620, Epoch 46/50, Batch 20/80
train_loss: 0.0089

Step 3625, Epoch 46/50, Batch 25/80
train_loss: 0.0083

Step 3630, Epoch 46/50, Batch 30/80
train_loss: 0.0087

Step 3635, Epoch 46/50, Batch 35/80
train_loss: 0.0091

Step 3640, Epoch 46/50, Batch 40/80
train_loss: 0.0112

Step 3645, Epoch 46/50, Batch 45/80
train_loss: 0.0116

Step 3650, Epoch 46/50, Batch 50/80
train_loss: 0.0085

Step 3655, Epoch 46/50, Batch 55/80
train_loss: 0.0101

Step 3660, Epoch 46/50, Batch 60/80
train_loss: 0.0088

Step 3665, Epoch 46/50, Batch 65/80
train_loss: 0.0104

Step 3670, Epoch 46/50, Batch 70/80
train_loss: 0.0099

Step 3675, Epoch 46/50, Batch 75/80
train_loss: 0.0106

Epoch 46/50 completed, Global Step: 3679
train_loss: 0.0106, val_loss: 0.0092

---------------------------------------------------------------------------


Step 3680, Epoch 47/50, Batch 0/80
train_loss: 0.0094

Step 3685, Epoch 47/50, Batch 5/80
train_loss: 0.0109

Step 3690, Epoch 47/50, Batch 10/80
train_loss: 0.0094

Step 3695, Epoch 47/50, Batch 15/80
train_loss: 0.0093

Step 3700, Epoch 47/50, Batch 20/80
train_loss: 0.0077

Step 3705, Epoch 47/50, Batch 25/80
train_loss: 0.0095

Step 3710, Epoch 47/50, Batch 30/80
train_loss: 0.0087

Step 3715, Epoch 47/50, Batch 35/80
train_loss: 0.0082

Step 3720, Epoch 47/50, Batch 40/80
train_loss: 0.0074

Step 3725, Epoch 47/50, Batch 45/80
train_loss: 0.0093

Step 3730, Epoch 47/50, Batch 50/80
train_loss: 0.0107

Step 3735, Epoch 47/50, Batch 55/80
train_loss: 0.0087

Step 3740, Epoch 47/50, Batch 60/80
train_loss: 0.0122

Step 3745, Epoch 47/50, Batch 65/80
train_loss: 0.0097

Step 3750, Epoch 47/50, Batch 70/80
train_loss: 0.0124

Step 3755, Epoch 47/50, Batch 75/80
train_loss: 0.0122

Epoch 47/50 completed, Global Step: 3759
train_loss: 0.0122, val_loss: 0.0113

---------------------------------------------------------------------------


Step 3760, Epoch 48/50, Batch 0/80
train_loss: 0.0114

Step 3765, Epoch 48/50, Batch 5/80
train_loss: 0.0103

Step 3770, Epoch 48/50, Batch 10/80
train_loss: 0.0115

Step 3775, Epoch 48/50, Batch 15/80
train_loss: 0.0102

Step 3780, Epoch 48/50, Batch 20/80
train_loss: 0.0102

Step 3785, Epoch 48/50, Batch 25/80
train_loss: 0.0104

Step 3790, Epoch 48/50, Batch 30/80
train_loss: 0.0103

Step 3795, Epoch 48/50, Batch 35/80
train_loss: 0.0102

Step 3800, Epoch 48/50, Batch 40/80
train_loss: 0.0090

Step 3805, Epoch 48/50, Batch 45/80
train_loss: 0.0081

Step 3810, Epoch 48/50, Batch 50/80
train_loss: 0.0082

Step 3815, Epoch 48/50, Batch 55/80
train_loss: 0.0081

Step 3820, Epoch 48/50, Batch 60/80
train_loss: 0.0068

Step 3825, Epoch 48/50, Batch 65/80
train_loss: 0.0068

Step 3830, Epoch 48/50, Batch 70/80
train_loss: 0.0071

Step 3835, Epoch 48/50, Batch 75/80
train_loss: 0.0066

Epoch 48/50 completed, Global Step: 3839
train_loss: 0.0066, val_loss: 0.0079

---------------------------------------------------------------------------


Step 3840, Epoch 49/50, Batch 0/80
train_loss: 0.0079

Step 3845, Epoch 49/50, Batch 5/80
train_loss: 0.0068

Step 3850, Epoch 49/50, Batch 10/80
train_loss: 0.0074

Step 3855, Epoch 49/50, Batch 15/80
train_loss: 0.0083

Step 3860, Epoch 49/50, Batch 20/80
train_loss: 0.0083

Step 3865, Epoch 49/50, Batch 25/80
train_loss: 0.0074

Step 3870, Epoch 49/50, Batch 30/80
train_loss: 0.0070

Step 3875, Epoch 49/50, Batch 35/80
train_loss: 0.0080

Step 3880, Epoch 49/50, Batch 40/80
train_loss: 0.0108

Step 3885, Epoch 49/50, Batch 45/80
train_loss: 0.0087

Step 3890, Epoch 49/50, Batch 50/80
train_loss: 0.0108

Step 3895, Epoch 49/50, Batch 55/80
train_loss: 0.0068

Step 3900, Epoch 49/50, Batch 60/80
train_loss: 0.0075

Step 3905, Epoch 49/50, Batch 65/80
train_loss: 0.0075

Step 3910, Epoch 49/50, Batch 70/80
train_loss: 0.0075

Step 3915, Epoch 49/50, Batch 75/80
train_loss: 0.0105

Epoch 49/50 completed, Global Step: 3919
train_loss: 0.0105, val_loss: 0.0086

---------------------------------------------------------------------------


Step 3920, Epoch 50/50, Batch 0/80
train_loss: 0.0092

Step 3925, Epoch 50/50, Batch 5/80
train_loss: 0.0099

Step 3930, Epoch 50/50, Batch 10/80
train_loss: 0.0086

Step 3935, Epoch 50/50, Batch 15/80
train_loss: 0.0095

Step 3940, Epoch 50/50, Batch 20/80
train_loss: 0.0101

Step 3945, Epoch 50/50, Batch 25/80
train_loss: 0.0102

Step 3950, Epoch 50/50, Batch 30/80
train_loss: 0.0085

Step 3955, Epoch 50/50, Batch 35/80
train_loss: 0.0087

Step 3960, Epoch 50/50, Batch 40/80
train_loss: 0.0076

Step 3965, Epoch 50/50, Batch 45/80
train_loss: 0.0093

Step 3970, Epoch 50/50, Batch 50/80
train_loss: 0.0083

Step 3975, Epoch 50/50, Batch 55/80
train_loss: 0.0082

Step 3980, Epoch 50/50, Batch 60/80
train_loss: 0.0073

Step 3985, Epoch 50/50, Batch 65/80
train_loss: 0.0084

Step 3990, Epoch 50/50, Batch 70/80
train_loss: 0.0079

Step 3995, Epoch 50/50, Batch 75/80
train_loss: 0.0082

Epoch 50/50 completed, Global Step: 3999
train_loss: 0.0082, val_loss: 0.0066

---------------------------------------------------------------------------


Training completed in 1308.53 seconds or 21.81 minutes or 0.363479551937845 hours.
Total training steps: 3999

Training completed for model '[m004_(apv+G)]-gru_dec_1.5'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m004_(apv+G)]-gru_dec_1.5...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.3284' for [m004_(apv+G)]-gru_dec_1.5...

Decoder output plot for rep '1001.3284' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.2106' for [m004_(apv+G)]-gru_dec_1.5...

Decoder output plot for rep '1001.2106' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5


---------------------------------------------------------------------------

TESTING TRAINED DECODER MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5\checkpoints:

['best-model-epoch=49-val_loss=0.0066.ckpt']

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5\test

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Trained Decoder Model Loaded for testing.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                     | 0/? [00:00<?, ?it/s]
Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|   | 0/10 [00:00<?, ?it/s]
Found rep_num = 1001.0001 in batch 0 of epoch 49. Decoder output plot will be made for this data.
Testing DataLoader 0:  10%|1| 1/10 [00:00<00:00,  9.6Testing DataLoader 0:  20%|2| 2/10 [00:00<00:00,  9.9Testing DataLoader 0:  30%|3| 3/10 [00:00<00:00, 10.2Testing DataLoader 0:  40%|4| 4/10 [00:00<00:00, 10.0Testing DataLoader 0:  50%|5| 5/10 [00:00<00:00, 10.1Testing DataLoader 0:  60%|6| 6/10 [00:00<00:00, 10.0Testing DataLoader 0:  70%|7| 7/10 [00:00<00:00,  9.7Testing DataLoader 0:  80%|8| 8/10 [00:00<00:00,  9.6Testing DataLoader 0:  90%|9| 9/10 [00:00<00:00,  9.5Testing DataLoader 0: 100%|#| 10/10 [00:01<00:00,  9.
Testing completed in 1.04 seconds or 0.02 minutes or 0.00028852515750461155 hours.

test_loss: 0.0087

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.0001' for [m004_(apv+G)]-gru_dec_1.5...

Decoder output plot for rep '1001.0001' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.5\test

Testing DataLoader 0: 100%|#| 10/10 [00:03<00:00,  3.

       Test metric              DataLoader 0       

        test_loss           0.008704759180545807   


===========================================================================

Decoder model '[m004_(apv+G)]-gru_dec_1.5' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-21 19:46:34
