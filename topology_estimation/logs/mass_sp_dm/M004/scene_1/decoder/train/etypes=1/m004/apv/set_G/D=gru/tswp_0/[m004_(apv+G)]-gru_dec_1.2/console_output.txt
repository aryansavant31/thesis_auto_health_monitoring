=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m004_(apv+G)]-gru_dec_1.2
Start Time: 2025-09-21 16:56:49
End Time: 2025-09-21 17:19:25

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting decoder model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]

Node group name: m004
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.

Target rep_num 1001.0001 found at index 0. This sample will be included in the test set.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 450/500 [OK=450, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 9
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (4, 4)
     n1   n2   n3   n4
n1  0.0  1.0  0.0  0.0
n2  1.0  0.0  1.0  0.0
n3  0.0  1.0  0.0  1.0
n4  0.0  0.0  1.0  0.0


**Receiver relation matrix** => shape: (12, 4)
      n1   n2   n3   n4
e12  0.0  1.0  0.0  0.0
e13  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0
e21  1.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0
e24  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0
e34  0.0  0.0  0.0  1.0
e41  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0
e43  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (12, 4)
      n1   n2   n3   n4
e12  1.0  0.0  0.0  0.0
e13  0.0  0.0  0.0  0.0
e14  0.0  0.0  0.0  0.0
e21  0.0  1.0  0.0  0.0
e23  0.0  1.0  0.0  0.0
e24  0.0  0.0  0.0  0.0
e31  0.0  0.0  0.0  0.0
e32  0.0  0.0  1.0  0.0
e34  0.0  0.0  1.0  0.0
e41  0.0  0.0  0.0  0.0
e42  0.0  0.0  0.0  0.0
e43  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 1
msg_out_size: 128
edge_mlp_config: [[128, 'tanh'], [128, 'tanh']]
out_mlp_config: [[128, 'relu'], [128, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
n_dims: 3

Decoder run parameters:
-------------------------
skip_first_edge_type: False
pred_steps: 10
is_burn_in: True
final_pred_steps: 50
is_dynamic_graph: False
temp: 1.0
is_hard: True
show_conf_band: False
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2

---------------------------------------------------------------------------

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Training parameters set to: 
lr=0.001, 
optimizer=adam, 
loss_type=mse

---------------------------------------------------------------------------

Decoder Model Initialized with the following configurations:

Decoder Model Summary:
Decoder(
  (edge_mlp_fn): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=128, out_features=128, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=128, bias=True)
    (hidden_u): Linear(in_features=128, out_features=128, bias=True)
    (input_r): Linear(in_features=3, out_features=128, bias=True)
    (hidden_r): Linear(in_features=128, out_features=128, bias=True)
    (input_h): Linear(in_features=3, out_features=128, bias=True)
    (hidden_h): Linear(in_features=128, out_features=128, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=128, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=128, out_features=3, bias=True)
)

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Step 0, Epoch 1/50, Batch 0/80
train_loss: 1.0627

Step 5, Epoch 1/50, Batch 5/80
train_loss: 0.1476

Step 10, Epoch 1/50, Batch 10/80
train_loss: 0.0683

Step 15, Epoch 1/50, Batch 15/80
train_loss: 0.0312

Step 20, Epoch 1/50, Batch 20/80
train_loss: 0.0379

Step 25, Epoch 1/50, Batch 25/80
train_loss: 0.0301

Step 30, Epoch 1/50, Batch 30/80
train_loss: 0.0281

Step 35, Epoch 1/50, Batch 35/80
train_loss: 0.0268

Step 40, Epoch 1/50, Batch 40/80
train_loss: 0.0265

Step 45, Epoch 1/50, Batch 45/80
train_loss: 0.0247

Step 50, Epoch 1/50, Batch 50/80
train_loss: 0.0245

Step 55, Epoch 1/50, Batch 55/80
train_loss: 0.0243

Step 60, Epoch 1/50, Batch 60/80
train_loss: 0.0239

Step 65, Epoch 1/50, Batch 65/80
train_loss: 0.0245

Step 70, Epoch 1/50, Batch 70/80
train_loss: 0.0238

Step 75, Epoch 1/50, Batch 75/80
train_loss: 0.0231

Epoch 1/50 completed, Global Step: 79
train_loss: 0.0231, val_loss: 0.0233

---------------------------------------------------------------------------


Step 80, Epoch 2/50, Batch 0/80
train_loss: 0.0229

Step 85, Epoch 2/50, Batch 5/80
train_loss: 0.0235

Step 90, Epoch 2/50, Batch 10/80
train_loss: 0.0234

Step 95, Epoch 2/50, Batch 15/80
train_loss: 0.0234

Step 100, Epoch 2/50, Batch 20/80
train_loss: 0.0231

Step 105, Epoch 2/50, Batch 25/80
train_loss: 0.0228

Step 110, Epoch 2/50, Batch 30/80
train_loss: 0.0227

Step 115, Epoch 2/50, Batch 35/80
train_loss: 0.0230

Step 120, Epoch 2/50, Batch 40/80
train_loss: 0.0227

Step 125, Epoch 2/50, Batch 45/80
train_loss: 0.0231

Step 130, Epoch 2/50, Batch 50/80
train_loss: 0.0231

Step 135, Epoch 2/50, Batch 55/80
train_loss: 0.0228

Step 140, Epoch 2/50, Batch 60/80
train_loss: 0.0227

Step 145, Epoch 2/50, Batch 65/80
train_loss: 0.0228

Step 150, Epoch 2/50, Batch 70/80
train_loss: 0.0231

Step 155, Epoch 2/50, Batch 75/80
train_loss: 0.0229

Epoch 2/50 completed, Global Step: 159
train_loss: 0.0229, val_loss: 0.0226

---------------------------------------------------------------------------


Step 160, Epoch 3/50, Batch 0/80
train_loss: 0.0227

Step 165, Epoch 3/50, Batch 5/80
train_loss: 0.0225

Step 170, Epoch 3/50, Batch 10/80
train_loss: 0.0228

Step 175, Epoch 3/50, Batch 15/80
train_loss: 0.0224

Step 180, Epoch 3/50, Batch 20/80
train_loss: 0.0227

Step 185, Epoch 3/50, Batch 25/80
train_loss: 0.0224

Step 190, Epoch 3/50, Batch 30/80
train_loss: 0.0223

Step 195, Epoch 3/50, Batch 35/80
train_loss: 0.0222

Step 200, Epoch 3/50, Batch 40/80
train_loss: 0.0220

Step 205, Epoch 3/50, Batch 45/80
train_loss: 0.0225

Step 210, Epoch 3/50, Batch 50/80
train_loss: 0.0219

Step 215, Epoch 3/50, Batch 55/80
train_loss: 0.0221

Step 220, Epoch 3/50, Batch 60/80
train_loss: 0.0223

Step 225, Epoch 3/50, Batch 65/80
train_loss: 0.0226

Step 230, Epoch 3/50, Batch 70/80
train_loss: 0.0226

Step 235, Epoch 3/50, Batch 75/80
train_loss: 0.0222

Epoch 3/50 completed, Global Step: 239
train_loss: 0.0222, val_loss: 0.0220

---------------------------------------------------------------------------


Step 240, Epoch 4/50, Batch 0/80
train_loss: 0.0218

Step 245, Epoch 4/50, Batch 5/80
train_loss: 0.0220

Step 250, Epoch 4/50, Batch 10/80
train_loss: 0.0218

Step 255, Epoch 4/50, Batch 15/80
train_loss: 0.0217

Step 260, Epoch 4/50, Batch 20/80
train_loss: 0.0217

Step 265, Epoch 4/50, Batch 25/80
train_loss: 0.0218

Step 270, Epoch 4/50, Batch 30/80
train_loss: 0.0220

Step 275, Epoch 4/50, Batch 35/80
train_loss: 0.0222

Step 280, Epoch 4/50, Batch 40/80
train_loss: 0.0215

Step 285, Epoch 4/50, Batch 45/80
train_loss: 0.0216

Step 290, Epoch 4/50, Batch 50/80
train_loss: 0.0219

Step 295, Epoch 4/50, Batch 55/80
train_loss: 0.0219

Step 300, Epoch 4/50, Batch 60/80
train_loss: 0.0216

Step 305, Epoch 4/50, Batch 65/80
train_loss: 0.0215

Step 310, Epoch 4/50, Batch 70/80
train_loss: 0.0217

Step 315, Epoch 4/50, Batch 75/80
train_loss: 0.0212

Epoch 4/50 completed, Global Step: 319
train_loss: 0.0212, val_loss: 0.0213

---------------------------------------------------------------------------


Step 320, Epoch 5/50, Batch 0/80
train_loss: 0.0212

Step 325, Epoch 5/50, Batch 5/80
train_loss: 0.0213

Step 330, Epoch 5/50, Batch 10/80
train_loss: 0.0215

Step 335, Epoch 5/50, Batch 15/80
train_loss: 0.0213

Step 340, Epoch 5/50, Batch 20/80
train_loss: 0.0226

Step 345, Epoch 5/50, Batch 25/80
train_loss: 0.0211

Step 350, Epoch 5/50, Batch 30/80
train_loss: 0.0213

Step 355, Epoch 5/50, Batch 35/80
train_loss: 0.0215

Step 360, Epoch 5/50, Batch 40/80
train_loss: 0.0207

Step 365, Epoch 5/50, Batch 45/80
train_loss: 0.0211

Step 370, Epoch 5/50, Batch 50/80
train_loss: 0.0211

Step 375, Epoch 5/50, Batch 55/80
train_loss: 0.0205

Step 380, Epoch 5/50, Batch 60/80
train_loss: 0.0209

Step 385, Epoch 5/50, Batch 65/80
train_loss: 0.0205

Step 390, Epoch 5/50, Batch 70/80
train_loss: 0.0205

Step 395, Epoch 5/50, Batch 75/80
train_loss: 0.0206

Epoch 5/50 completed, Global Step: 399
train_loss: 0.0206, val_loss: 0.0204

---------------------------------------------------------------------------


Step 400, Epoch 6/50, Batch 0/80
train_loss: 0.0205

Step 405, Epoch 6/50, Batch 5/80
train_loss: 0.0201

Step 410, Epoch 6/50, Batch 10/80
train_loss: 0.0201

Step 415, Epoch 6/50, Batch 15/80
train_loss: 0.0202

Step 420, Epoch 6/50, Batch 20/80
train_loss: 0.0207

Step 425, Epoch 6/50, Batch 25/80
train_loss: 0.0212

Step 430, Epoch 6/50, Batch 30/80
train_loss: 0.0203

Step 435, Epoch 6/50, Batch 35/80
train_loss: 0.0206

Step 440, Epoch 6/50, Batch 40/80
train_loss: 0.0199

Step 445, Epoch 6/50, Batch 45/80
train_loss: 0.0198

Step 450, Epoch 6/50, Batch 50/80
train_loss: 0.0204

Step 455, Epoch 6/50, Batch 55/80
train_loss: 0.0206

Step 460, Epoch 6/50, Batch 60/80
train_loss: 0.0194

Step 465, Epoch 6/50, Batch 65/80
train_loss: 0.0211

Step 470, Epoch 6/50, Batch 70/80
train_loss: 0.0199

Step 475, Epoch 6/50, Batch 75/80
train_loss: 0.0202

Epoch 6/50 completed, Global Step: 479
train_loss: 0.0202, val_loss: 0.0198

---------------------------------------------------------------------------


Step 480, Epoch 7/50, Batch 0/80
train_loss: 0.0200

Step 485, Epoch 7/50, Batch 5/80
train_loss: 0.0199

Step 490, Epoch 7/50, Batch 10/80
train_loss: 0.0197

Step 495, Epoch 7/50, Batch 15/80
train_loss: 0.0195

Step 500, Epoch 7/50, Batch 20/80
train_loss: 0.0199

Step 505, Epoch 7/50, Batch 25/80
train_loss: 0.0194

Step 510, Epoch 7/50, Batch 30/80
train_loss: 0.0196

Step 515, Epoch 7/50, Batch 35/80
train_loss: 0.0194

Step 520, Epoch 7/50, Batch 40/80
train_loss: 0.0191

Step 525, Epoch 7/50, Batch 45/80
train_loss: 0.0189

Step 530, Epoch 7/50, Batch 50/80
train_loss: 0.0191

Step 535, Epoch 7/50, Batch 55/80
train_loss: 0.0193

Step 540, Epoch 7/50, Batch 60/80
train_loss: 0.0186

Step 545, Epoch 7/50, Batch 65/80
train_loss: 0.0185

Step 550, Epoch 7/50, Batch 70/80
train_loss: 0.0184

Step 555, Epoch 7/50, Batch 75/80
train_loss: 0.0182

Epoch 7/50 completed, Global Step: 559
train_loss: 0.0182, val_loss: 0.0183

---------------------------------------------------------------------------


Step 560, Epoch 8/50, Batch 0/80
train_loss: 0.0185

Step 565, Epoch 8/50, Batch 5/80
train_loss: 0.0184

Step 570, Epoch 8/50, Batch 10/80
train_loss: 0.0179

Step 575, Epoch 8/50, Batch 15/80
train_loss: 0.0179

Step 580, Epoch 8/50, Batch 20/80
train_loss: 0.0179

Step 585, Epoch 8/50, Batch 25/80
train_loss: 0.0177

Step 590, Epoch 8/50, Batch 30/80
train_loss: 0.0181

Step 595, Epoch 8/50, Batch 35/80
train_loss: 0.0178

Step 600, Epoch 8/50, Batch 40/80
train_loss: 0.0176

Step 605, Epoch 8/50, Batch 45/80
train_loss: 0.0172

Step 610, Epoch 8/50, Batch 50/80
train_loss: 0.0170

Step 615, Epoch 8/50, Batch 55/80
train_loss: 0.0176

Step 620, Epoch 8/50, Batch 60/80
train_loss: 0.0170

Step 625, Epoch 8/50, Batch 65/80
train_loss: 0.0171

Step 630, Epoch 8/50, Batch 70/80
train_loss: 0.0171

Step 635, Epoch 8/50, Batch 75/80
train_loss: 0.0185

Epoch 8/50 completed, Global Step: 639
train_loss: 0.0185, val_loss: 0.0173

---------------------------------------------------------------------------


Step 640, Epoch 9/50, Batch 0/80
train_loss: 0.0172

Step 645, Epoch 9/50, Batch 5/80
train_loss: 0.0173

Step 650, Epoch 9/50, Batch 10/80
train_loss: 0.0170

Step 655, Epoch 9/50, Batch 15/80
train_loss: 0.0168

Step 660, Epoch 9/50, Batch 20/80
train_loss: 0.0165

Step 665, Epoch 9/50, Batch 25/80
train_loss: 0.0160

Step 670, Epoch 9/50, Batch 30/80
train_loss: 0.0152

Step 675, Epoch 9/50, Batch 35/80
train_loss: 0.0317

Step 680, Epoch 9/50, Batch 40/80
train_loss: 0.0242

Step 685, Epoch 9/50, Batch 45/80
train_loss: 0.0201

Step 690, Epoch 9/50, Batch 50/80
train_loss: 0.0199

Step 695, Epoch 9/50, Batch 55/80
train_loss: 0.0194

Step 700, Epoch 9/50, Batch 60/80
train_loss: 0.0186

Step 705, Epoch 9/50, Batch 65/80
train_loss: 0.0185

Step 710, Epoch 9/50, Batch 70/80
train_loss: 0.0182

Step 715, Epoch 9/50, Batch 75/80
train_loss: 0.0181

Epoch 9/50 completed, Global Step: 719
train_loss: 0.0181, val_loss: 0.0180

---------------------------------------------------------------------------


Step 720, Epoch 10/50, Batch 0/80
train_loss: 0.0181

Step 725, Epoch 10/50, Batch 5/80
train_loss: 0.0179

Step 730, Epoch 10/50, Batch 10/80
train_loss: 0.0179

Step 735, Epoch 10/50, Batch 15/80
train_loss: 0.0178

Step 740, Epoch 10/50, Batch 20/80
train_loss: 0.0175

Step 745, Epoch 10/50, Batch 25/80
train_loss: 0.0174

Step 750, Epoch 10/50, Batch 30/80
train_loss: 0.0175

Step 755, Epoch 10/50, Batch 35/80
train_loss: 0.0174

Step 760, Epoch 10/50, Batch 40/80
train_loss: 0.0173

Step 765, Epoch 10/50, Batch 45/80
train_loss: 0.0173

Step 770, Epoch 10/50, Batch 50/80
train_loss: 0.0172

Step 775, Epoch 10/50, Batch 55/80
train_loss: 0.0169

Step 780, Epoch 10/50, Batch 60/80
train_loss: 0.0169

Step 785, Epoch 10/50, Batch 65/80
train_loss: 0.0167

Step 790, Epoch 10/50, Batch 70/80
train_loss: 0.0165

Step 795, Epoch 10/50, Batch 75/80
train_loss: 0.0166

Epoch 10/50 completed, Global Step: 799
train_loss: 0.0166, val_loss: 0.0164

---------------------------------------------------------------------------


Step 800, Epoch 11/50, Batch 0/80
train_loss: 0.0165

Step 805, Epoch 11/50, Batch 5/80
train_loss: 0.0162

Step 810, Epoch 11/50, Batch 10/80
train_loss: 0.0161

Step 815, Epoch 11/50, Batch 15/80
train_loss: 0.0159

Step 820, Epoch 11/50, Batch 20/80
train_loss: 0.0159

Step 825, Epoch 11/50, Batch 25/80
train_loss: 0.0159

Step 830, Epoch 11/50, Batch 30/80
train_loss: 0.0159

Step 835, Epoch 11/50, Batch 35/80
train_loss: 0.0159

Step 840, Epoch 11/50, Batch 40/80
train_loss: 0.0156

Step 845, Epoch 11/50, Batch 45/80
train_loss: 0.0158

Step 850, Epoch 11/50, Batch 50/80
train_loss: 0.0155

Step 855, Epoch 11/50, Batch 55/80
train_loss: 0.0155

Step 860, Epoch 11/50, Batch 60/80
train_loss: 0.0154

Step 865, Epoch 11/50, Batch 65/80
train_loss: 0.0150

Step 870, Epoch 11/50, Batch 70/80
train_loss: 0.0151

Step 875, Epoch 11/50, Batch 75/80
train_loss: 0.0154

Epoch 11/50 completed, Global Step: 879
train_loss: 0.0154, val_loss: 0.0157

---------------------------------------------------------------------------


Step 880, Epoch 12/50, Batch 0/80
train_loss: 0.0157

Step 885, Epoch 12/50, Batch 5/80
train_loss: 0.0158

Step 890, Epoch 12/50, Batch 10/80
train_loss: 0.0146

Step 895, Epoch 12/50, Batch 15/80
train_loss: 0.0146

Step 900, Epoch 12/50, Batch 20/80
train_loss: 0.0145

Step 905, Epoch 12/50, Batch 25/80
train_loss: 0.0145

Step 910, Epoch 12/50, Batch 30/80
train_loss: 0.0141

Step 915, Epoch 12/50, Batch 35/80
train_loss: 0.0142

Step 920, Epoch 12/50, Batch 40/80
train_loss: 0.0137

Step 925, Epoch 12/50, Batch 45/80
train_loss: 0.0169

Step 930, Epoch 12/50, Batch 50/80
train_loss: 0.0164

Step 935, Epoch 12/50, Batch 55/80
train_loss: 0.0142

Step 940, Epoch 12/50, Batch 60/80
train_loss: 0.0128

Step 945, Epoch 12/50, Batch 65/80
train_loss: 0.0127

Step 950, Epoch 12/50, Batch 70/80
train_loss: 0.0119

Step 955, Epoch 12/50, Batch 75/80
train_loss: 0.0114

Epoch 12/50 completed, Global Step: 959
train_loss: 0.0114, val_loss: 0.0114

---------------------------------------------------------------------------


Step 960, Epoch 13/50, Batch 0/80
train_loss: 0.0114

Step 965, Epoch 13/50, Batch 5/80
train_loss: 0.0122

Step 970, Epoch 13/50, Batch 10/80
train_loss: 0.0108

Step 975, Epoch 13/50, Batch 15/80
train_loss: 0.0107

Step 980, Epoch 13/50, Batch 20/80
train_loss: 0.0132

Step 985, Epoch 13/50, Batch 25/80
train_loss: 0.0113

Step 990, Epoch 13/50, Batch 30/80
train_loss: 0.0104

Step 995, Epoch 13/50, Batch 35/80
train_loss: 0.0106

Step 1000, Epoch 13/50, Batch 40/80
train_loss: 0.0124

Step 1005, Epoch 13/50, Batch 45/80
train_loss: 0.0103

Step 1010, Epoch 13/50, Batch 50/80
train_loss: 0.0111

Step 1015, Epoch 13/50, Batch 55/80
train_loss: 0.0098

Step 1020, Epoch 13/50, Batch 60/80
train_loss: 0.0099

Step 1025, Epoch 13/50, Batch 65/80
train_loss: 0.0095

Step 1030, Epoch 13/50, Batch 70/80
train_loss: 0.0102

Step 1035, Epoch 13/50, Batch 75/80
train_loss: 0.0095

Epoch 13/50 completed, Global Step: 1039
train_loss: 0.0095, val_loss: 0.0092

---------------------------------------------------------------------------


Step 1040, Epoch 14/50, Batch 0/80
train_loss: 0.0092

Step 1045, Epoch 14/50, Batch 5/80
train_loss: 0.0089

Step 1050, Epoch 14/50, Batch 10/80
train_loss: 0.0090

Step 1055, Epoch 14/50, Batch 15/80
train_loss: 0.0084

Step 1060, Epoch 14/50, Batch 20/80
train_loss: 0.0115

Step 1065, Epoch 14/50, Batch 25/80
train_loss: 0.0101

Step 1070, Epoch 14/50, Batch 30/80
train_loss: 0.0065

Step 1075, Epoch 14/50, Batch 35/80
train_loss: 0.0064

Step 1080, Epoch 14/50, Batch 40/80
train_loss: 0.0056

Step 1085, Epoch 14/50, Batch 45/80
train_loss: 0.0051

Step 1090, Epoch 14/50, Batch 50/80
train_loss: 0.0044

Step 1095, Epoch 14/50, Batch 55/80
train_loss: 0.0041

Step 1100, Epoch 14/50, Batch 60/80
train_loss: 0.0085

Step 1105, Epoch 14/50, Batch 65/80
train_loss: 0.0067

Step 1110, Epoch 14/50, Batch 70/80
train_loss: 0.0051

Step 1115, Epoch 14/50, Batch 75/80
train_loss: 0.0040

Epoch 14/50 completed, Global Step: 1119
train_loss: 0.0040, val_loss: 0.0045

---------------------------------------------------------------------------


Step 1120, Epoch 15/50, Batch 0/80
train_loss: 0.0044

Step 1125, Epoch 15/50, Batch 5/80
train_loss: 0.0034

Step 1130, Epoch 15/50, Batch 10/80
train_loss: 0.0033

Step 1135, Epoch 15/50, Batch 15/80
train_loss: 0.0032

Step 1140, Epoch 15/50, Batch 20/80
train_loss: 0.0028

Step 1145, Epoch 15/50, Batch 25/80
train_loss: 0.0028

Step 1150, Epoch 15/50, Batch 30/80
train_loss: 0.0027

Step 1155, Epoch 15/50, Batch 35/80
train_loss: 0.0025

Step 1160, Epoch 15/50, Batch 40/80
train_loss: 0.0027

Step 1165, Epoch 15/50, Batch 45/80
train_loss: 0.0024

Step 1170, Epoch 15/50, Batch 50/80
train_loss: 0.0020

Step 1175, Epoch 15/50, Batch 55/80
train_loss: 0.0064

Step 1180, Epoch 15/50, Batch 60/80
train_loss: 0.0046

Step 1185, Epoch 15/50, Batch 65/80
train_loss: 0.0028

Step 1190, Epoch 15/50, Batch 70/80
train_loss: 0.0030

Step 1195, Epoch 15/50, Batch 75/80
train_loss: 0.0022

Epoch 15/50 completed, Global Step: 1199
train_loss: 0.0022, val_loss: 0.0022

---------------------------------------------------------------------------


Step 1200, Epoch 16/50, Batch 0/80
train_loss: 0.0022

Step 1205, Epoch 16/50, Batch 5/80
train_loss: 0.0019

Step 1210, Epoch 16/50, Batch 10/80
train_loss: 0.0018

Step 1215, Epoch 16/50, Batch 15/80
train_loss: 0.0017

Step 1220, Epoch 16/50, Batch 20/80
train_loss: 0.0017

Step 1225, Epoch 16/50, Batch 25/80
train_loss: 0.0022

Step 1230, Epoch 16/50, Batch 30/80
train_loss: 0.0017

Step 1235, Epoch 16/50, Batch 35/80
train_loss: 0.0020

Step 1240, Epoch 16/50, Batch 40/80
train_loss: 0.0017

Step 1245, Epoch 16/50, Batch 45/80
train_loss: 0.0016

Step 1250, Epoch 16/50, Batch 50/80
train_loss: 0.0016

Step 1255, Epoch 16/50, Batch 55/80
train_loss: 0.0015

Step 1260, Epoch 16/50, Batch 60/80
train_loss: 0.0017

Step 1265, Epoch 16/50, Batch 65/80
train_loss: 0.0021

Step 1270, Epoch 16/50, Batch 70/80
train_loss: 0.0015

Step 1275, Epoch 16/50, Batch 75/80
train_loss: 0.0015

Epoch 16/50 completed, Global Step: 1279
train_loss: 0.0015, val_loss: 0.0023

---------------------------------------------------------------------------


Step 1280, Epoch 17/50, Batch 0/80
train_loss: 0.0022

Step 1285, Epoch 17/50, Batch 5/80
train_loss: 0.0020

Step 1290, Epoch 17/50, Batch 10/80
train_loss: 0.0014

Step 1295, Epoch 17/50, Batch 15/80
train_loss: 0.0015

Step 1300, Epoch 17/50, Batch 20/80
train_loss: 0.0014

Step 1305, Epoch 17/50, Batch 25/80
train_loss: 0.0014

Step 1310, Epoch 17/50, Batch 30/80
train_loss: 0.0014

Step 1315, Epoch 17/50, Batch 35/80
train_loss: 0.0013

Step 1320, Epoch 17/50, Batch 40/80
train_loss: 0.0013

Step 1325, Epoch 17/50, Batch 45/80
train_loss: 0.0038

Step 1330, Epoch 17/50, Batch 50/80
train_loss: 0.0019

Step 1335, Epoch 17/50, Batch 55/80
train_loss: 0.0017

Step 1340, Epoch 17/50, Batch 60/80
train_loss: 0.0012

Step 1345, Epoch 17/50, Batch 65/80
train_loss: 0.0013

Step 1350, Epoch 17/50, Batch 70/80
train_loss: 0.0015

Step 1355, Epoch 17/50, Batch 75/80
train_loss: 0.0013

Epoch 17/50 completed, Global Step: 1359
train_loss: 0.0013, val_loss: 0.0012

---------------------------------------------------------------------------


Step 1360, Epoch 18/50, Batch 0/80
train_loss: 0.0011

Step 1365, Epoch 18/50, Batch 5/80
train_loss: 0.0012

Step 1370, Epoch 18/50, Batch 10/80
train_loss: 0.0011

Step 1375, Epoch 18/50, Batch 15/80
train_loss: 0.0011

Step 1380, Epoch 18/50, Batch 20/80
train_loss: 0.0013

Step 1385, Epoch 18/50, Batch 25/80
train_loss: 0.0011

Step 1390, Epoch 18/50, Batch 30/80
train_loss: 0.0012

Step 1395, Epoch 18/50, Batch 35/80
train_loss: 0.0011

Step 1400, Epoch 18/50, Batch 40/80
train_loss: 0.0011

Step 1405, Epoch 18/50, Batch 45/80
train_loss: 0.0019

Step 1410, Epoch 18/50, Batch 50/80
train_loss: 0.0025

Step 1415, Epoch 18/50, Batch 55/80
train_loss: 0.0033

Step 1420, Epoch 18/50, Batch 60/80
train_loss: 0.0024

Step 1425, Epoch 18/50, Batch 65/80
train_loss: 0.0018

Step 1430, Epoch 18/50, Batch 70/80
train_loss: 0.0015

Step 1435, Epoch 18/50, Batch 75/80
train_loss: 0.0016

Epoch 18/50 completed, Global Step: 1439
train_loss: 0.0016, val_loss: 0.0014

---------------------------------------------------------------------------


Step 1440, Epoch 19/50, Batch 0/80
train_loss: 0.0014

Step 1445, Epoch 19/50, Batch 5/80
train_loss: 0.0011

Step 1450, Epoch 19/50, Batch 10/80
train_loss: 0.0011

Step 1455, Epoch 19/50, Batch 15/80
train_loss: 0.0010

Step 1460, Epoch 19/50, Batch 20/80
train_loss: 0.0010

Step 1465, Epoch 19/50, Batch 25/80
train_loss: 0.0010

Step 1470, Epoch 19/50, Batch 30/80
train_loss: 0.0010

Step 1475, Epoch 19/50, Batch 35/80
train_loss: 0.0010

Step 1480, Epoch 19/50, Batch 40/80
train_loss: 0.0011

Step 1485, Epoch 19/50, Batch 45/80
train_loss: 0.0010

Step 1490, Epoch 19/50, Batch 50/80
train_loss: 0.0009

Step 1495, Epoch 19/50, Batch 55/80
train_loss: 0.0009

Step 1500, Epoch 19/50, Batch 60/80
train_loss: 0.0010

Step 1505, Epoch 19/50, Batch 65/80
train_loss: 0.0009

Step 1510, Epoch 19/50, Batch 70/80
train_loss: 0.0009

Step 1515, Epoch 19/50, Batch 75/80
train_loss: 0.0010

Epoch 19/50 completed, Global Step: 1519
train_loss: 0.0010, val_loss: 0.0011

---------------------------------------------------------------------------


Step 1520, Epoch 20/50, Batch 0/80
train_loss: 0.0010

Step 1525, Epoch 20/50, Batch 5/80
train_loss: 0.0009

Step 1530, Epoch 20/50, Batch 10/80
train_loss: 0.0010

Step 1535, Epoch 20/50, Batch 15/80
train_loss: 0.0010

Step 1540, Epoch 20/50, Batch 20/80
train_loss: 0.0009

Step 1545, Epoch 20/50, Batch 25/80
train_loss: 0.0010

Step 1550, Epoch 20/50, Batch 30/80
train_loss: 0.0013

Step 1555, Epoch 20/50, Batch 35/80
train_loss: 0.0015

Step 1560, Epoch 20/50, Batch 40/80
train_loss: 0.0013

Step 1565, Epoch 20/50, Batch 45/80
train_loss: 0.0014

Step 1570, Epoch 20/50, Batch 50/80
train_loss: 0.0016

Step 1575, Epoch 20/50, Batch 55/80
train_loss: 0.0014

Step 1580, Epoch 20/50, Batch 60/80
train_loss: 0.0010

Step 1585, Epoch 20/50, Batch 65/80
train_loss: 0.0009

Step 1590, Epoch 20/50, Batch 70/80
train_loss: 0.0009

Step 1595, Epoch 20/50, Batch 75/80
train_loss: 0.0009

Epoch 20/50 completed, Global Step: 1599
train_loss: 0.0009, val_loss: 0.0008

---------------------------------------------------------------------------


Step 1600, Epoch 21/50, Batch 0/80
train_loss: 0.0009

Step 1605, Epoch 21/50, Batch 5/80
train_loss: 0.0008

Step 1610, Epoch 21/50, Batch 10/80
train_loss: 0.0008

Step 1615, Epoch 21/50, Batch 15/80
train_loss: 0.0008

Step 1620, Epoch 21/50, Batch 20/80
train_loss: 0.0008

Step 1625, Epoch 21/50, Batch 25/80
train_loss: 0.0009

Step 1630, Epoch 21/50, Batch 30/80
train_loss: 0.0012

Step 1635, Epoch 21/50, Batch 35/80
train_loss: 0.0011

Step 1640, Epoch 21/50, Batch 40/80
train_loss: 0.0012

Step 1645, Epoch 21/50, Batch 45/80
train_loss: 0.0010

Step 1650, Epoch 21/50, Batch 50/80
train_loss: 0.0008

Step 1655, Epoch 21/50, Batch 55/80
train_loss: 0.0007

Step 1660, Epoch 21/50, Batch 60/80
train_loss: 0.0008

Step 1665, Epoch 21/50, Batch 65/80
train_loss: 0.0010

Step 1670, Epoch 21/50, Batch 70/80
train_loss: 0.0008

Step 1675, Epoch 21/50, Batch 75/80
train_loss: 0.0008

Epoch 21/50 completed, Global Step: 1679
train_loss: 0.0008, val_loss: 0.0012

---------------------------------------------------------------------------


Step 1680, Epoch 22/50, Batch 0/80
train_loss: 0.0012

Step 1685, Epoch 22/50, Batch 5/80
train_loss: 0.0008

Step 1690, Epoch 22/50, Batch 10/80
train_loss: 0.0007

Step 1695, Epoch 22/50, Batch 15/80
train_loss: 0.0008

Step 1700, Epoch 22/50, Batch 20/80
train_loss: 0.0011

Step 1705, Epoch 22/50, Batch 25/80
train_loss: 0.0007

Step 1710, Epoch 22/50, Batch 30/80
train_loss: 0.0007

Step 1715, Epoch 22/50, Batch 35/80
train_loss: 0.0007

Step 1720, Epoch 22/50, Batch 40/80
train_loss: 0.0007

Step 1725, Epoch 22/50, Batch 45/80
train_loss: 0.0007

Step 1730, Epoch 22/50, Batch 50/80
train_loss: 0.0007

Step 1735, Epoch 22/50, Batch 55/80
train_loss: 0.0007

Step 1740, Epoch 22/50, Batch 60/80
train_loss: 0.0007

Step 1745, Epoch 22/50, Batch 65/80
train_loss: 0.0006

Step 1750, Epoch 22/50, Batch 70/80
train_loss: 0.0007

Step 1755, Epoch 22/50, Batch 75/80
train_loss: 0.0008

Epoch 22/50 completed, Global Step: 1759
train_loss: 0.0008, val_loss: 0.0010

---------------------------------------------------------------------------


Step 1760, Epoch 23/50, Batch 0/80
train_loss: 0.0010

Step 1765, Epoch 23/50, Batch 5/80
train_loss: 0.0006

Step 1770, Epoch 23/50, Batch 10/80
train_loss: 0.0007

Step 1775, Epoch 23/50, Batch 15/80
train_loss: 0.0007

Step 1780, Epoch 23/50, Batch 20/80
train_loss: 0.0007

Step 1785, Epoch 23/50, Batch 25/80
train_loss: 0.0010

Step 1790, Epoch 23/50, Batch 30/80
train_loss: 0.0008

Step 1795, Epoch 23/50, Batch 35/80
train_loss: 0.0006

Step 1800, Epoch 23/50, Batch 40/80
train_loss: 0.0007

Step 1805, Epoch 23/50, Batch 45/80
train_loss: 0.0007

Step 1810, Epoch 23/50, Batch 50/80
train_loss: 0.0008

Step 1815, Epoch 23/50, Batch 55/80
train_loss: 0.0007

Step 1820, Epoch 23/50, Batch 60/80
train_loss: 0.0016

Step 1825, Epoch 23/50, Batch 65/80
train_loss: 0.0008

Step 1830, Epoch 23/50, Batch 70/80
train_loss: 0.0009

Step 1835, Epoch 23/50, Batch 75/80
train_loss: 0.0006

Epoch 23/50 completed, Global Step: 1839
train_loss: 0.0006, val_loss: 0.0006

---------------------------------------------------------------------------


Step 1840, Epoch 24/50, Batch 0/80
train_loss: 0.0006

Step 1845, Epoch 24/50, Batch 5/80
train_loss: 0.0006

Step 1850, Epoch 24/50, Batch 10/80
train_loss: 0.0006

Step 1855, Epoch 24/50, Batch 15/80
train_loss: 0.0006

Step 1860, Epoch 24/50, Batch 20/80
train_loss: 0.0008

Step 1865, Epoch 24/50, Batch 25/80
train_loss: 0.0006

Step 1870, Epoch 24/50, Batch 30/80
train_loss: 0.0008

Step 1875, Epoch 24/50, Batch 35/80
train_loss: 0.0006

Step 1880, Epoch 24/50, Batch 40/80
train_loss: 0.0007

Step 1885, Epoch 24/50, Batch 45/80
train_loss: 0.0007

Step 1890, Epoch 24/50, Batch 50/80
train_loss: 0.0006

Step 1895, Epoch 24/50, Batch 55/80
train_loss: 0.0007

Step 1900, Epoch 24/50, Batch 60/80
train_loss: 0.0007

Step 1905, Epoch 24/50, Batch 65/80
train_loss: 0.0007

Step 1910, Epoch 24/50, Batch 70/80
train_loss: 0.0007

Step 1915, Epoch 24/50, Batch 75/80
train_loss: 0.0007

Epoch 24/50 completed, Global Step: 1919
train_loss: 0.0007, val_loss: 0.0006

---------------------------------------------------------------------------


Step 1920, Epoch 25/50, Batch 0/80
train_loss: 0.0006

Step 1925, Epoch 25/50, Batch 5/80
train_loss: 0.0006

Step 1930, Epoch 25/50, Batch 10/80
train_loss: 0.0006

Step 1935, Epoch 25/50, Batch 15/80
train_loss: 0.0006

Step 1940, Epoch 25/50, Batch 20/80
train_loss: 0.0006

Step 1945, Epoch 25/50, Batch 25/80
train_loss: 0.0006

Step 1950, Epoch 25/50, Batch 30/80
train_loss: 0.0005

Step 1955, Epoch 25/50, Batch 35/80
train_loss: 0.0006

Step 1960, Epoch 25/50, Batch 40/80
train_loss: 0.0005

Step 1965, Epoch 25/50, Batch 45/80
train_loss: 0.0019

Step 1970, Epoch 25/50, Batch 50/80
train_loss: 0.0017

Step 1975, Epoch 25/50, Batch 55/80
train_loss: 0.0007

Step 1980, Epoch 25/50, Batch 60/80
train_loss: 0.0014

Step 1985, Epoch 25/50, Batch 65/80
train_loss: 0.0008

Step 1990, Epoch 25/50, Batch 70/80
train_loss: 0.0007

Step 1995, Epoch 25/50, Batch 75/80
train_loss: 0.0007

Epoch 25/50 completed, Global Step: 1999
train_loss: 0.0007, val_loss: 0.0007

---------------------------------------------------------------------------


Step 2000, Epoch 26/50, Batch 0/80
train_loss: 0.0006

Step 2005, Epoch 26/50, Batch 5/80
train_loss: 0.0006

Step 2010, Epoch 26/50, Batch 10/80
train_loss: 0.0006

Step 2015, Epoch 26/50, Batch 15/80
train_loss: 0.0005

Step 2020, Epoch 26/50, Batch 20/80
train_loss: 0.0006

Step 2025, Epoch 26/50, Batch 25/80
train_loss: 0.0006

Step 2030, Epoch 26/50, Batch 30/80
train_loss: 0.0005

Step 2035, Epoch 26/50, Batch 35/80
train_loss: 0.0005

Step 2040, Epoch 26/50, Batch 40/80
train_loss: 0.0006

Step 2045, Epoch 26/50, Batch 45/80
train_loss: 0.0006

Step 2050, Epoch 26/50, Batch 50/80
train_loss: 0.0005

Step 2055, Epoch 26/50, Batch 55/80
train_loss: 0.0006

Step 2060, Epoch 26/50, Batch 60/80
train_loss: 0.0005

Step 2065, Epoch 26/50, Batch 65/80
train_loss: 0.0005

Step 2070, Epoch 26/50, Batch 70/80
train_loss: 0.0005

Step 2075, Epoch 26/50, Batch 75/80
train_loss: 0.0004

Epoch 26/50 completed, Global Step: 2079
train_loss: 0.0004, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2080, Epoch 27/50, Batch 0/80
train_loss: 0.0004

Step 2085, Epoch 27/50, Batch 5/80
train_loss: 0.0006

Step 2090, Epoch 27/50, Batch 10/80
train_loss: 0.0007

Step 2095, Epoch 27/50, Batch 15/80
train_loss: 0.0005

Step 2100, Epoch 27/50, Batch 20/80
train_loss: 0.0004

Step 2105, Epoch 27/50, Batch 25/80
train_loss: 0.0005

Step 2110, Epoch 27/50, Batch 30/80
train_loss: 0.0006

Step 2115, Epoch 27/50, Batch 35/80
train_loss: 0.0005

Step 2120, Epoch 27/50, Batch 40/80
train_loss: 0.0004

Step 2125, Epoch 27/50, Batch 45/80
train_loss: 0.0005

Step 2130, Epoch 27/50, Batch 50/80
train_loss: 0.0005

Step 2135, Epoch 27/50, Batch 55/80
train_loss: 0.0004

Step 2140, Epoch 27/50, Batch 60/80
train_loss: 0.0004

Step 2145, Epoch 27/50, Batch 65/80
train_loss: 0.0004

Step 2150, Epoch 27/50, Batch 70/80
train_loss: 0.0004

Step 2155, Epoch 27/50, Batch 75/80
train_loss: 0.0021

Epoch 27/50 completed, Global Step: 2159
train_loss: 0.0021, val_loss: 0.0011

---------------------------------------------------------------------------


Step 2160, Epoch 28/50, Batch 0/80
train_loss: 0.0012

Step 2165, Epoch 28/50, Batch 5/80
train_loss: 0.0009

Step 2170, Epoch 28/50, Batch 10/80
train_loss: 0.0005

Step 2175, Epoch 28/50, Batch 15/80
train_loss: 0.0005

Step 2180, Epoch 28/50, Batch 20/80
train_loss: 0.0007

Step 2185, Epoch 28/50, Batch 25/80
train_loss: 0.0005

Step 2190, Epoch 28/50, Batch 30/80
train_loss: 0.0006

Step 2195, Epoch 28/50, Batch 35/80
train_loss: 0.0005

Step 2200, Epoch 28/50, Batch 40/80
train_loss: 0.0004

Step 2205, Epoch 28/50, Batch 45/80
train_loss: 0.0005

Step 2210, Epoch 28/50, Batch 50/80
train_loss: 0.0005

Step 2215, Epoch 28/50, Batch 55/80
train_loss: 0.0004

Step 2220, Epoch 28/50, Batch 60/80
train_loss: 0.0005

Step 2225, Epoch 28/50, Batch 65/80
train_loss: 0.0006

Step 2230, Epoch 28/50, Batch 70/80
train_loss: 0.0005

Step 2235, Epoch 28/50, Batch 75/80
train_loss: 0.0004

Epoch 28/50 completed, Global Step: 2239
train_loss: 0.0004, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2240, Epoch 29/50, Batch 0/80
train_loss: 0.0004

Step 2245, Epoch 29/50, Batch 5/80
train_loss: 0.0004

Step 2250, Epoch 29/50, Batch 10/80
train_loss: 0.0004

Step 2255, Epoch 29/50, Batch 15/80
train_loss: 0.0004

Step 2260, Epoch 29/50, Batch 20/80
train_loss: 0.0004

Step 2265, Epoch 29/50, Batch 25/80
train_loss: 0.0004

Step 2270, Epoch 29/50, Batch 30/80
train_loss: 0.0004

Step 2275, Epoch 29/50, Batch 35/80
train_loss: 0.0004

Step 2280, Epoch 29/50, Batch 40/80
train_loss: 0.0004

Step 2285, Epoch 29/50, Batch 45/80
train_loss: 0.0005

Step 2290, Epoch 29/50, Batch 50/80
train_loss: 0.0004

Step 2295, Epoch 29/50, Batch 55/80
train_loss: 0.0006

Step 2300, Epoch 29/50, Batch 60/80
train_loss: 0.0003

Step 2305, Epoch 29/50, Batch 65/80
train_loss: 0.0004

Step 2310, Epoch 29/50, Batch 70/80
train_loss: 0.0004

Step 2315, Epoch 29/50, Batch 75/80
train_loss: 0.0004

Epoch 29/50 completed, Global Step: 2319
train_loss: 0.0004, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2320, Epoch 30/50, Batch 0/80
train_loss: 0.0004

Step 2325, Epoch 30/50, Batch 5/80
train_loss: 0.0004

Step 2330, Epoch 30/50, Batch 10/80
train_loss: 0.0004

Step 2335, Epoch 30/50, Batch 15/80
train_loss: 0.0006

Step 2340, Epoch 30/50, Batch 20/80
train_loss: 0.0006

Step 2345, Epoch 30/50, Batch 25/80
train_loss: 0.0009

Step 2350, Epoch 30/50, Batch 30/80
train_loss: 0.0005

Step 2355, Epoch 30/50, Batch 35/80
train_loss: 0.0004

Step 2360, Epoch 30/50, Batch 40/80
train_loss: 0.0004

Step 2365, Epoch 30/50, Batch 45/80
train_loss: 0.0004

Step 2370, Epoch 30/50, Batch 50/80
train_loss: 0.0003

Step 2375, Epoch 30/50, Batch 55/80
train_loss: 0.0004

Step 2380, Epoch 30/50, Batch 60/80
train_loss: 0.0003

Step 2385, Epoch 30/50, Batch 65/80
train_loss: 0.0004

Step 2390, Epoch 30/50, Batch 70/80
train_loss: 0.0003

Step 2395, Epoch 30/50, Batch 75/80
train_loss: 0.0004

Epoch 30/50 completed, Global Step: 2399
train_loss: 0.0004, val_loss: 0.0003

---------------------------------------------------------------------------


Step 2400, Epoch 31/50, Batch 0/80
train_loss: 0.0003

Step 2405, Epoch 31/50, Batch 5/80
train_loss: 0.0003

Step 2410, Epoch 31/50, Batch 10/80
train_loss: 0.0004

Step 2415, Epoch 31/50, Batch 15/80
train_loss: 0.0012

Step 2420, Epoch 31/50, Batch 20/80
train_loss: 0.0012

Step 2425, Epoch 31/50, Batch 25/80
train_loss: 0.0020

Step 2430, Epoch 31/50, Batch 30/80
train_loss: 0.0005

Step 2435, Epoch 31/50, Batch 35/80
train_loss: 0.0005

Step 2440, Epoch 31/50, Batch 40/80
train_loss: 0.0004

Step 2445, Epoch 31/50, Batch 45/80
train_loss: 0.0004

Step 2450, Epoch 31/50, Batch 50/80
train_loss: 0.0003

Step 2455, Epoch 31/50, Batch 55/80
train_loss: 0.0003

Step 2460, Epoch 31/50, Batch 60/80
train_loss: 0.0003

Step 2465, Epoch 31/50, Batch 65/80
train_loss: 0.0003

Step 2470, Epoch 31/50, Batch 70/80
train_loss: 0.0004

Step 2475, Epoch 31/50, Batch 75/80
train_loss: 0.0004

Epoch 31/50 completed, Global Step: 2479
train_loss: 0.0004, val_loss: 0.0003

---------------------------------------------------------------------------


Step 2480, Epoch 32/50, Batch 0/80
train_loss: 0.0003

Step 2485, Epoch 32/50, Batch 5/80
train_loss: 0.0004

Step 2490, Epoch 32/50, Batch 10/80
train_loss: 0.0003

Step 2495, Epoch 32/50, Batch 15/80
train_loss: 0.0004

Step 2500, Epoch 32/50, Batch 20/80
train_loss: 0.0004

Step 2505, Epoch 32/50, Batch 25/80
train_loss: 0.0004

Step 2510, Epoch 32/50, Batch 30/80
train_loss: 0.0005

Step 2515, Epoch 32/50, Batch 35/80
train_loss: 0.0003

Step 2520, Epoch 32/50, Batch 40/80
train_loss: 0.0003

Step 2525, Epoch 32/50, Batch 45/80
train_loss: 0.0003

Step 2530, Epoch 32/50, Batch 50/80
train_loss: 0.0003

Step 2535, Epoch 32/50, Batch 55/80
train_loss: 0.0004

Step 2540, Epoch 32/50, Batch 60/80
train_loss: 0.0003

Step 2545, Epoch 32/50, Batch 65/80
train_loss: 0.0005

Step 2550, Epoch 32/50, Batch 70/80
train_loss: 0.0007

Step 2555, Epoch 32/50, Batch 75/80
train_loss: 0.0006

Epoch 32/50 completed, Global Step: 2559
train_loss: 0.0006, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2560, Epoch 33/50, Batch 0/80
train_loss: 0.0004

Step 2565, Epoch 33/50, Batch 5/80
train_loss: 0.0004

Step 2570, Epoch 33/50, Batch 10/80
train_loss: 0.0003

Step 2575, Epoch 33/50, Batch 15/80
train_loss: 0.0003

Step 2580, Epoch 33/50, Batch 20/80
train_loss: 0.0003

Step 2585, Epoch 33/50, Batch 25/80
train_loss: 0.0003

Step 2590, Epoch 33/50, Batch 30/80
train_loss: 0.0003

Step 2595, Epoch 33/50, Batch 35/80
train_loss: 0.0003

Step 2600, Epoch 33/50, Batch 40/80
train_loss: 0.0003

Step 2605, Epoch 33/50, Batch 45/80
train_loss: 0.0003

Step 2610, Epoch 33/50, Batch 50/80
train_loss: 0.0003

Step 2615, Epoch 33/50, Batch 55/80
train_loss: 0.0003

Step 2620, Epoch 33/50, Batch 60/80
train_loss: 0.0004

Step 2625, Epoch 33/50, Batch 65/80
train_loss: 0.0003

Step 2630, Epoch 33/50, Batch 70/80
train_loss: 0.0004

Step 2635, Epoch 33/50, Batch 75/80
train_loss: 0.0005

Epoch 33/50 completed, Global Step: 2639
train_loss: 0.0005, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2640, Epoch 34/50, Batch 0/80
train_loss: 0.0004

Step 2645, Epoch 34/50, Batch 5/80
train_loss: 0.0003

Step 2650, Epoch 34/50, Batch 10/80
train_loss: 0.0004

Step 2655, Epoch 34/50, Batch 15/80
train_loss: 0.0004

Step 2660, Epoch 34/50, Batch 20/80
train_loss: 0.0003

Step 2665, Epoch 34/50, Batch 25/80
train_loss: 0.0003

Step 2670, Epoch 34/50, Batch 30/80
train_loss: 0.0003

Step 2675, Epoch 34/50, Batch 35/80
train_loss: 0.0003

Step 2680, Epoch 34/50, Batch 40/80
train_loss: 0.0003

Step 2685, Epoch 34/50, Batch 45/80
train_loss: 0.0009

Step 2690, Epoch 34/50, Batch 50/80
train_loss: 0.0008

Step 2695, Epoch 34/50, Batch 55/80
train_loss: 0.0008

Step 2700, Epoch 34/50, Batch 60/80
train_loss: 0.0005

Step 2705, Epoch 34/50, Batch 65/80
train_loss: 0.0005

Step 2710, Epoch 34/50, Batch 70/80
train_loss: 0.0005

Step 2715, Epoch 34/50, Batch 75/80
train_loss: 0.0003

Epoch 34/50 completed, Global Step: 2719
train_loss: 0.0003, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2720, Epoch 35/50, Batch 0/80
train_loss: 0.0004

Step 2725, Epoch 35/50, Batch 5/80
train_loss: 0.0003

Step 2730, Epoch 35/50, Batch 10/80
train_loss: 0.0004

Step 2735, Epoch 35/50, Batch 15/80
train_loss: 0.0004

Step 2740, Epoch 35/50, Batch 20/80
train_loss: 0.0004

Step 2745, Epoch 35/50, Batch 25/80
train_loss: 0.0003

Step 2750, Epoch 35/50, Batch 30/80
train_loss: 0.0003

Step 2755, Epoch 35/50, Batch 35/80
train_loss: 0.0003

Step 2760, Epoch 35/50, Batch 40/80
train_loss: 0.0003

Step 2765, Epoch 35/50, Batch 45/80
train_loss: 0.0003

Step 2770, Epoch 35/50, Batch 50/80
train_loss: 0.0003

Step 2775, Epoch 35/50, Batch 55/80
train_loss: 0.0003

Step 2780, Epoch 35/50, Batch 60/80
train_loss: 0.0004

Step 2785, Epoch 35/50, Batch 65/80
train_loss: 0.0003

Step 2790, Epoch 35/50, Batch 70/80
train_loss: 0.0003

Step 2795, Epoch 35/50, Batch 75/80
train_loss: 0.0004

Epoch 35/50 completed, Global Step: 2799
train_loss: 0.0004, val_loss: 0.0003

---------------------------------------------------------------------------


Step 2800, Epoch 36/50, Batch 0/80
train_loss: 0.0003

Step 2805, Epoch 36/50, Batch 5/80
train_loss: 0.0003

Step 2810, Epoch 36/50, Batch 10/80
train_loss: 0.0003

Step 2815, Epoch 36/50, Batch 15/80
train_loss: 0.0003

Step 2820, Epoch 36/50, Batch 20/80
train_loss: 0.0003

Step 2825, Epoch 36/50, Batch 25/80
train_loss: 0.0003

Step 2830, Epoch 36/50, Batch 30/80
train_loss: 0.0005

Step 2835, Epoch 36/50, Batch 35/80
train_loss: 0.0003

Step 2840, Epoch 36/50, Batch 40/80
train_loss: 0.0003

Step 2845, Epoch 36/50, Batch 45/80
train_loss: 0.0004

Step 2850, Epoch 36/50, Batch 50/80
train_loss: 0.0004

Step 2855, Epoch 36/50, Batch 55/80
train_loss: 0.0003

Step 2860, Epoch 36/50, Batch 60/80
train_loss: 0.0003

Step 2865, Epoch 36/50, Batch 65/80
train_loss: 0.0002

Step 2870, Epoch 36/50, Batch 70/80
train_loss: 0.0002

Step 2875, Epoch 36/50, Batch 75/80
train_loss: 0.0003

Epoch 36/50 completed, Global Step: 2879
train_loss: 0.0003, val_loss: 0.0004

---------------------------------------------------------------------------


Step 2880, Epoch 37/50, Batch 0/80
train_loss: 0.0004

Step 2885, Epoch 37/50, Batch 5/80
train_loss: 0.0003

Step 2890, Epoch 37/50, Batch 10/80
train_loss: 0.0003

Step 2895, Epoch 37/50, Batch 15/80
train_loss: 0.0003

Step 2900, Epoch 37/50, Batch 20/80
train_loss: 0.0003

Step 2905, Epoch 37/50, Batch 25/80
train_loss: 0.0003

Step 2910, Epoch 37/50, Batch 30/80
train_loss: 0.0005

Step 2915, Epoch 37/50, Batch 35/80
train_loss: 0.0005

Step 2920, Epoch 37/50, Batch 40/80
train_loss: 0.0004

Step 2925, Epoch 37/50, Batch 45/80
train_loss: 0.0004

Step 2930, Epoch 37/50, Batch 50/80
train_loss: 0.0004

Step 2935, Epoch 37/50, Batch 55/80
train_loss: 0.0005

Step 2940, Epoch 37/50, Batch 60/80
train_loss: 0.0003

Step 2945, Epoch 37/50, Batch 65/80
train_loss: 0.0003

Step 2950, Epoch 37/50, Batch 70/80
train_loss: 0.0003

Step 2955, Epoch 37/50, Batch 75/80
train_loss: 0.0003

Epoch 37/50 completed, Global Step: 2959
train_loss: 0.0003, val_loss: 0.0003

---------------------------------------------------------------------------


Step 2960, Epoch 38/50, Batch 0/80
train_loss: 0.0002

Step 2965, Epoch 38/50, Batch 5/80
train_loss: 0.0002

Step 2970, Epoch 38/50, Batch 10/80
train_loss: 0.0003

Step 2975, Epoch 38/50, Batch 15/80
train_loss: 0.0003

Step 2980, Epoch 38/50, Batch 20/80
train_loss: 0.0003

Step 2985, Epoch 38/50, Batch 25/80
train_loss: 0.0003

Step 2990, Epoch 38/50, Batch 30/80
train_loss: 0.0004

Step 2995, Epoch 38/50, Batch 35/80
train_loss: 0.0003

Step 3000, Epoch 38/50, Batch 40/80
train_loss: 0.0003

Step 3005, Epoch 38/50, Batch 45/80
train_loss: 0.0004

Step 3010, Epoch 38/50, Batch 50/80
train_loss: 0.0004

Step 3015, Epoch 38/50, Batch 55/80
train_loss: 0.0003

Step 3020, Epoch 38/50, Batch 60/80
train_loss: 0.0003

Step 3025, Epoch 38/50, Batch 65/80
train_loss: 0.0003

Step 3030, Epoch 38/50, Batch 70/80
train_loss: 0.0003

Step 3035, Epoch 38/50, Batch 75/80
train_loss: 0.0002

Epoch 38/50 completed, Global Step: 3039
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3040, Epoch 39/50, Batch 0/80
train_loss: 0.0002

Step 3045, Epoch 39/50, Batch 5/80
train_loss: 0.0002

Step 3050, Epoch 39/50, Batch 10/80
train_loss: 0.0002

Step 3055, Epoch 39/50, Batch 15/80
train_loss: 0.0003

Step 3060, Epoch 39/50, Batch 20/80
train_loss: 0.0004

Step 3065, Epoch 39/50, Batch 25/80
train_loss: 0.0003

Step 3070, Epoch 39/50, Batch 30/80
train_loss: 0.0005

Step 3075, Epoch 39/50, Batch 35/80
train_loss: 0.0004

Step 3080, Epoch 39/50, Batch 40/80
train_loss: 0.0003

Step 3085, Epoch 39/50, Batch 45/80
train_loss: 0.0005

Step 3090, Epoch 39/50, Batch 50/80
train_loss: 0.0003

Step 3095, Epoch 39/50, Batch 55/80
train_loss: 0.0003

Step 3100, Epoch 39/50, Batch 60/80
train_loss: 0.0003

Step 3105, Epoch 39/50, Batch 65/80
train_loss: 0.0002

Step 3110, Epoch 39/50, Batch 70/80
train_loss: 0.0004

Step 3115, Epoch 39/50, Batch 75/80
train_loss: 0.0004

Epoch 39/50 completed, Global Step: 3119
train_loss: 0.0004, val_loss: 0.0003

---------------------------------------------------------------------------


Step 3120, Epoch 40/50, Batch 0/80
train_loss: 0.0003

Step 3125, Epoch 40/50, Batch 5/80
train_loss: 0.0002

Step 3130, Epoch 40/50, Batch 10/80
train_loss: 0.0003

Step 3135, Epoch 40/50, Batch 15/80
train_loss: 0.0003

Step 3140, Epoch 40/50, Batch 20/80
train_loss: 0.0002

Step 3145, Epoch 40/50, Batch 25/80
train_loss: 0.0002

Step 3150, Epoch 40/50, Batch 30/80
train_loss: 0.0002

Step 3155, Epoch 40/50, Batch 35/80
train_loss: 0.0002

Step 3160, Epoch 40/50, Batch 40/80
train_loss: 0.0006

Step 3165, Epoch 40/50, Batch 45/80
train_loss: 0.0007

Step 3170, Epoch 40/50, Batch 50/80
train_loss: 0.0005

Step 3175, Epoch 40/50, Batch 55/80
train_loss: 0.0003

Step 3180, Epoch 40/50, Batch 60/80
train_loss: 0.0003

Step 3185, Epoch 40/50, Batch 65/80
train_loss: 0.0002

Step 3190, Epoch 40/50, Batch 70/80
train_loss: 0.0002

Step 3195, Epoch 40/50, Batch 75/80
train_loss: 0.0002

Epoch 40/50 completed, Global Step: 3199
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3200, Epoch 41/50, Batch 0/80
train_loss: 0.0002

Step 3205, Epoch 41/50, Batch 5/80
train_loss: 0.0002

Step 3210, Epoch 41/50, Batch 10/80
train_loss: 0.0002

Step 3215, Epoch 41/50, Batch 15/80
train_loss: 0.0003

Step 3220, Epoch 41/50, Batch 20/80
train_loss: 0.0003

Step 3225, Epoch 41/50, Batch 25/80
train_loss: 0.0004

Step 3230, Epoch 41/50, Batch 30/80
train_loss: 0.0007

Step 3235, Epoch 41/50, Batch 35/80
train_loss: 0.0005

Step 3240, Epoch 41/50, Batch 40/80
train_loss: 0.0004

Step 3245, Epoch 41/50, Batch 45/80
train_loss: 0.0004

Step 3250, Epoch 41/50, Batch 50/80
train_loss: 0.0003

Step 3255, Epoch 41/50, Batch 55/80
train_loss: 0.0004

Step 3260, Epoch 41/50, Batch 60/80
train_loss: 0.0003

Step 3265, Epoch 41/50, Batch 65/80
train_loss: 0.0002

Step 3270, Epoch 41/50, Batch 70/80
train_loss: 0.0002

Step 3275, Epoch 41/50, Batch 75/80
train_loss: 0.0002

Epoch 41/50 completed, Global Step: 3279
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3280, Epoch 42/50, Batch 0/80
train_loss: 0.0002

Step 3285, Epoch 42/50, Batch 5/80
train_loss: 0.0002

Step 3290, Epoch 42/50, Batch 10/80
train_loss: 0.0003

Step 3295, Epoch 42/50, Batch 15/80
train_loss: 0.0002

Step 3300, Epoch 42/50, Batch 20/80
train_loss: 0.0002

Step 3305, Epoch 42/50, Batch 25/80
train_loss: 0.0002

Step 3310, Epoch 42/50, Batch 30/80
train_loss: 0.0003

Step 3315, Epoch 42/50, Batch 35/80
train_loss: 0.0004

Step 3320, Epoch 42/50, Batch 40/80
train_loss: 0.0005

Step 3325, Epoch 42/50, Batch 45/80
train_loss: 0.0005

Step 3330, Epoch 42/50, Batch 50/80
train_loss: 0.0004

Step 3335, Epoch 42/50, Batch 55/80
train_loss: 0.0004

Step 3340, Epoch 42/50, Batch 60/80
train_loss: 0.0003

Step 3345, Epoch 42/50, Batch 65/80
train_loss: 0.0002

Step 3350, Epoch 42/50, Batch 70/80
train_loss: 0.0002

Step 3355, Epoch 42/50, Batch 75/80
train_loss: 0.0002

Epoch 42/50 completed, Global Step: 3359
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3360, Epoch 43/50, Batch 0/80
train_loss: 0.0002

Step 3365, Epoch 43/50, Batch 5/80
train_loss: 0.0002

Step 3370, Epoch 43/50, Batch 10/80
train_loss: 0.0002

Step 3375, Epoch 43/50, Batch 15/80
train_loss: 0.0004

Step 3380, Epoch 43/50, Batch 20/80
train_loss: 0.0002

Step 3385, Epoch 43/50, Batch 25/80
train_loss: 0.0002

Step 3390, Epoch 43/50, Batch 30/80
train_loss: 0.0003

Step 3395, Epoch 43/50, Batch 35/80
train_loss: 0.0004

Step 3400, Epoch 43/50, Batch 40/80
train_loss: 0.0002

Step 3405, Epoch 43/50, Batch 45/80
train_loss: 0.0002

Step 3410, Epoch 43/50, Batch 50/80
train_loss: 0.0002

Step 3415, Epoch 43/50, Batch 55/80
train_loss: 0.0002

Step 3420, Epoch 43/50, Batch 60/80
train_loss: 0.0002

Step 3425, Epoch 43/50, Batch 65/80
train_loss: 0.0002

Step 3430, Epoch 43/50, Batch 70/80
train_loss: 0.0003

Step 3435, Epoch 43/50, Batch 75/80
train_loss: 0.0002

Epoch 43/50 completed, Global Step: 3439
train_loss: 0.0002, val_loss: 0.0003

---------------------------------------------------------------------------


Step 3440, Epoch 44/50, Batch 0/80
train_loss: 0.0003

Step 3445, Epoch 44/50, Batch 5/80
train_loss: 0.0004

Step 3450, Epoch 44/50, Batch 10/80
train_loss: 0.0002

Step 3455, Epoch 44/50, Batch 15/80
train_loss: 0.0002

Step 3460, Epoch 44/50, Batch 20/80
train_loss: 0.0002

Step 3465, Epoch 44/50, Batch 25/80
train_loss: 0.0002

Step 3470, Epoch 44/50, Batch 30/80
train_loss: 0.0002

Step 3475, Epoch 44/50, Batch 35/80
train_loss: 0.0003

Step 3480, Epoch 44/50, Batch 40/80
train_loss: 0.0003

Step 3485, Epoch 44/50, Batch 45/80
train_loss: 0.0003

Step 3490, Epoch 44/50, Batch 50/80
train_loss: 0.0004

Step 3495, Epoch 44/50, Batch 55/80
train_loss: 0.0004

Step 3500, Epoch 44/50, Batch 60/80
train_loss: 0.0003

Step 3505, Epoch 44/50, Batch 65/80
train_loss: 0.0002

Step 3510, Epoch 44/50, Batch 70/80
train_loss: 0.0002

Step 3515, Epoch 44/50, Batch 75/80
train_loss: 0.0002

Epoch 44/50 completed, Global Step: 3519
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3520, Epoch 45/50, Batch 0/80
train_loss: 0.0002

Step 3525, Epoch 45/50, Batch 5/80
train_loss: 0.0002

Step 3530, Epoch 45/50, Batch 10/80
train_loss: 0.0002

Step 3535, Epoch 45/50, Batch 15/80
train_loss: 0.0002

Step 3540, Epoch 45/50, Batch 20/80
train_loss: 0.0002

Step 3545, Epoch 45/50, Batch 25/80
train_loss: 0.0008

Step 3550, Epoch 45/50, Batch 30/80
train_loss: 0.0003

Step 3555, Epoch 45/50, Batch 35/80
train_loss: 0.0004

Step 3560, Epoch 45/50, Batch 40/80
train_loss: 0.0004

Step 3565, Epoch 45/50, Batch 45/80
train_loss: 0.0002

Step 3570, Epoch 45/50, Batch 50/80
train_loss: 0.0003

Step 3575, Epoch 45/50, Batch 55/80
train_loss: 0.0002

Step 3580, Epoch 45/50, Batch 60/80
train_loss: 0.0002

Step 3585, Epoch 45/50, Batch 65/80
train_loss: 0.0002

Step 3590, Epoch 45/50, Batch 70/80
train_loss: 0.0002

Step 3595, Epoch 45/50, Batch 75/80
train_loss: 0.0002

Epoch 45/50 completed, Global Step: 3599
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3600, Epoch 46/50, Batch 0/80
train_loss: 0.0002

Step 3605, Epoch 46/50, Batch 5/80
train_loss: 0.0002

Step 3610, Epoch 46/50, Batch 10/80
train_loss: 0.0002

Step 3615, Epoch 46/50, Batch 15/80
train_loss: 0.0002

Step 3620, Epoch 46/50, Batch 20/80
train_loss: 0.0005

Step 3625, Epoch 46/50, Batch 25/80
train_loss: 0.0002

Step 3630, Epoch 46/50, Batch 30/80
train_loss: 0.0002

Step 3635, Epoch 46/50, Batch 35/80
train_loss: 0.0003

Step 3640, Epoch 46/50, Batch 40/80
train_loss: 0.0002

Step 3645, Epoch 46/50, Batch 45/80
train_loss: 0.0002

Step 3650, Epoch 46/50, Batch 50/80
train_loss: 0.0003

Step 3655, Epoch 46/50, Batch 55/80
train_loss: 0.0002

Step 3660, Epoch 46/50, Batch 60/80
train_loss: 0.0002

Step 3665, Epoch 46/50, Batch 65/80
train_loss: 0.0002

Step 3670, Epoch 46/50, Batch 70/80
train_loss: 0.0002

Step 3675, Epoch 46/50, Batch 75/80
train_loss: 0.0003

Epoch 46/50 completed, Global Step: 3679
train_loss: 0.0003, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3680, Epoch 47/50, Batch 0/80
train_loss: 0.0002

Step 3685, Epoch 47/50, Batch 5/80
train_loss: 0.0005

Step 3690, Epoch 47/50, Batch 10/80
train_loss: 0.0002

Step 3695, Epoch 47/50, Batch 15/80
train_loss: 0.0003

Step 3700, Epoch 47/50, Batch 20/80
train_loss: 0.0002

Step 3705, Epoch 47/50, Batch 25/80
train_loss: 0.0002

Step 3710, Epoch 47/50, Batch 30/80
train_loss: 0.0002

Step 3715, Epoch 47/50, Batch 35/80
train_loss: 0.0002

Step 3720, Epoch 47/50, Batch 40/80
train_loss: 0.0002

Step 3725, Epoch 47/50, Batch 45/80
train_loss: 0.0002

Step 3730, Epoch 47/50, Batch 50/80
train_loss: 0.0002

Step 3735, Epoch 47/50, Batch 55/80
train_loss: 0.0002

Step 3740, Epoch 47/50, Batch 60/80
train_loss: 0.0002

Step 3745, Epoch 47/50, Batch 65/80
train_loss: 0.0002

Step 3750, Epoch 47/50, Batch 70/80
train_loss: 0.0002

Step 3755, Epoch 47/50, Batch 75/80
train_loss: 0.0002

Epoch 47/50 completed, Global Step: 3759
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3760, Epoch 48/50, Batch 0/80
train_loss: 0.0002

Step 3765, Epoch 48/50, Batch 5/80
train_loss: 0.0003

Step 3770, Epoch 48/50, Batch 10/80
train_loss: 0.0009

Step 3775, Epoch 48/50, Batch 15/80
train_loss: 0.0006

Step 3780, Epoch 48/50, Batch 20/80
train_loss: 0.0003

Step 3785, Epoch 48/50, Batch 25/80
train_loss: 0.0003

Step 3790, Epoch 48/50, Batch 30/80
train_loss: 0.0002

Step 3795, Epoch 48/50, Batch 35/80
train_loss: 0.0002

Step 3800, Epoch 48/50, Batch 40/80
train_loss: 0.0002

Step 3805, Epoch 48/50, Batch 45/80
train_loss: 0.0002

Step 3810, Epoch 48/50, Batch 50/80
train_loss: 0.0002

Step 3815, Epoch 48/50, Batch 55/80
train_loss: 0.0002

Step 3820, Epoch 48/50, Batch 60/80
train_loss: 0.0002

Step 3825, Epoch 48/50, Batch 65/80
train_loss: 0.0002

Step 3830, Epoch 48/50, Batch 70/80
train_loss: 0.0002

Step 3835, Epoch 48/50, Batch 75/80
train_loss: 0.0002

Epoch 48/50 completed, Global Step: 3839
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Step 3840, Epoch 49/50, Batch 0/80
train_loss: 0.0002

Step 3845, Epoch 49/50, Batch 5/80
train_loss: 0.0003

Step 3850, Epoch 49/50, Batch 10/80
train_loss: 0.0004

Step 3855, Epoch 49/50, Batch 15/80
train_loss: 0.0002

Step 3860, Epoch 49/50, Batch 20/80
train_loss: 0.0005

Step 3865, Epoch 49/50, Batch 25/80
train_loss: 0.0003

Step 3870, Epoch 49/50, Batch 30/80
train_loss: 0.0005

Step 3875, Epoch 49/50, Batch 35/80
train_loss: 0.0002

Step 3880, Epoch 49/50, Batch 40/80
train_loss: 0.0002

Step 3885, Epoch 49/50, Batch 45/80
train_loss: 0.0002

Step 3890, Epoch 49/50, Batch 50/80
train_loss: 0.0003

Step 3895, Epoch 49/50, Batch 55/80
train_loss: 0.0003

Step 3900, Epoch 49/50, Batch 60/80
train_loss: 0.0002

Step 3905, Epoch 49/50, Batch 65/80
train_loss: 0.0002

Step 3910, Epoch 49/50, Batch 70/80
train_loss: 0.0002

Step 3915, Epoch 49/50, Batch 75/80
train_loss: 0.0005

Epoch 49/50 completed, Global Step: 3919
train_loss: 0.0005, val_loss: 0.0003

---------------------------------------------------------------------------


Step 3920, Epoch 50/50, Batch 0/80
train_loss: 0.0003

Step 3925, Epoch 50/50, Batch 5/80
train_loss: 0.0003

Step 3930, Epoch 50/50, Batch 10/80
train_loss: 0.0002

Step 3935, Epoch 50/50, Batch 15/80
train_loss: 0.0002

Step 3940, Epoch 50/50, Batch 20/80
train_loss: 0.0002

Step 3945, Epoch 50/50, Batch 25/80
train_loss: 0.0002

Step 3950, Epoch 50/50, Batch 30/80
train_loss: 0.0002

Step 3955, Epoch 50/50, Batch 35/80
train_loss: 0.0002

Step 3960, Epoch 50/50, Batch 40/80
train_loss: 0.0002

Step 3965, Epoch 50/50, Batch 45/80
train_loss: 0.0002

Step 3970, Epoch 50/50, Batch 50/80
train_loss: 0.0005

Step 3975, Epoch 50/50, Batch 55/80
train_loss: 0.0002

Step 3980, Epoch 50/50, Batch 60/80
train_loss: 0.0002

Step 3985, Epoch 50/50, Batch 65/80
train_loss: 0.0002

Step 3990, Epoch 50/50, Batch 70/80
train_loss: 0.0002

Step 3995, Epoch 50/50, Batch 75/80
train_loss: 0.0002

Epoch 50/50 completed, Global Step: 3999
train_loss: 0.0002, val_loss: 0.0002

---------------------------------------------------------------------------


Training completed in 1330.46 seconds or 22.17 minutes or 0.3695720822943581 hours.
Total training steps: 3999

Training completed for model '[m004_(apv+G)]-gru_dec_1.2'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m004_(apv+G)]-gru_dec_1.2...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.2509' for [m004_(apv+G)]-gru_dec_1.2...

Decoder output plot for rep '1001.2509' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.4309' for [m004_(apv+G)]-gru_dec_1.2...

Decoder output plot for rep '1001.4309' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2


---------------------------------------------------------------------------

TESTING TRAINED DECODER MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2\checkpoints:

['best-model-epoch=45-val_loss=0.0002.ckpt']

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2\test

Edge matrix is created from relation matrices and set to decoder.

Relation matrices are overridden to make fully connected graph in decoder.

Trained Decoder Model Loaded for testing.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                                                                                                 | 0/? [00:00<?, ?it/s]
Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                                                                                            | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|                                                                               | 0/10 [00:00<?, ?it/s]
Found rep_num = 1001.0001 in batch 0 of epoch 45. Decoder output plot will be made for this data.
Testing DataLoader 0:  10%|#######1                                                               | 1/10 [00:00<00:01,  4.60it/s]Testing DataLoader 0:  20%|##############2                                                        | 2/10 [00:00<00:01,  6.19it/s]Testing DataLoader 0:  30%|#####################3                                                 | 3/10 [00:00<00:01,  6.93it/s]Testing DataLoader 0:  40%|############################4                                          | 4/10 [00:00<00:00,  7.51it/s]Testing DataLoader 0:  50%|###################################5                                   | 5/10 [00:00<00:00,  7.84it/s]Testing DataLoader 0:  60%|##########################################6                            | 6/10 [00:00<00:00,  8.09it/s]Testing DataLoader 0:  70%|#################################################6                     | 7/10 [00:00<00:00,  8.08it/s]Testing DataLoader 0:  80%|########################################################8              | 8/10 [00:00<00:00,  8.20it/s]Testing DataLoader 0:  90%|###############################################################9       | 9/10 [00:01<00:00,  8.31it/s]Testing DataLoader 0: 100%|######################################################################| 10/10 [00:01<00:00,  8.37it/s]
Testing completed in 1.20 seconds or 0.02 minutes or 0.0003322425153520372 hours.

test_loss: 0.0003

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.0001' for [m004_(apv+G)]-gru_dec_1.2...

Decoder output plot for rep '1001.0001' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\decoder\train\etypes=1\m004\apv\set_G\D=gru\tswp_0\[m004_(apv+G)]-gru_dec_1.2\test

Testing DataLoader 0: 100%|######################################################################| 10/10 [00:03<00:00,  2.83it/s]

        Test metric               DataLoader 0        

         test_loss            0.0002519407426007092   


===========================================================================

Decoder model '[m004_(apv+G)]-gru_dec_1.2' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-21 17:19:25
