=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3
Start Time: 2025-09-16 12:04:06
End Time: 2025-09-16 12:05:50

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting nri model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]

Node group name: m004
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 500/500 [OK=500, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (4, 4)
     n1   n2   n3   n4
n1  0.0  1.0  1.0  1.0
n2  1.0  0.0  1.0  1.0
n3  1.0  1.0  0.0  1.0
n4  1.0  1.0  1.0  0.0


**Receiver relation matrix** => shape: (12, 4)
      n1   n2   n3   n4
e12  0.0  1.0  0.0  0.0
e13  0.0  0.0  1.0  0.0
e14  0.0  0.0  0.0  1.0
e21  1.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0
e24  0.0  0.0  0.0  1.0
e31  1.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0
e34  0.0  0.0  0.0  1.0
e41  1.0  0.0  0.0  0.0
e42  0.0  1.0  0.0  0.0
e43  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (12, 4)
      n1   n2   n3   n4
e12  1.0  0.0  0.0  0.0
e13  1.0  0.0  0.0  0.0
e14  1.0  0.0  0.0  0.0
e21  0.0  1.0  0.0  0.0
e23  0.0  1.0  0.0  0.0
e24  0.0  1.0  0.0  0.0
e31  0.0  0.0  1.0  0.0
e32  0.0  0.0  1.0  0.0
e34  0.0  0.0  1.0  0.0
e41  0.0  0.0  0.0  1.0
e42  0.0  0.0  0.0  1.0
e43  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< ENCODER PARAMETERS >>>>>>
Encoder model parameters:
-------------------------
n_edge_types: 2
is_residual_connection: False
do_prob: {'mlp': 0.0, 'cnn': 0.0}
is_batch_norm: {'mlp': True, 'cnn': False}
is_xavier_weights: True
attention_output_size: 5
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
pipeline: [['1/node_emd.1', 'mlp'], ['1/pairwise_op', 'concat'], ['1/edge_emd.1.@', 'mlp'], ['2/aggregate', 'mean'], ['2/node_emd.1', 'mlp'], ['2/pairwise_op', 'concat'], ['2/edge_emd.1', 'mlp']]
edge_emb_configs: {'mlp': [[256, 'elu'], [256, 'elu']], 'cnn': [[5, 2, 64], [8]]}
node_emb_configs: {'mlp': [[256, 'elu'], [256, 'elu']], 'cnn': [[5, 2, 64], [8]]}
n_comps: 100
n_dims: 3

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 2
msg_out_size: 64
edge_mlp_config: [[64, 'tanh'], [64, 'tanh']]
out_mlp_config: [[64, 'relu'], [64, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
feat_norm: None
reduc_config: None
n_dims: 3

Decoder run parameters:
-------------------------
is_hard: False
skip_first_edge_type: True
pred_steps: 10
is_burn_in: True
final_pred_steps: 20
is_dynamic_graph: False
show_conf_band: False

Training parameters set to: 
lr_enc=0.0001, 
lr_dec=0.0001, 
final_beta=0.0001, 
warmup_frac=0.6, 
optimizer=adam, 
loss_type_encoder=kld, 
loss_type_decoder=mse, 
prior=tensor([0.5000, 0.5000]), 
add_const_kld=True

---------------------------------------------------------------------------

NRI Model Initialized with the following configurations:
----- NRI Model Summary -----
-- Encoder Summary
Encoder(
  (emb_fn_dict): ModuleDict(
    (1/node_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=300, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (1/edge_emd1@): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (2/node_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (2/edge_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=256, out_features=256, bias=True)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
  )
  (attention_layer_dict): ModuleDict()
  (output_layer): Linear(in_features=256, out_features=2, bias=True)
)
-- Decoder Summary
Decoder(
  (edge_mlp_fn): ModuleList(
    (0-1): 2 x MLP(
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=64, bias=True)
    (hidden_u): Linear(in_features=64, out_features=64, bias=True)
    (input_r): Linear(in_features=3, out_features=64, bias=True)
    (hidden_r): Linear(in_features=64, out_features=64, bias=True)
    (input_h): Linear(in_features=3, out_features=64, bias=True)
    (hidden_h): Linear(in_features=64, out_features=64, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=64, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=64, out_features=3, bias=True)
)

---------------------------------------------------------------------------

'[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3' already exists in the log path 'C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3'.
(a) Overwrite exsiting version, (b) create new version, (c) stop training (Choose 'a', 'b' or 'c'):  Are you sure you want to remove the '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3? (y/n): Overwrote '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3.
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for encoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Step 0, Epoch 1/2, Batch 0/80
temp: 0.9990, beta: 0.0000, 
nri_train_loss: 0.7886, enc_train_loss: 2.1875, enc_train_entropy: 0.5109, dec_train_loss: 0.7886, train_edge_accuracy: 0.4900

Step 5, Epoch 1/2, Batch 5/80
temp: 0.9940, beta: 0.0000, 
nri_train_loss: 0.6350, enc_train_loss: 4.9323, enc_train_entropy: 0.2821, dec_train_loss: 0.6350, train_edge_accuracy: 0.4833

Step 10, Epoch 1/2, Batch 10/80
temp: 0.9891, beta: 0.0000, 
nri_train_loss: 0.5036, enc_train_loss: 5.4931, enc_train_entropy: 0.2354, dec_train_loss: 0.5035, train_edge_accuracy: 0.4850

Step 15, Epoch 1/2, Batch 15/80
temp: 0.9841, beta: 0.0000, 
nri_train_loss: 0.3917, enc_train_loss: 5.1888, enc_train_entropy: 0.2607, dec_train_loss: 0.3916, train_edge_accuracy: 0.5017

Step 20, Epoch 1/2, Batch 20/80
temp: 0.9792, beta: 0.0000, 
nri_train_loss: 0.3001, enc_train_loss: 5.2518, enc_train_entropy: 0.2555, dec_train_loss: 0.3000, train_edge_accuracy: 0.4867

Step 25, Epoch 1/2, Batch 25/80
temp: 0.9743, beta: 0.0000, 
nri_train_loss: 0.2267, enc_train_loss: 5.6674, enc_train_entropy: 0.2209, dec_train_loss: 0.2265, train_edge_accuracy: 0.4583

Step 30, Epoch 1/2, Batch 30/80
temp: 0.9695, beta: 0.0000, 
nri_train_loss: 0.1683, enc_train_loss: 5.5064, enc_train_entropy: 0.2343, dec_train_loss: 0.1681, train_edge_accuracy: 0.4833

Step 35, Epoch 1/2, Batch 35/80
temp: 0.9646, beta: 0.0000, 
nri_train_loss: 0.1250, enc_train_loss: 6.0520, enc_train_entropy: 0.1888, dec_train_loss: 0.1248, train_edge_accuracy: 0.4933

Step 40, Epoch 1/2, Batch 40/80
temp: 0.9598, beta: 0.0000, 
nri_train_loss: 0.0907, enc_train_loss: 6.4753, enc_train_entropy: 0.1535, dec_train_loss: 0.0905, train_edge_accuracy: 0.4717

Step 45, Epoch 1/2, Batch 45/80
temp: 0.9550, beta: 0.0000, 
nri_train_loss: 0.0662, enc_train_loss: 6.3817, enc_train_entropy: 0.1613, dec_train_loss: 0.0659, train_edge_accuracy: 0.4967

Step 50, Epoch 1/2, Batch 50/80
temp: 0.9503, beta: 0.0001, 
nri_train_loss: 0.0496, enc_train_loss: 6.7486, enc_train_entropy: 0.1308, dec_train_loss: 0.0492, train_edge_accuracy: 0.4783

Step 55, Epoch 1/2, Batch 55/80
temp: 0.9455, beta: 0.0001, 
nri_train_loss: 0.0383, enc_train_loss: 6.4780, enc_train_entropy: 0.1533, dec_train_loss: 0.0379, train_edge_accuracy: 0.4617

Step 60, Epoch 1/2, Batch 60/80
temp: 0.9408, beta: 0.0001, 
nri_train_loss: 0.0328, enc_train_loss: 6.2797, enc_train_entropy: 0.1698, dec_train_loss: 0.0324, train_edge_accuracy: 0.4667

Step 65, Epoch 1/2, Batch 65/80
temp: 0.9361, beta: 0.0001, 
nri_train_loss: 0.0275, enc_train_loss: 5.4793, enc_train_entropy: 0.2365, dec_train_loss: 0.0272, train_edge_accuracy: 0.4867

Step 70, Epoch 1/2, Batch 70/80
temp: 0.9314, beta: 0.0001, 
nri_train_loss: 0.0253, enc_train_loss: 5.6003, enc_train_entropy: 0.2265, dec_train_loss: 0.0249, train_edge_accuracy: 0.4750

Step 75, Epoch 1/2, Batch 75/80
temp: 0.9268, beta: 0.0001, 
nri_train_loss: 0.0237, enc_train_loss: 5.5302, enc_train_entropy: 0.2323, dec_train_loss: 0.0233, train_edge_accuracy: 0.4767


Epoch 1/2 completed, Global Step: 80
nri_train_loss: 0.0237, enc_train_loss: 5.5302, enc_train_entropy: 0.2323, dec_train_loss: 0.0233, train_edge_accuracy: 0.4767
nri_val_loss: 0.0237, enc_val_loss: 7.0085, enc_val_entropy: 0.1091, dec_val_loss: 0.0231, val_edge_accuracy: 0.5050

---------------------------------------------------------------------------

Step 80, Epoch 2/2, Batch 0/80
temp: 0.9130, beta: 0.0001, 
nri_train_loss: 0.0231, enc_train_loss: 5.3594, enc_train_entropy: 0.2465, dec_train_loss: 0.0226, train_edge_accuracy: 0.4767

Step 85, Epoch 2/2, Batch 5/80
temp: 0.9084, beta: 0.0001, 
nri_train_loss: 0.0226, enc_train_loss: 5.3344, enc_train_entropy: 0.2486, dec_train_loss: 0.0221, train_edge_accuracy: 0.4983

Step 90, Epoch 2/2, Batch 10/80
temp: 0.9039, beta: 0.0001, 
nri_train_loss: 0.0222, enc_train_loss: 5.3010, enc_train_entropy: 0.2514, dec_train_loss: 0.0217, train_edge_accuracy: 0.4817

Step 95, Epoch 2/2, Batch 15/80
temp: 0.8994, beta: 0.0001, 
nri_train_loss: 0.0218, enc_train_loss: 5.4897, enc_train_entropy: 0.2357, dec_train_loss: 0.0213, train_edge_accuracy: 0.4850

Step 100, Epoch 2/2, Batch 20/80
temp: 0.8949, beta: 0.0001, 
nri_train_loss: 0.0215, enc_train_loss: 4.7806, enc_train_entropy: 0.2948, dec_train_loss: 0.0211, train_edge_accuracy: 0.4850

Step 105, Epoch 2/2, Batch 25/80
temp: 0.8904, beta: 0.0001, 
nri_train_loss: 0.0210, enc_train_loss: 4.7728, enc_train_entropy: 0.2954, dec_train_loss: 0.0205, train_edge_accuracy: 0.5133

Step 110, Epoch 2/2, Batch 30/80
temp: 0.8860, beta: 0.0001, 
nri_train_loss: 0.0210, enc_train_loss: 4.6886, enc_train_entropy: 0.3024, dec_train_loss: 0.0205, train_edge_accuracy: 0.4917

Step 115, Epoch 2/2, Batch 35/80
temp: 0.8816, beta: 0.0001, 
nri_train_loss: 0.0208, enc_train_loss: 4.5475, enc_train_entropy: 0.3142, dec_train_loss: 0.0203, train_edge_accuracy: 0.4983

Step 120, Epoch 2/2, Batch 40/80
temp: 0.8772, beta: 0.0001, 
nri_train_loss: 0.0210, enc_train_loss: 4.3553, enc_train_entropy: 0.3302, dec_train_loss: 0.0206, train_edge_accuracy: 0.5117

Step 125, Epoch 2/2, Batch 45/80
temp: 0.8728, beta: 0.0001, 
nri_train_loss: 0.0207, enc_train_loss: 4.0892, enc_train_entropy: 0.3524, dec_train_loss: 0.0203, train_edge_accuracy: 0.4817

Step 130, Epoch 2/2, Batch 50/80
temp: 0.8684, beta: 0.0001, 
nri_train_loss: 0.0205, enc_train_loss: 3.7372, enc_train_entropy: 0.3817, dec_train_loss: 0.0202, train_edge_accuracy: 0.4783

Step 135, Epoch 2/2, Batch 55/80
temp: 0.8641, beta: 0.0001, 
nri_train_loss: 0.0203, enc_train_loss: 3.4096, enc_train_entropy: 0.4090, dec_train_loss: 0.0200, train_edge_accuracy: 0.5083

Step 140, Epoch 2/2, Batch 60/80
temp: 0.8598, beta: 0.0001, 
nri_train_loss: 0.0204, enc_train_loss: 3.4373, enc_train_entropy: 0.4067, dec_train_loss: 0.0201, train_edge_accuracy: 0.5050

Step 145, Epoch 2/2, Batch 65/80
temp: 0.8555, beta: 0.0001, 
nri_train_loss: 0.0203, enc_train_loss: 3.3428, enc_train_entropy: 0.4146, dec_train_loss: 0.0199, train_edge_accuracy: 0.4983

Step 150, Epoch 2/2, Batch 70/80
temp: 0.8512, beta: 0.0001, 
nri_train_loss: 0.0201, enc_train_loss: 3.0547, enc_train_entropy: 0.4386, dec_train_loss: 0.0198, train_edge_accuracy: 0.4783

Step 155, Epoch 2/2, Batch 75/80
temp: 0.8470, beta: 0.0001, 
nri_train_loss: 0.0202, enc_train_loss: 3.3454, enc_train_entropy: 0.4144, dec_train_loss: 0.0199, train_edge_accuracy: 0.4683


Epoch 2/2 completed, Global Step: 160
nri_train_loss: 0.0202, enc_train_loss: 3.3454, enc_train_entropy: 0.4144, dec_train_loss: 0.0199, train_edge_accuracy: 0.4683
nri_val_loss: 0.0200, enc_val_loss: 3.8178, enc_val_entropy: 0.3750, dec_val_loss: 0.0197, val_edge_accuracy: 0.4840

---------------------------------------------------------------------------


Training completed in 71.06 seconds or 1.18 minutes or 0.01973958909511566 hours.
Total training steps: 160

Training completed for model '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3


<<<<<<<<<<<< ENCODER EDGE ACCURACY PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating encoder edge accuracy plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3...

Encoder edge accuracy (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.392' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3...

Decoder output plot for rep '1001.3923' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.172' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3...

Decoder output plot for rep '1001.1717' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3


---------------------------------------------------------------------------

TESTING TRAINED NRI MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3\checkpoints:

['best-model-epoch=01-val_loss=0.0000.ckpt']

Trained NRI Model Loaded for testing.

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3\test
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                                                              | 0/? [00:00<?, ?it/s]
Initializing input processors for encoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                                                         | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|                                            | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|###6                                | 1/10 [00:00<00:02,  4.34it/s]Testing DataLoader 0:  20%|#######2                            | 2/10 [00:00<00:01,  5.48it/s]Testing DataLoader 0:  30%|##########7                         | 3/10 [00:00<00:01,  6.22it/s]Testing DataLoader 0:  40%|##############4                     | 4/10 [00:00<00:00,  6.26it/s]Testing DataLoader 0:  50%|##################                  | 5/10 [00:00<00:00,  5.98it/s]Testing DataLoader 0:  60%|#####################5              | 6/10 [00:00<00:00,  6.22it/s]Testing DataLoader 0:  70%|#########################2          | 7/10 [00:01<00:00,  6.17it/s]Testing DataLoader 0:  80%|############################8       | 8/10 [00:01<00:00,  5.94it/s]Testing DataLoader 0:  90%|################################4   | 9/10 [00:01<00:00,  6.04it/s]Testing DataLoader 0: 100%|###################################| 10/10 [00:01<00:00,  6.09it/s]
Testing completed in 1.65 seconds or 0.03 minutes or 0.00045724623733096653 hours.

nri_test_loss: 3.8227, enc_test_loss: 3.8028, dec_test_loss: 0.0199, test_edge_accuracy: 0.4795

Edge predictions are as follows (showing probabilities for each edge type):

Rep 1,001.971:
[[0.01109636 0.9889037 ]
 [0.02559827 0.9744017 ]
 [0.00639104 0.99360895]
 [0.30875313 0.6912468 ]
 [0.41907305 0.5809269 ]
 [0.13934706 0.860653  ]
 [0.27188596 0.728114  ]
 [0.24263416 0.7573658 ]
 [0.17104025 0.8289597 ]
 [0.11779184 0.88220817]
 [0.08130334 0.91869664]
 [0.24005292 0.75994706]]

Rep 1,001.411:
[[0.8623328  0.13766718]
 [0.9238409  0.07615913]
 [0.15607862 0.84392136]
 [0.995275   0.00472499]
 [0.998319   0.00168101]
 [0.8934771  0.1065229 ]
 [0.9272253  0.07277476]
 [0.90923506 0.09076496]
 [0.32938805 0.6706119 ]
 [0.79263467 0.20736532]
 [0.8382541  0.16174598]
 [0.8688926  0.1311074 ]]

Rep 1,001.484:
[[5.5982540e-03 9.9440175e-01]
 [3.0070809e-03 9.9699295e-01]
 [3.1413743e-04 9.9968588e-01]
 [9.2119902e-02 9.0788013e-01]
 [2.0394960e-02 9.7960508e-01]
 [4.5601167e-03 9.9543989e-01]
 [3.5823676e-01 6.4176327e-01]
 [2.2233114e-01 7.7766883e-01]
 [3.1847432e-02 9.6815252e-01]
 [3.4242332e-01 6.5757662e-01]
 [1.8547773e-01 8.1452233e-01]
 [1.0091117e-01 8.9908880e-01]]

Rep 1,001.345:
[[0.09827435 0.9017257 ]
 [0.3508982  0.64910185]
 [0.03292892 0.9670711 ]
 [0.1308461  0.8691539 ]
 [0.33317208 0.6668279 ]
 [0.04104482 0.95895517]
 [0.04613201 0.953868  ]
 [0.0710298  0.92897016]
 [0.0116863  0.9883137 ]
 [0.11074971 0.88925034]
 [0.10939076 0.8906093 ]
 [0.47662053 0.5233795 ]]

Rep 1,001.326:
[[0.9464838  0.05351627]
 [0.9958431  0.00415691]
 [0.9015892  0.09841078]
 [0.8572717  0.14272831]
 [0.9984493  0.0015506 ]
 [0.94299275 0.05700727]
 [0.7913213  0.20867868]
 [0.95510095 0.04489906]
 [0.8454415  0.1545585 ]
 [0.8815912  0.11840879]
 [0.9732861  0.02671386]
 [0.9981109  0.00188912]]

Rep 1,001.411:
[[0.27793115 0.72206885]
 [0.42601436 0.57398564]
 [0.11445855 0.8855414 ]
 [0.01671134 0.9832887 ]
 [0.06888643 0.9311136 ]
 [0.01067323 0.98932683]
 [0.01648166 0.98351836]
 [0.04390689 0.95609313]
 [0.01078425 0.98921573]
 [0.16200036 0.83799964]
 [0.26532993 0.73467004]
 [0.42883566 0.57116437]]

Rep 1,001.395:
[[0.99205196 0.00794801]
 [0.9971209  0.00287901]
 [0.9043545  0.09564553]
 [0.9605491  0.03945083]
 [0.99351066 0.00648931]
 [0.80622965 0.1937704 ]
 [0.9096155  0.09038452]
 [0.960844   0.03915606]
 [0.76786983 0.23213017]
 [0.9333593  0.06664073]
 [0.97384363 0.02615638]
 [0.9926363  0.0073637 ]]

Rep 1,001.927:
[[0.13562818 0.8643718 ]
 [0.06644547 0.93355453]
 [0.0396707  0.9603293 ]
 [0.25302717 0.74697286]
 [0.08388324 0.9161167 ]
 [0.06154268 0.9384573 ]
 [0.4253605  0.5746395 ]
 [0.3058441  0.69415593]
 [0.13098991 0.8690101 ]
 [0.35330334 0.6466967 ]
 [0.20191117 0.79808885]
 [0.1012025  0.89879745]]

Rep 1,001.188:
[[0.98040295 0.019597  ]
 [0.99414295 0.00585702]
 [0.6876844  0.31231558]
 [0.9138598  0.08614025]
 [0.99715745 0.00284259]
 [0.7737363  0.22626375]
 [0.8736196  0.12638041]
 [0.98422295 0.01577701]
 [0.6929826  0.3070174 ]
 [0.9364611  0.06353887]
 [0.99047256 0.00952744]
 [0.9968072  0.00319282]]

Rep 1,001.273:
[[0.9455493  0.05445074]
 [0.80046886 0.19953114]
 [0.42748994 0.57251006]
 [0.9667087  0.03329125]
 [0.7980438  0.20195621]
 [0.46659413 0.5334059 ]
 [0.94209385 0.05790612]
 [0.8887007  0.11129931]
 [0.38381562 0.6161844 ]
 [0.9664197  0.03358027]
 [0.92202884 0.07797114]
 [0.7680358  0.23196419]]

Rep 1,001.688:
[[0.88109255 0.11890748]
 [0.651931   0.348069  ]
 [0.4719023  0.5280977 ]
 [0.08118367 0.91881627]
 [0.40377674 0.5962233 ]
 [0.12846026 0.8715397 ]
 [0.17470133 0.82529867]
 [0.8434859  0.15651411]
 [0.25661755 0.74338245]
 [0.18504609 0.814954  ]
 [0.80208856 0.19791143]
 [0.6324777  0.3675223 ]]

Rep 1,001.116:
[[0.86217946 0.13782053]
 [0.9250156  0.0749844 ]
 [0.54000014 0.45999992]
 [0.41231334 0.58768666]
 [0.9782507  0.02174938]
 [0.5894915  0.41050845]
 [0.48814666 0.51185334]
 [0.953106   0.04689401]
 [0.58213043 0.41786957]
 [0.36423096 0.63576907]
 [0.9152375  0.08476255]
 [0.9633636  0.03663643]]

Rep 1,001.379:
[[0.15310642 0.8468936 ]
 [0.11330382 0.8866962 ]
 [0.02912983 0.9708702 ]
 [0.4576585  0.5423415 ]
 [0.3477607  0.65223926]
 [0.11643278 0.88356715]
 [0.4309511  0.5690489 ]
 [0.3724848  0.6275152 ]
 [0.14527397 0.8547261 ]
 [0.45533076 0.5446693 ]
 [0.34119868 0.6588013 ]
 [0.33058172 0.66941833]]

Rep 1,001.364:
[[0.9921469  0.00785315]
 [0.9845821  0.01541785]
 [0.812377   0.18762304]
 [0.91808283 0.08191715]
 [0.97390544 0.0260945 ]
 [0.6333549  0.36664513]
 [0.95851874 0.04148131]
 [0.99075574 0.00924423]
 [0.8094951  0.19050491]
 [0.90388423 0.09611575]
 [0.9805659  0.01943405]
 [0.97504336 0.02495668]]

Rep 1,001.471:
[[0.58416265 0.41583738]
 [0.4861313  0.5138687 ]
 [0.20480864 0.79519135]
 [0.6885199  0.31148013]
 [0.5290363  0.4709637 ]
 [0.19529521 0.8047048 ]
 [0.7932451  0.2067549 ]
 [0.7542828  0.24571726]
 [0.39100122 0.60899884]
 [0.76261014 0.23738986]
 [0.68252367 0.31747627]
 [0.59051204 0.40948796]]

Rep 1,001.359:
[[0.9982982  0.00170175]
 [0.9939498  0.0060502 ]
 [0.92820674 0.0717933 ]
 [0.9919743  0.00802574]
 [0.95155174 0.04844821]
 [0.48902032 0.5109797 ]
 [0.9894163  0.01058366]
 [0.9864704  0.01352955]
 [0.49983802 0.50016195]
 [0.99472517 0.00527479]
 [0.9882754  0.01172458]
 [0.95964146 0.04035855]]

Rep 1,001.419:
[[0.10119171 0.89880824]
 [0.1892507  0.8107493 ]
 [0.05472394 0.945276  ]
 [0.07282746 0.9271725 ]
 [0.11902346 0.8809766 ]
 [0.02413553 0.97586447]
 [0.12961777 0.8703822 ]
 [0.11690491 0.88309515]
 [0.07018377 0.92981625]
 [0.1607093  0.8392907 ]
 [0.10618287 0.8938171 ]
 [0.25002235 0.74997765]]

Rep 1,001.724:
[[0.22208726 0.77791274]
 [0.3612922  0.6387078 ]
 [0.13511282 0.8648872 ]
 [0.00407612 0.9959239 ]
 [0.03666448 0.9633355 ]
 [0.00247016 0.9975298 ]
 [0.00296842 0.99703157]
 [0.02544159 0.9745584 ]
 [0.00200731 0.99799275]
 [0.0429442  0.95705575]
 [0.12675178 0.8732483 ]
 [0.2250145  0.77498555]]

Rep 1,001.294:
[[0.8830955  0.11690454]
 [0.9779468  0.02205318]
 [0.76822406 0.23177597]
 [0.6950862  0.3049138 ]
 [0.97242326 0.0275768 ]
 [0.67868346 0.3213166 ]
 [0.7531706  0.24682942]
 [0.8573964  0.14260353]
 [0.7067899  0.2932101 ]
 [0.770481   0.22951898]
 [0.8969627  0.10303725]
 [0.9860763  0.01392379]]

Rep 1,001.268:
[[1.1151094e-02 9.8884892e-01]
 [8.7762084e-03 9.9122381e-01]
 [9.9195517e-04 9.9900812e-01]
 [2.6023161e-01 7.3976839e-01]
 [8.6300008e-02 9.1369992e-01]
 [2.5727049e-02 9.7427297e-01]
 [3.0054942e-01 6.9945055e-01]
 [2.1174714e-01 7.8825289e-01]
 [4.7359053e-02 9.5264101e-01]
 [2.6257598e-01 7.3742402e-01]
 [8.6915545e-02 9.1308445e-01]
 [6.2875949e-02 9.3712407e-01]]

Rep 1,001.303:
[[0.15822014 0.8417798 ]
 [0.5591204  0.4408796 ]
 [0.2659616  0.7340384 ]
 [0.37015802 0.6298419 ]
 [0.7729222  0.22707786]
 [0.33921748 0.6607826 ]
 [0.29014242 0.7098576 ]
 [0.28782368 0.7121763 ]
 [0.21949235 0.7805076 ]
 [0.2770682  0.72293174]
 [0.26610157 0.7338984 ]
 [0.7069778  0.29302227]]

Rep 1,001.530:
[[0.30728987 0.6927101 ]
 [0.2516433  0.7483567 ]
 [0.04327589 0.95672417]
 [0.884846   0.11515399]
 [0.72550255 0.27449745]
 [0.24268301 0.757317  ]
 [0.4842074  0.5157926 ]
 [0.33553907 0.66446096]
 [0.03990489 0.9600951 ]
 [0.6010186  0.39898133]
 [0.4597486  0.54025143]
 [0.34138444 0.6586156 ]]

Rep 1,001.474:
[[0.9820817  0.01791826]
 [0.9238661  0.07613388]
 [0.45136434 0.5486357 ]
 [0.9709919  0.02900809]
 [0.89831007 0.10168991]
 [0.43235603 0.56764394]
 [0.951515   0.04848497]
 [0.9553246  0.04467544]
 [0.3711303  0.6288698 ]
 [0.98072433 0.0192757 ]
 [0.9798252  0.02017478]
 [0.91943944 0.08056057]]

Rep 1,001.133:
[[0.98682773 0.01317224]
 [0.88676566 0.1132344 ]
 [0.5453394  0.45466056]
 [0.96531814 0.03468183]
 [0.8217612  0.17823882]
 [0.4191591  0.5808409 ]
 [0.9653315  0.03466849]
 [0.9677     0.03230006]
 [0.38868    0.61132   ]
 [0.98417085 0.01582908]
 [0.9860508  0.01394927]
 [0.8889788  0.11102121]]

Rep 1,001.711:
[[0.05029242 0.9497075 ]
 [0.13514549 0.8648545 ]
 [0.04125372 0.95874625]
 [0.12750953 0.8724904 ]
 [0.14454307 0.85545695]
 [0.09956204 0.90043795]
 [0.19688787 0.8031121 ]
 [0.09962997 0.90037   ]
 [0.11317073 0.8868293 ]
 [0.05380258 0.94619745]
 [0.0374015  0.9625985 ]
 [0.07682534 0.92317474]]

Rep 1,001.166:
[[0.9657218  0.03427827]
 [0.97989696 0.02010298]
 [0.40667495 0.5933251 ]
 [0.9927723  0.00722775]
 [0.9925404  0.00745957]
 [0.6091644  0.39083564]
 [0.97713906 0.02286092]
 [0.9561016  0.04389841]
 [0.3860086  0.6139914 ]
 [0.99210113 0.00789891]
 [0.98234415 0.01765586]
 [0.98714894 0.01285104]]

Rep 1,001.354:
[[0.1279726  0.8720274 ]
 [0.07469252 0.92530745]
 [0.01851567 0.9814843 ]
 [0.6660664  0.33393356]
 [0.3519403  0.6480597 ]
 [0.13229723 0.8677027 ]
 [0.45786172 0.54213834]
 [0.2595447  0.7404553 ]
 [0.06467935 0.9353206 ]
 [0.60075885 0.39924115]
 [0.41162997 0.58837   ]
 [0.22269821 0.7773018 ]]

Rep 1,001.783:
[[0.82259226 0.17740779]
 [0.73572266 0.26427737]
 [0.51015586 0.48984417]
 [0.6520753  0.34792468]
 [0.6872176  0.31278244]
 [0.17716363 0.8228364 ]
 [0.8419778  0.15802222]
 [0.8491134  0.15088661]
 [0.43427247 0.56572753]
 [0.88517547 0.11482447]
 [0.91636753 0.08363247]
 [0.9155888  0.0844112 ]]

Rep 1,001.669:
[[0.33225098 0.667749  ]
 [0.26684242 0.7331575 ]
 [0.05143552 0.94856447]
 [0.97902465 0.02097537]
 [0.9115703  0.0884297 ]
 [0.4067604  0.59323955]
 [0.70424265 0.29575735]
 [0.4133071  0.5866929 ]
 [0.11020311 0.88979685]
 [0.73620915 0.26379082]
 [0.44298017 0.5570198 ]
 [0.36747912 0.63252085]]

Rep 1,001.268:
[[0.99506193 0.0049381 ]
 [0.97835106 0.02164889]
 [0.58608675 0.41391325]
 [0.9311749  0.06882513]
 [0.9623583  0.03764163]
 [0.5477471  0.45225292]
 [0.97873497 0.02126511]
 [0.9982339  0.00176605]
 [0.85123026 0.14876974]
 [0.9491134  0.0508866 ]
 [0.99038154 0.00961841]
 [0.9591144  0.04088559]]

Rep 1,001.432:
[[0.7501255  0.2498745 ]
 [0.83801794 0.16198204]
 [0.5166668  0.4833332 ]
 [0.7825211  0.21747884]
 [0.89652026 0.10347976]
 [0.52924776 0.4707522 ]
 [0.61659414 0.3834058 ]
 [0.65956247 0.34043753]
 [0.41010943 0.58989054]
 [0.72993934 0.27006066]
 [0.85934097 0.14065903]
 [0.91809666 0.08190331]]

Rep 1,001.366:
[[0.9891418  0.01085822]
 [0.9978243  0.0021757 ]
 [0.86597055 0.1340295 ]
 [0.75534946 0.24465059]
 [0.9948486  0.00515147]
 [0.7487217  0.25127825]
 [0.65209514 0.3479049 ]
 [0.97173023 0.02826979]
 [0.6809564  0.3190436 ]
 [0.8085297  0.1914703 ]
 [0.98039055 0.0196094 ]
 [0.9968727  0.00312728]]

Rep 1,001.451:
[[0.9203834  0.07961664]
 [0.69588476 0.30411524]
 [0.36792126 0.63207877]
 [0.9480726  0.05192736]
 [0.7825905  0.21740945]
 [0.45273483 0.5472652 ]
 [0.8164064  0.18359356]
 [0.8302341  0.16976589]
 [0.3005944  0.6994056 ]
 [0.9004783  0.09952169]
 [0.90697163 0.0930284 ]
 [0.63112956 0.3688704 ]]

Rep 1,001.055:
[[0.01166363 0.9883364 ]
 [0.06894984 0.9310501 ]
 [0.01144443 0.9885556 ]
 [0.18199332 0.8180067 ]
 [0.26720354 0.7327965 ]
 [0.13143228 0.8685677 ]
 [0.14318942 0.85681057]
 [0.04360688 0.9563931 ]
 [0.0633756  0.93662447]
 [0.04686091 0.9531391 ]
 [0.02688313 0.9731169 ]
 [0.09427399 0.905726  ]]

Rep 1,001.245:
[[0.6394807  0.3605193 ]
 [0.44431257 0.5556874 ]
 [0.09149528 0.90850466]
 [0.9846102  0.0153898 ]
 [0.9082814  0.0917186 ]
 [0.3981649  0.60183513]
 [0.70265335 0.29734665]
 [0.5573329  0.44266716]
 [0.08072075 0.9192793 ]
 [0.56006414 0.43993583]
 [0.43790874 0.5620913 ]
 [0.2275274  0.7724726 ]]

Rep 1,001.718:
[[2.4748129e-01 7.5251877e-01]
 [3.1433493e-01 6.8566507e-01]
 [6.6319585e-02 9.3368042e-01]
 [2.4723189e-02 9.7527677e-01]
 [5.7860903e-02 9.4213915e-01]
 [3.1628897e-03 9.9683714e-01]
 [4.1182810e-03 9.9588168e-01]
 [1.0530736e-02 9.8946929e-01]
 [5.6110375e-04 9.9943882e-01]
 [1.3599554e-01 8.6400443e-01]
 [1.3349395e-01 8.6650604e-01]
 [2.4283986e-01 7.5716019e-01]]

Rep 1,001.157:
[[0.9550294  0.04497062]
 [0.9862807  0.01371929]
 [0.8560611  0.14393891]
 [0.8178202  0.18217984]
 [0.98639536 0.01360468]
 [0.88029623 0.11970378]
 [0.8653762  0.13462384]
 [0.972203   0.02779702]
 [0.8400805  0.15991947]
 [0.92219543 0.07780458]
 [0.9852342  0.01476579]
 [0.99369323 0.00630676]]

Rep 1,001.487:
[[0.18828532 0.8117147 ]
 [0.05022856 0.9497714 ]
 [0.00979299 0.990207  ]
 [0.10502169 0.8949783 ]
 [0.02321579 0.97678417]
 [0.00314083 0.9968592 ]
 [0.12100931 0.8789907 ]
 [0.08531406 0.91468596]
 [0.00399093 0.9960091 ]
 [0.31630227 0.6836977 ]
 [0.1783481  0.8216519 ]
 [0.05941691 0.94058305]]

Rep 1,001.294:
[[0.5767737  0.42322627]
 [0.56680685 0.43319312]
 [0.10610663 0.89389336]
 [0.29613656 0.70386344]
 [0.19473758 0.80526245]
 [0.01723897 0.98276097]
 [0.18870237 0.8112976 ]
 [0.16689177 0.83310825]
 [0.01283701 0.987163  ]
 [0.74810827 0.25189176]
 [0.55172575 0.44827422]
 [0.60015255 0.3998475 ]]

Rep 1,001.116:
[[0.91324925 0.08675077]
 [0.9619394  0.03806065]
 [0.5372002  0.46279976]
 [0.98968554 0.01031447]
 [0.9605093  0.03949075]
 [0.54881316 0.45118687]
 [0.9776299  0.02237009]
 [0.85129327 0.14870675]
 [0.5007755  0.49922445]
 [0.98851776 0.01148231]
 [0.91782683 0.0821731 ]
 [0.9701812  0.0298188 ]]

Rep 1,001.218:
[[0.76250315 0.23749691]
 [0.91149086 0.0885091 ]
 [0.62629724 0.3737028 ]
 [0.94188356 0.05811645]
 [0.89942753 0.10057243]
 [0.6658826  0.33411738]
 [0.9718992  0.02810081]
 [0.86109185 0.13890816]
 [0.8543181  0.14568195]
 [0.91340756 0.08659244]
 [0.7178016  0.2821984 ]
 [0.90086615 0.09913389]]

Rep 1,001.472:
[[0.62456805 0.37543195]
 [0.41218463 0.58781534]
 [0.07434248 0.92565745]
 [0.6489579  0.35104206]
 [0.4642173  0.5357827 ]
 [0.10979692 0.89020306]
 [0.43276498 0.567235  ]
 [0.43289253 0.56710744]
 [0.07909334 0.92090666]
 [0.56362706 0.4363729 ]
 [0.5269236  0.47307643]
 [0.31977916 0.68022084]]

Rep 1,001.307:
[[0.490289   0.50971097]
 [0.771305   0.22869496]
 [0.50854325 0.49145672]
 [0.5824653  0.4175347 ]
 [0.8945217  0.10547829]
 [0.7635819  0.23641811]
 [0.81810105 0.18189897]
 [0.9152127  0.0847873 ]
 [0.8856237  0.11437627]
 [0.622744   0.37725598]
 [0.6353566  0.36464345]
 [0.9003888  0.09961126]]

Rep 1,001.220:
[[0.04600294 0.9539971 ]
 [0.07676241 0.9232376 ]
 [0.03076692 0.9692331 ]
 [0.32436028 0.67563975]
 [0.28079695 0.71920305]
 [0.11074252 0.8892575 ]
 [0.44504133 0.5549587 ]
 [0.41763455 0.5823654 ]
 [0.30100986 0.69899017]
 [0.22131793 0.77868205]
 [0.14446008 0.8555399 ]
 [0.3322716  0.66772836]]

Rep 1,001.184:
[[0.9844253  0.01557469]
 [0.99752027 0.00247976]
 [0.8166396  0.18336035]
 [0.70326567 0.29673427]
 [0.99679834 0.00320175]
 [0.76672846 0.23327158]
 [0.7121848  0.2878152 ]
 [0.9804299  0.01957012]
 [0.6691668  0.33083326]
 [0.86487    0.13512993]
 [0.9898636  0.01013635]
 [0.9981421  0.00185782]]

Rep 1,001.212:
[[0.9962903  0.00370971]
 [0.9989642  0.0010358 ]
 [0.98928714 0.01071286]
 [0.5800316  0.41996846]
 [0.7887684  0.21123157]
 [0.30115542 0.69884455]
 [0.45354512 0.54645485]
 [0.6170704  0.3829296 ]
 [0.33783957 0.6621604 ]
 [0.737356   0.26264402]
 [0.510979   0.489021  ]
 [0.8086946  0.19130535]]

Rep 1,001.193:
[[0.4533848  0.54661524]
 [0.61609703 0.38390294]
 [0.28675562 0.7132443 ]
 [0.28691062 0.71308935]
 [0.44511312 0.5548869 ]
 [0.10808831 0.8919117 ]
 [0.3046701  0.69532996]
 [0.3778203  0.62217975]
 [0.1255038  0.87449616]
 [0.4209786  0.5790214 ]
 [0.49434888 0.5056511 ]
 [0.50448835 0.49551168]]

Rep 1,001.866:
[[0.9770955  0.02290456]
 [0.94806266 0.05193732]
 [0.83368105 0.16631898]
 [0.9902383  0.00976167]
 [0.9700326  0.02996741]
 [0.82146037 0.17853959]
 [0.9848381  0.01516183]
 [0.9722448  0.02775518]
 [0.7956656  0.20433438]
 [0.9948258  0.00517416]
 [0.9875218  0.0124782 ]
 [0.9727466  0.02725331]]

Rep 1,001.191:
[[0.974936   0.02506391]
 [0.9672455  0.03275446]
 [0.5560081  0.44399184]
 [0.9938711  0.0061289 ]
 [0.99275476 0.00724525]
 [0.87117773 0.12882222]
 [0.9812702  0.01872978]
 [0.9781885  0.02181152]
 [0.5913903  0.40860963]
 [0.9945305  0.00546952]
 [0.99412775 0.00587223]
 [0.99328965 0.00671037]]

Rep 1,001.572:
[[0.23880737 0.7611927 ]
 [0.28591642 0.7140836 ]
 [0.09155522 0.90844476]
 [0.23596542 0.7640346 ]
 [0.27535206 0.72464794]
 [0.07348451 0.92651546]
 [0.18053938 0.81946063]
 [0.1322363  0.86776376]
 [0.0412102  0.95878977]
 [0.21545444 0.78454554]
 [0.13910356 0.86089647]
 [0.19691123 0.8030887 ]]

Adjacency matrix from edge pred is as follows:

Rep 1,001.971:
[[[0.         0.        ]
  [0.01109636 0.9889037 ]
  [0.02559827 0.9744017 ]
  [0.00639104 0.99360895]]

 [[0.30875313 0.6912468 ]
  [0.         0.        ]
  [0.41907305 0.5809269 ]
  [0.13934706 0.860653  ]]

 [[0.27188596 0.728114  ]
  [0.24263416 0.7573658 ]
  [0.         0.        ]
  [0.17104025 0.8289597 ]]

 [[0.11779184 0.88220817]
  [0.08130334 0.91869664]
  [0.24005292 0.75994706]
  [0.         0.        ]]]

Rep 1,001.411:
[[[0.         0.        ]
  [0.8623328  0.13766718]
  [0.9238409  0.07615913]
  [0.15607862 0.84392136]]

 [[0.995275   0.00472499]
  [0.         0.        ]
  [0.998319   0.00168101]
  [0.8934771  0.1065229 ]]

 [[0.9272253  0.07277476]
  [0.90923506 0.09076496]
  [0.         0.        ]
  [0.32938805 0.6706119 ]]

 [[0.79263467 0.20736532]
  [0.8382541  0.16174598]
  [0.8688926  0.1311074 ]
  [0.         0.        ]]]

Rep 1,001.484:
[[[0.0000000e+00 0.0000000e+00]
  [5.5982540e-03 9.9440175e-01]
  [3.0070809e-03 9.9699295e-01]
  [3.1413743e-04 9.9968588e-01]]

 [[9.2119902e-02 9.0788013e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.0394960e-02 9.7960508e-01]
  [4.5601167e-03 9.9543989e-01]]

 [[3.5823676e-01 6.4176327e-01]
  [2.2233114e-01 7.7766883e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.1847432e-02 9.6815252e-01]]

 [[3.4242332e-01 6.5757662e-01]
  [1.8547773e-01 8.1452233e-01]
  [1.0091117e-01 8.9908880e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.345:
[[[0.         0.        ]
  [0.09827435 0.9017257 ]
  [0.3508982  0.64910185]
  [0.03292892 0.9670711 ]]

 [[0.1308461  0.8691539 ]
  [0.         0.        ]
  [0.33317208 0.6668279 ]
  [0.04104482 0.95895517]]

 [[0.04613201 0.953868  ]
  [0.0710298  0.92897016]
  [0.         0.        ]
  [0.0116863  0.9883137 ]]

 [[0.11074971 0.88925034]
  [0.10939076 0.8906093 ]
  [0.47662053 0.5233795 ]
  [0.         0.        ]]]

Rep 1,001.326:
[[[0.         0.        ]
  [0.9464838  0.05351627]
  [0.9958431  0.00415691]
  [0.9015892  0.09841078]]

 [[0.8572717  0.14272831]
  [0.         0.        ]
  [0.9984493  0.0015506 ]
  [0.94299275 0.05700727]]

 [[0.7913213  0.20867868]
  [0.95510095 0.04489906]
  [0.         0.        ]
  [0.8454415  0.1545585 ]]

 [[0.8815912  0.11840879]
  [0.9732861  0.02671386]
  [0.9981109  0.00188912]
  [0.         0.        ]]]

Rep 1,001.411:
[[[0.         0.        ]
  [0.27793115 0.72206885]
  [0.42601436 0.57398564]
  [0.11445855 0.8855414 ]]

 [[0.01671134 0.9832887 ]
  [0.         0.        ]
  [0.06888643 0.9311136 ]
  [0.01067323 0.98932683]]

 [[0.01648166 0.98351836]
  [0.04390689 0.95609313]
  [0.         0.        ]
  [0.01078425 0.98921573]]

 [[0.16200036 0.83799964]
  [0.26532993 0.73467004]
  [0.42883566 0.57116437]
  [0.         0.        ]]]

Rep 1,001.395:
[[[0.         0.        ]
  [0.99205196 0.00794801]
  [0.9971209  0.00287901]
  [0.9043545  0.09564553]]

 [[0.9605491  0.03945083]
  [0.         0.        ]
  [0.99351066 0.00648931]
  [0.80622965 0.1937704 ]]

 [[0.9096155  0.09038452]
  [0.960844   0.03915606]
  [0.         0.        ]
  [0.76786983 0.23213017]]

 [[0.9333593  0.06664073]
  [0.97384363 0.02615638]
  [0.9926363  0.0073637 ]
  [0.         0.        ]]]

Rep 1,001.927:
[[[0.         0.        ]
  [0.13562818 0.8643718 ]
  [0.06644547 0.93355453]
  [0.0396707  0.9603293 ]]

 [[0.25302717 0.74697286]
  [0.         0.        ]
  [0.08388324 0.9161167 ]
  [0.06154268 0.9384573 ]]

 [[0.4253605  0.5746395 ]
  [0.3058441  0.69415593]
  [0.         0.        ]
  [0.13098991 0.8690101 ]]

 [[0.35330334 0.6466967 ]
  [0.20191117 0.79808885]
  [0.1012025  0.89879745]
  [0.         0.        ]]]

Rep 1,001.188:
[[[0.         0.        ]
  [0.98040295 0.019597  ]
  [0.99414295 0.00585702]
  [0.6876844  0.31231558]]

 [[0.9138598  0.08614025]
  [0.         0.        ]
  [0.99715745 0.00284259]
  [0.7737363  0.22626375]]

 [[0.8736196  0.12638041]
  [0.98422295 0.01577701]
  [0.         0.        ]
  [0.6929826  0.3070174 ]]

 [[0.9364611  0.06353887]
  [0.99047256 0.00952744]
  [0.9968072  0.00319282]
  [0.         0.        ]]]

Rep 1,001.273:
[[[0.         0.        ]
  [0.9455493  0.05445074]
  [0.80046886 0.19953114]
  [0.42748994 0.57251006]]

 [[0.9667087  0.03329125]
  [0.         0.        ]
  [0.7980438  0.20195621]
  [0.46659413 0.5334059 ]]

 [[0.94209385 0.05790612]
  [0.8887007  0.11129931]
  [0.         0.        ]
  [0.38381562 0.6161844 ]]

 [[0.9664197  0.03358027]
  [0.92202884 0.07797114]
  [0.7680358  0.23196419]
  [0.         0.        ]]]

Rep 1,001.688:
[[[0.         0.        ]
  [0.88109255 0.11890748]
  [0.651931   0.348069  ]
  [0.4719023  0.5280977 ]]

 [[0.08118367 0.91881627]
  [0.         0.        ]
  [0.40377674 0.5962233 ]
  [0.12846026 0.8715397 ]]

 [[0.17470133 0.82529867]
  [0.8434859  0.15651411]
  [0.         0.        ]
  [0.25661755 0.74338245]]

 [[0.18504609 0.814954  ]
  [0.80208856 0.19791143]
  [0.6324777  0.3675223 ]
  [0.         0.        ]]]

Rep 1,001.116:
[[[0.         0.        ]
  [0.86217946 0.13782053]
  [0.9250156  0.0749844 ]
  [0.54000014 0.45999992]]

 [[0.41231334 0.58768666]
  [0.         0.        ]
  [0.9782507  0.02174938]
  [0.5894915  0.41050845]]

 [[0.48814666 0.51185334]
  [0.953106   0.04689401]
  [0.         0.        ]
  [0.58213043 0.41786957]]

 [[0.36423096 0.63576907]
  [0.9152375  0.08476255]
  [0.9633636  0.03663643]
  [0.         0.        ]]]

Rep 1,001.379:
[[[0.         0.        ]
  [0.15310642 0.8468936 ]
  [0.11330382 0.8866962 ]
  [0.02912983 0.9708702 ]]

 [[0.4576585  0.5423415 ]
  [0.         0.        ]
  [0.3477607  0.65223926]
  [0.11643278 0.88356715]]

 [[0.4309511  0.5690489 ]
  [0.3724848  0.6275152 ]
  [0.         0.        ]
  [0.14527397 0.8547261 ]]

 [[0.45533076 0.5446693 ]
  [0.34119868 0.6588013 ]
  [0.33058172 0.66941833]
  [0.         0.        ]]]

Rep 1,001.364:
[[[0.         0.        ]
  [0.9921469  0.00785315]
  [0.9845821  0.01541785]
  [0.812377   0.18762304]]

 [[0.91808283 0.08191715]
  [0.         0.        ]
  [0.97390544 0.0260945 ]
  [0.6333549  0.36664513]]

 [[0.95851874 0.04148131]
  [0.99075574 0.00924423]
  [0.         0.        ]
  [0.8094951  0.19050491]]

 [[0.90388423 0.09611575]
  [0.9805659  0.01943405]
  [0.97504336 0.02495668]
  [0.         0.        ]]]

Rep 1,001.471:
[[[0.         0.        ]
  [0.58416265 0.41583738]
  [0.4861313  0.5138687 ]
  [0.20480864 0.79519135]]

 [[0.6885199  0.31148013]
  [0.         0.        ]
  [0.5290363  0.4709637 ]
  [0.19529521 0.8047048 ]]

 [[0.7932451  0.2067549 ]
  [0.7542828  0.24571726]
  [0.         0.        ]
  [0.39100122 0.60899884]]

 [[0.76261014 0.23738986]
  [0.68252367 0.31747627]
  [0.59051204 0.40948796]
  [0.         0.        ]]]

Rep 1,001.359:
[[[0.         0.        ]
  [0.9982982  0.00170175]
  [0.9939498  0.0060502 ]
  [0.92820674 0.0717933 ]]

 [[0.9919743  0.00802574]
  [0.         0.        ]
  [0.95155174 0.04844821]
  [0.48902032 0.5109797 ]]

 [[0.9894163  0.01058366]
  [0.9864704  0.01352955]
  [0.         0.        ]
  [0.49983802 0.50016195]]

 [[0.99472517 0.00527479]
  [0.9882754  0.01172458]
  [0.95964146 0.04035855]
  [0.         0.        ]]]

Rep 1,001.419:
[[[0.         0.        ]
  [0.10119171 0.89880824]
  [0.1892507  0.8107493 ]
  [0.05472394 0.945276  ]]

 [[0.07282746 0.9271725 ]
  [0.         0.        ]
  [0.11902346 0.8809766 ]
  [0.02413553 0.97586447]]

 [[0.12961777 0.8703822 ]
  [0.11690491 0.88309515]
  [0.         0.        ]
  [0.07018377 0.92981625]]

 [[0.1607093  0.8392907 ]
  [0.10618287 0.8938171 ]
  [0.25002235 0.74997765]
  [0.         0.        ]]]

Rep 1,001.724:
[[[0.         0.        ]
  [0.22208726 0.77791274]
  [0.3612922  0.6387078 ]
  [0.13511282 0.8648872 ]]

 [[0.00407612 0.9959239 ]
  [0.         0.        ]
  [0.03666448 0.9633355 ]
  [0.00247016 0.9975298 ]]

 [[0.00296842 0.99703157]
  [0.02544159 0.9745584 ]
  [0.         0.        ]
  [0.00200731 0.99799275]]

 [[0.0429442  0.95705575]
  [0.12675178 0.8732483 ]
  [0.2250145  0.77498555]
  [0.         0.        ]]]

Rep 1,001.294:
[[[0.         0.        ]
  [0.8830955  0.11690454]
  [0.9779468  0.02205318]
  [0.76822406 0.23177597]]

 [[0.6950862  0.3049138 ]
  [0.         0.        ]
  [0.97242326 0.0275768 ]
  [0.67868346 0.3213166 ]]

 [[0.7531706  0.24682942]
  [0.8573964  0.14260353]
  [0.         0.        ]
  [0.7067899  0.2932101 ]]

 [[0.770481   0.22951898]
  [0.8969627  0.10303725]
  [0.9860763  0.01392379]
  [0.         0.        ]]]

Rep 1,001.268:
[[[0.0000000e+00 0.0000000e+00]
  [1.1151094e-02 9.8884892e-01]
  [8.7762084e-03 9.9122381e-01]
  [9.9195517e-04 9.9900812e-01]]

 [[2.6023161e-01 7.3976839e-01]
  [0.0000000e+00 0.0000000e+00]
  [8.6300008e-02 9.1369992e-01]
  [2.5727049e-02 9.7427297e-01]]

 [[3.0054942e-01 6.9945055e-01]
  [2.1174714e-01 7.8825289e-01]
  [0.0000000e+00 0.0000000e+00]
  [4.7359053e-02 9.5264101e-01]]

 [[2.6257598e-01 7.3742402e-01]
  [8.6915545e-02 9.1308445e-01]
  [6.2875949e-02 9.3712407e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.303:
[[[0.         0.        ]
  [0.15822014 0.8417798 ]
  [0.5591204  0.4408796 ]
  [0.2659616  0.7340384 ]]

 [[0.37015802 0.6298419 ]
  [0.         0.        ]
  [0.7729222  0.22707786]
  [0.33921748 0.6607826 ]]

 [[0.29014242 0.7098576 ]
  [0.28782368 0.7121763 ]
  [0.         0.        ]
  [0.21949235 0.7805076 ]]

 [[0.2770682  0.72293174]
  [0.26610157 0.7338984 ]
  [0.7069778  0.29302227]
  [0.         0.        ]]]

Rep 1,001.530:
[[[0.         0.        ]
  [0.30728987 0.6927101 ]
  [0.2516433  0.7483567 ]
  [0.04327589 0.95672417]]

 [[0.884846   0.11515399]
  [0.         0.        ]
  [0.72550255 0.27449745]
  [0.24268301 0.757317  ]]

 [[0.4842074  0.5157926 ]
  [0.33553907 0.66446096]
  [0.         0.        ]
  [0.03990489 0.9600951 ]]

 [[0.6010186  0.39898133]
  [0.4597486  0.54025143]
  [0.34138444 0.6586156 ]
  [0.         0.        ]]]

Rep 1,001.474:
[[[0.         0.        ]
  [0.9820817  0.01791826]
  [0.9238661  0.07613388]
  [0.45136434 0.5486357 ]]

 [[0.9709919  0.02900809]
  [0.         0.        ]
  [0.89831007 0.10168991]
  [0.43235603 0.56764394]]

 [[0.951515   0.04848497]
  [0.9553246  0.04467544]
  [0.         0.        ]
  [0.3711303  0.6288698 ]]

 [[0.98072433 0.0192757 ]
  [0.9798252  0.02017478]
  [0.91943944 0.08056057]
  [0.         0.        ]]]

Rep 1,001.133:
[[[0.         0.        ]
  [0.98682773 0.01317224]
  [0.88676566 0.1132344 ]
  [0.5453394  0.45466056]]

 [[0.96531814 0.03468183]
  [0.         0.        ]
  [0.8217612  0.17823882]
  [0.4191591  0.5808409 ]]

 [[0.9653315  0.03466849]
  [0.9677     0.03230006]
  [0.         0.        ]
  [0.38868    0.61132   ]]

 [[0.98417085 0.01582908]
  [0.9860508  0.01394927]
  [0.8889788  0.11102121]
  [0.         0.        ]]]

Rep 1,001.711:
[[[0.         0.        ]
  [0.05029242 0.9497075 ]
  [0.13514549 0.8648545 ]
  [0.04125372 0.95874625]]

 [[0.12750953 0.8724904 ]
  [0.         0.        ]
  [0.14454307 0.85545695]
  [0.09956204 0.90043795]]

 [[0.19688787 0.8031121 ]
  [0.09962997 0.90037   ]
  [0.         0.        ]
  [0.11317073 0.8868293 ]]

 [[0.05380258 0.94619745]
  [0.0374015  0.9625985 ]
  [0.07682534 0.92317474]
  [0.         0.        ]]]

Rep 1,001.166:
[[[0.         0.        ]
  [0.9657218  0.03427827]
  [0.97989696 0.02010298]
  [0.40667495 0.5933251 ]]

 [[0.9927723  0.00722775]
  [0.         0.        ]
  [0.9925404  0.00745957]
  [0.6091644  0.39083564]]

 [[0.97713906 0.02286092]
  [0.9561016  0.04389841]
  [0.         0.        ]
  [0.3860086  0.6139914 ]]

 [[0.99210113 0.00789891]
  [0.98234415 0.01765586]
  [0.98714894 0.01285104]
  [0.         0.        ]]]

Rep 1,001.354:
[[[0.         0.        ]
  [0.1279726  0.8720274 ]
  [0.07469252 0.92530745]
  [0.01851567 0.9814843 ]]

 [[0.6660664  0.33393356]
  [0.         0.        ]
  [0.3519403  0.6480597 ]
  [0.13229723 0.8677027 ]]

 [[0.45786172 0.54213834]
  [0.2595447  0.7404553 ]
  [0.         0.        ]
  [0.06467935 0.9353206 ]]

 [[0.60075885 0.39924115]
  [0.41162997 0.58837   ]
  [0.22269821 0.7773018 ]
  [0.         0.        ]]]

Rep 1,001.783:
[[[0.         0.        ]
  [0.82259226 0.17740779]
  [0.73572266 0.26427737]
  [0.51015586 0.48984417]]

 [[0.6520753  0.34792468]
  [0.         0.        ]
  [0.6872176  0.31278244]
  [0.17716363 0.8228364 ]]

 [[0.8419778  0.15802222]
  [0.8491134  0.15088661]
  [0.         0.        ]
  [0.43427247 0.56572753]]

 [[0.88517547 0.11482447]
  [0.91636753 0.08363247]
  [0.9155888  0.0844112 ]
  [0.         0.        ]]]

Rep 1,001.669:
[[[0.         0.        ]
  [0.33225098 0.667749  ]
  [0.26684242 0.7331575 ]
  [0.05143552 0.94856447]]

 [[0.97902465 0.02097537]
  [0.         0.        ]
  [0.9115703  0.0884297 ]
  [0.4067604  0.59323955]]

 [[0.70424265 0.29575735]
  [0.4133071  0.5866929 ]
  [0.         0.        ]
  [0.11020311 0.88979685]]

 [[0.73620915 0.26379082]
  [0.44298017 0.5570198 ]
  [0.36747912 0.63252085]
  [0.         0.        ]]]

Rep 1,001.268:
[[[0.         0.        ]
  [0.99506193 0.0049381 ]
  [0.97835106 0.02164889]
  [0.58608675 0.41391325]]

 [[0.9311749  0.06882513]
  [0.         0.        ]
  [0.9623583  0.03764163]
  [0.5477471  0.45225292]]

 [[0.97873497 0.02126511]
  [0.9982339  0.00176605]
  [0.         0.        ]
  [0.85123026 0.14876974]]

 [[0.9491134  0.0508866 ]
  [0.99038154 0.00961841]
  [0.9591144  0.04088559]
  [0.         0.        ]]]

Rep 1,001.432:
[[[0.         0.        ]
  [0.7501255  0.2498745 ]
  [0.83801794 0.16198204]
  [0.5166668  0.4833332 ]]

 [[0.7825211  0.21747884]
  [0.         0.        ]
  [0.89652026 0.10347976]
  [0.52924776 0.4707522 ]]

 [[0.61659414 0.3834058 ]
  [0.65956247 0.34043753]
  [0.         0.        ]
  [0.41010943 0.58989054]]

 [[0.72993934 0.27006066]
  [0.85934097 0.14065903]
  [0.91809666 0.08190331]
  [0.         0.        ]]]

Rep 1,001.366:
[[[0.         0.        ]
  [0.9891418  0.01085822]
  [0.9978243  0.0021757 ]
  [0.86597055 0.1340295 ]]

 [[0.75534946 0.24465059]
  [0.         0.        ]
  [0.9948486  0.00515147]
  [0.7487217  0.25127825]]

 [[0.65209514 0.3479049 ]
  [0.97173023 0.02826979]
  [0.         0.        ]
  [0.6809564  0.3190436 ]]

 [[0.8085297  0.1914703 ]
  [0.98039055 0.0196094 ]
  [0.9968727  0.00312728]
  [0.         0.        ]]]

Rep 1,001.451:
[[[0.         0.        ]
  [0.9203834  0.07961664]
  [0.69588476 0.30411524]
  [0.36792126 0.63207877]]

 [[0.9480726  0.05192736]
  [0.         0.        ]
  [0.7825905  0.21740945]
  [0.45273483 0.5472652 ]]

 [[0.8164064  0.18359356]
  [0.8302341  0.16976589]
  [0.         0.        ]
  [0.3005944  0.6994056 ]]

 [[0.9004783  0.09952169]
  [0.90697163 0.0930284 ]
  [0.63112956 0.3688704 ]
  [0.         0.        ]]]

Rep 1,001.055:
[[[0.         0.        ]
  [0.01166363 0.9883364 ]
  [0.06894984 0.9310501 ]
  [0.01144443 0.9885556 ]]

 [[0.18199332 0.8180067 ]
  [0.         0.        ]
  [0.26720354 0.7327965 ]
  [0.13143228 0.8685677 ]]

 [[0.14318942 0.85681057]
  [0.04360688 0.9563931 ]
  [0.         0.        ]
  [0.0633756  0.93662447]]

 [[0.04686091 0.9531391 ]
  [0.02688313 0.9731169 ]
  [0.09427399 0.905726  ]
  [0.         0.        ]]]

Rep 1,001.245:
[[[0.         0.        ]
  [0.6394807  0.3605193 ]
  [0.44431257 0.5556874 ]
  [0.09149528 0.90850466]]

 [[0.9846102  0.0153898 ]
  [0.         0.        ]
  [0.9082814  0.0917186 ]
  [0.3981649  0.60183513]]

 [[0.70265335 0.29734665]
  [0.5573329  0.44266716]
  [0.         0.        ]
  [0.08072075 0.9192793 ]]

 [[0.56006414 0.43993583]
  [0.43790874 0.5620913 ]
  [0.2275274  0.7724726 ]
  [0.         0.        ]]]

Rep 1,001.718:
[[[0.0000000e+00 0.0000000e+00]
  [2.4748129e-01 7.5251877e-01]
  [3.1433493e-01 6.8566507e-01]
  [6.6319585e-02 9.3368042e-01]]

 [[2.4723189e-02 9.7527677e-01]
  [0.0000000e+00 0.0000000e+00]
  [5.7860903e-02 9.4213915e-01]
  [3.1628897e-03 9.9683714e-01]]

 [[4.1182810e-03 9.9588168e-01]
  [1.0530736e-02 9.8946929e-01]
  [0.0000000e+00 0.0000000e+00]
  [5.6110375e-04 9.9943882e-01]]

 [[1.3599554e-01 8.6400443e-01]
  [1.3349395e-01 8.6650604e-01]
  [2.4283986e-01 7.5716019e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.157:
[[[0.         0.        ]
  [0.9550294  0.04497062]
  [0.9862807  0.01371929]
  [0.8560611  0.14393891]]

 [[0.8178202  0.18217984]
  [0.         0.        ]
  [0.98639536 0.01360468]
  [0.88029623 0.11970378]]

 [[0.8653762  0.13462384]
  [0.972203   0.02779702]
  [0.         0.        ]
  [0.8400805  0.15991947]]

 [[0.92219543 0.07780458]
  [0.9852342  0.01476579]
  [0.99369323 0.00630676]
  [0.         0.        ]]]

Rep 1,001.487:
[[[0.         0.        ]
  [0.18828532 0.8117147 ]
  [0.05022856 0.9497714 ]
  [0.00979299 0.990207  ]]

 [[0.10502169 0.8949783 ]
  [0.         0.        ]
  [0.02321579 0.97678417]
  [0.00314083 0.9968592 ]]

 [[0.12100931 0.8789907 ]
  [0.08531406 0.91468596]
  [0.         0.        ]
  [0.00399093 0.9960091 ]]

 [[0.31630227 0.6836977 ]
  [0.1783481  0.8216519 ]
  [0.05941691 0.94058305]
  [0.         0.        ]]]

Rep 1,001.294:
[[[0.         0.        ]
  [0.5767737  0.42322627]
  [0.56680685 0.43319312]
  [0.10610663 0.89389336]]

 [[0.29613656 0.70386344]
  [0.         0.        ]
  [0.19473758 0.80526245]
  [0.01723897 0.98276097]]

 [[0.18870237 0.8112976 ]
  [0.16689177 0.83310825]
  [0.         0.        ]
  [0.01283701 0.987163  ]]

 [[0.74810827 0.25189176]
  [0.55172575 0.44827422]
  [0.60015255 0.3998475 ]
  [0.         0.        ]]]

Rep 1,001.116:
[[[0.         0.        ]
  [0.91324925 0.08675077]
  [0.9619394  0.03806065]
  [0.5372002  0.46279976]]

 [[0.98968554 0.01031447]
  [0.         0.        ]
  [0.9605093  0.03949075]
  [0.54881316 0.45118687]]

 [[0.9776299  0.02237009]
  [0.85129327 0.14870675]
  [0.         0.        ]
  [0.5007755  0.49922445]]

 [[0.98851776 0.01148231]
  [0.91782683 0.0821731 ]
  [0.9701812  0.0298188 ]
  [0.         0.        ]]]

Rep 1,001.218:
[[[0.         0.        ]
  [0.76250315 0.23749691]
  [0.91149086 0.0885091 ]
  [0.62629724 0.3737028 ]]

 [[0.94188356 0.05811645]
  [0.         0.        ]
  [0.89942753 0.10057243]
  [0.6658826  0.33411738]]

 [[0.9718992  0.02810081]
  [0.86109185 0.13890816]
  [0.         0.        ]
  [0.8543181  0.14568195]]

 [[0.91340756 0.08659244]
  [0.7178016  0.2821984 ]
  [0.90086615 0.09913389]
  [0.         0.        ]]]

Rep 1,001.472:
[[[0.         0.        ]
  [0.62456805 0.37543195]
  [0.41218463 0.58781534]
  [0.07434248 0.92565745]]

 [[0.6489579  0.35104206]
  [0.         0.        ]
  [0.4642173  0.5357827 ]
  [0.10979692 0.89020306]]

 [[0.43276498 0.567235  ]
  [0.43289253 0.56710744]
  [0.         0.        ]
  [0.07909334 0.92090666]]

 [[0.56362706 0.4363729 ]
  [0.5269236  0.47307643]
  [0.31977916 0.68022084]
  [0.         0.        ]]]

Rep 1,001.307:
[[[0.         0.        ]
  [0.490289   0.50971097]
  [0.771305   0.22869496]
  [0.50854325 0.49145672]]

 [[0.5824653  0.4175347 ]
  [0.         0.        ]
  [0.8945217  0.10547829]
  [0.7635819  0.23641811]]

 [[0.81810105 0.18189897]
  [0.9152127  0.0847873 ]
  [0.         0.        ]
  [0.8856237  0.11437627]]

 [[0.622744   0.37725598]
  [0.6353566  0.36464345]
  [0.9003888  0.09961126]
  [0.         0.        ]]]

Rep 1,001.220:
[[[0.         0.        ]
  [0.04600294 0.9539971 ]
  [0.07676241 0.9232376 ]
  [0.03076692 0.9692331 ]]

 [[0.32436028 0.67563975]
  [0.         0.        ]
  [0.28079695 0.71920305]
  [0.11074252 0.8892575 ]]

 [[0.44504133 0.5549587 ]
  [0.41763455 0.5823654 ]
  [0.         0.        ]
  [0.30100986 0.69899017]]

 [[0.22131793 0.77868205]
  [0.14446008 0.8555399 ]
  [0.3322716  0.66772836]
  [0.         0.        ]]]

Rep 1,001.184:
[[[0.         0.        ]
  [0.9844253  0.01557469]
  [0.99752027 0.00247976]
  [0.8166396  0.18336035]]

 [[0.70326567 0.29673427]
  [0.         0.        ]
  [0.99679834 0.00320175]
  [0.76672846 0.23327158]]

 [[0.7121848  0.2878152 ]
  [0.9804299  0.01957012]
  [0.         0.        ]
  [0.6691668  0.33083326]]

 [[0.86487    0.13512993]
  [0.9898636  0.01013635]
  [0.9981421  0.00185782]
  [0.         0.        ]]]

Rep 1,001.212:
[[[0.         0.        ]
  [0.9962903  0.00370971]
  [0.9989642  0.0010358 ]
  [0.98928714 0.01071286]]

 [[0.5800316  0.41996846]
  [0.         0.        ]
  [0.7887684  0.21123157]
  [0.30115542 0.69884455]]

 [[0.45354512 0.54645485]
  [0.6170704  0.3829296 ]
  [0.         0.        ]
  [0.33783957 0.6621604 ]]

 [[0.737356   0.26264402]
  [0.510979   0.489021  ]
  [0.8086946  0.19130535]
  [0.         0.        ]]]

Rep 1,001.193:
[[[0.         0.        ]
  [0.4533848  0.54661524]
  [0.61609703 0.38390294]
  [0.28675562 0.7132443 ]]

 [[0.28691062 0.71308935]
  [0.         0.        ]
  [0.44511312 0.5548869 ]
  [0.10808831 0.8919117 ]]

 [[0.3046701  0.69532996]
  [0.3778203  0.62217975]
  [0.         0.        ]
  [0.1255038  0.87449616]]

 [[0.4209786  0.5790214 ]
  [0.49434888 0.5056511 ]
  [0.50448835 0.49551168]
  [0.         0.        ]]]

Rep 1,001.866:
[[[0.         0.        ]
  [0.9770955  0.02290456]
  [0.94806266 0.05193732]
  [0.83368105 0.16631898]]

 [[0.9902383  0.00976167]
  [0.         0.        ]
  [0.9700326  0.02996741]
  [0.82146037 0.17853959]]

 [[0.9848381  0.01516183]
  [0.9722448  0.02775518]
  [0.         0.        ]
  [0.7956656  0.20433438]]

 [[0.9948258  0.00517416]
  [0.9875218  0.0124782 ]
  [0.9727466  0.02725331]
  [0.         0.        ]]]

Rep 1,001.191:
[[[0.         0.        ]
  [0.974936   0.02506391]
  [0.9672455  0.03275446]
  [0.5560081  0.44399184]]

 [[0.9938711  0.0061289 ]
  [0.         0.        ]
  [0.99275476 0.00724525]
  [0.87117773 0.12882222]]

 [[0.9812702  0.01872978]
  [0.9781885  0.02181152]
  [0.         0.        ]
  [0.5913903  0.40860963]]

 [[0.9945305  0.00546952]
  [0.99412775 0.00587223]
  [0.99328965 0.00671037]
  [0.         0.        ]]]

Rep 1,001.572:
[[[0.         0.        ]
  [0.23880737 0.7611927 ]
  [0.28591642 0.7140836 ]
  [0.09155522 0.90844476]]

 [[0.23596542 0.7640346 ]
  [0.         0.        ]
  [0.27535206 0.72464794]
  [0.07348451 0.92651546]]

 [[0.18053938 0.81946063]
  [0.1322363  0.86776376]
  [0.         0.        ]
  [0.0412102  0.95878977]]

 [[0.21545444 0.78454554]
  [0.13910356 0.86089647]
  [0.19691123 0.8030887 ]
  [0.         0.        ]]]

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.971' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3...

Decoder output plot for rep '1001.971' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3\test

Testing DataLoader 0: 100%|###################################| 10/10 [00:03<00:00,  2.62it/s]

        Test metric               DataLoader 0        

       dec/test_loss           0.01989835314452648    
  enc/test_edge_accuracy       0.47950002551078796    
     enc/test_entropy          0.37624499201774597    
       enc/test_loss            3.802826404571533     
       nri/test_loss           3.8227243423461914     


===========================================================================

Nri model '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.3' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-16 12:05:50
