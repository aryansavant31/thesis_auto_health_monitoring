=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10
Start Time: 2025-09-17 10:31:49
End Time: 2025-09-17 10:40:59

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting nri model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]

Node group name: m004
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 500/500 [OK=500, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (4, 4)
     n1   n2   n3   n4
n1  0.0  1.0  1.0  1.0
n2  1.0  0.0  1.0  1.0
n3  1.0  1.0  0.0  1.0
n4  1.0  1.0  1.0  0.0


**Receiver relation matrix** => shape: (12, 4)
      n1   n2   n3   n4
e12  0.0  1.0  0.0  0.0
e13  0.0  0.0  1.0  0.0
e14  0.0  0.0  0.0  1.0
e21  1.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0
e24  0.0  0.0  0.0  1.0
e31  1.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0
e34  0.0  0.0  0.0  1.0
e41  1.0  0.0  0.0  0.0
e42  0.0  1.0  0.0  0.0
e43  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (12, 4)
      n1   n2   n3   n4
e12  1.0  0.0  0.0  0.0
e13  1.0  0.0  0.0  0.0
e14  1.0  0.0  0.0  0.0
e21  0.0  1.0  0.0  0.0
e23  0.0  1.0  0.0  0.0
e24  0.0  1.0  0.0  0.0
e31  0.0  0.0  1.0  0.0
e32  0.0  0.0  1.0  0.0
e34  0.0  0.0  1.0  0.0
e41  0.0  0.0  0.0  1.0
e42  0.0  0.0  0.0  1.0
e43  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< ENCODER PARAMETERS >>>>>>
Encoder model parameters:
-------------------------
n_edge_types: 2
is_residual_connection: False
do_prob: {'mlp': 0.0, 'cnn': 0.0}
is_batch_norm: {'mlp': True, 'cnn': False}
is_xavier_weights: True
attention_output_size: 5
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
pipeline: [['1/node_emd.1', 'mlp'], ['1/pairwise_op', 'concat'], ['1/edge_emd.1.@', 'mlp'], ['2/aggregate', 'mean'], ['2/node_emd.1', 'mlp'], ['2/pairwise_op', 'concat'], ['2/edge_emd.1', 'mlp']]
edge_emb_configs: {'mlp': [[512, 'elu'], [512, 'elu']], 'cnn': [[5, 2, 64], [8]]}
node_emb_configs: {'mlp': [[512, 'elu'], [512, 'elu']], 'cnn': [[5, 2, 64], [8]]}
n_comps: 100
n_dims: 3

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 2
msg_out_size: 64
edge_mlp_config: [[64, 'tanh'], [64, 'tanh']]
out_mlp_config: [[64, 'relu'], [64, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
feat_norm: None
reduc_config: None
n_dims: 3

Decoder run parameters:
-------------------------
is_hard: False
skip_first_edge_type: True
pred_steps: 10
is_burn_in: True
final_pred_steps: 30
is_dynamic_graph: False
show_conf_band: False

Training parameters set to: 
lr_enc=0.0002, 
lr_dec=0.001, 
final_beta=0.0, 
warmup_frac=0.9, 
optimizer=adam, 
loss_type_encoder=kld, 
loss_type_decoder=mse, 
prior=tensor([0.5000, 0.5000]), 
add_const_kld=True

---------------------------------------------------------------------------

NRI Model Initialized with the following configurations:
----- NRI Model Summary -----
-- Encoder Summary
Encoder(
  (emb_fn_dict): ModuleDict(
    (1/node_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=300, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (1/edge_emd1@): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (2/node_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (2/edge_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
  )
  (attention_layer_dict): ModuleDict()
  (output_layer): Linear(in_features=512, out_features=2, bias=True)
)
-- Decoder Summary
Decoder(
  (edge_mlp_fn): ModuleList(
    (0-1): 2 x MLP(
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=64, bias=True)
    (hidden_u): Linear(in_features=64, out_features=64, bias=True)
    (input_r): Linear(in_features=3, out_features=64, bias=True)
    (hidden_r): Linear(in_features=64, out_features=64, bias=True)
    (input_h): Linear(in_features=3, out_features=64, bias=True)
    (hidden_h): Linear(in_features=64, out_features=64, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=64, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=64, out_features=3, bias=True)
)

---------------------------------------------------------------------------

'[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10' already exists in the log path 'C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10'.
(a) Overwrite exsiting version, (b) create new version, (c) stop training (Choose 'a', 'b' or 'c'):  Are you sure you want to remove the '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10? (y/n): Overwrote '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10.
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for encoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Step 0, Epoch 1/20, Batch 0/80
temp: 0.9990, beta: 0.0000, 
nri_train_loss: 0.5486, enc_train_loss: 2.5633, enc_train_entropy: 0.4795, dec_train_loss: 0.5486, train_edge_accuracy_pred: 0.5200train_edge_accuracy_matrix: 0.5067

Step 5, Epoch 1/20, Batch 5/80
temp: 0.9940, beta: 0.0000, 
nri_train_loss: 0.0358, enc_train_loss: 7.0796, enc_train_entropy: 0.1032, dec_train_loss: 0.0358, train_edge_accuracy_pred: 0.4733train_edge_accuracy_matrix: 0.4683

Step 10, Epoch 1/20, Batch 10/80
temp: 0.9891, beta: 0.0000, 
nri_train_loss: 0.0390, enc_train_loss: 6.8564, enc_train_entropy: 0.1218, dec_train_loss: 0.0390, train_edge_accuracy_pred: 0.4983train_edge_accuracy_matrix: 0.4983

Step 15, Epoch 1/20, Batch 15/80
temp: 0.9841, beta: 0.0000, 
nri_train_loss: 0.0267, enc_train_loss: 7.1198, enc_train_entropy: 0.0998, dec_train_loss: 0.0267, train_edge_accuracy_pred: 0.5117train_edge_accuracy_matrix: 0.5133

Step 20, Epoch 1/20, Batch 20/80
temp: 0.9792, beta: 0.0000, 
nri_train_loss: 0.0259, enc_train_loss: 7.1319, enc_train_entropy: 0.0988, dec_train_loss: 0.0259, train_edge_accuracy_pred: 0.4883train_edge_accuracy_matrix: 0.4867

Step 25, Epoch 1/20, Batch 25/80
temp: 0.9743, beta: 0.0000, 
nri_train_loss: 0.0248, enc_train_loss: 7.3319, enc_train_entropy: 0.0822, dec_train_loss: 0.0248, train_edge_accuracy_pred: 0.4867train_edge_accuracy_matrix: 0.4850

Step 30, Epoch 1/20, Batch 30/80
temp: 0.9695, beta: 0.0000, 
nri_train_loss: 0.0230, enc_train_loss: 7.3422, enc_train_entropy: 0.0813, dec_train_loss: 0.0230, train_edge_accuracy_pred: 0.4700train_edge_accuracy_matrix: 0.4800

Step 35, Epoch 1/20, Batch 35/80
temp: 0.9646, beta: 0.0000, 
nri_train_loss: 0.0224, enc_train_loss: 7.3646, enc_train_entropy: 0.0794, dec_train_loss: 0.0224, train_edge_accuracy_pred: 0.4767train_edge_accuracy_matrix: 0.4767

Step 40, Epoch 1/20, Batch 40/80
temp: 0.9598, beta: 0.0000, 
nri_train_loss: 0.0219, enc_train_loss: 7.6246, enc_train_entropy: 0.0578, dec_train_loss: 0.0219, train_edge_accuracy_pred: 0.4883train_edge_accuracy_matrix: 0.4767

Step 45, Epoch 1/20, Batch 45/80
temp: 0.9550, beta: 0.0000, 
nri_train_loss: 0.0211, enc_train_loss: 7.4776, enc_train_entropy: 0.0700, dec_train_loss: 0.0211, train_edge_accuracy_pred: 0.5017train_edge_accuracy_matrix: 0.5000

Step 50, Epoch 1/20, Batch 50/80
temp: 0.9503, beta: 0.0000, 
nri_train_loss: 0.0208, enc_train_loss: 7.5377, enc_train_entropy: 0.0650, dec_train_loss: 0.0208, train_edge_accuracy_pred: 0.4817train_edge_accuracy_matrix: 0.4917

Step 55, Epoch 1/20, Batch 55/80
temp: 0.9455, beta: 0.0000, 
nri_train_loss: 0.0198, enc_train_loss: 7.4998, enc_train_entropy: 0.0682, dec_train_loss: 0.0198, train_edge_accuracy_pred: 0.5067train_edge_accuracy_matrix: 0.5067

Step 60, Epoch 1/20, Batch 60/80
temp: 0.9408, beta: 0.0000, 
nri_train_loss: 0.0199, enc_train_loss: 7.5115, enc_train_entropy: 0.0672, dec_train_loss: 0.0199, train_edge_accuracy_pred: 0.5000train_edge_accuracy_matrix: 0.4983

Step 65, Epoch 1/20, Batch 65/80
temp: 0.9361, beta: 0.0000, 
nri_train_loss: 0.0194, enc_train_loss: 7.5163, enc_train_entropy: 0.0668, dec_train_loss: 0.0194, train_edge_accuracy_pred: 0.5283train_edge_accuracy_matrix: 0.5217

Step 70, Epoch 1/20, Batch 70/80
temp: 0.9314, beta: 0.0000, 
nri_train_loss: 0.0190, enc_train_loss: 7.2627, enc_train_entropy: 0.0879, dec_train_loss: 0.0190, train_edge_accuracy_pred: 0.4867train_edge_accuracy_matrix: 0.4917

Step 75, Epoch 1/20, Batch 75/80
temp: 0.9268, beta: 0.0000, 
nri_train_loss: 0.0193, enc_train_loss: 7.3019, enc_train_entropy: 0.0847, dec_train_loss: 0.0193, train_edge_accuracy_pred: 0.4733train_edge_accuracy_matrix: 0.4717


Epoch 1/20 completed, Global Step: 80
nri_train_loss: 0.0193, enc_train_loss: 7.3019, enc_train_entropy: 0.0847, dec_train_loss: 0.0193, train_edge_accuracy: 0.4733
nri_val_loss: 0.0187, enc_val_loss: 7.1738, enc_val_entropy: 0.0953, dec_val_loss: 0.0187, val_edge_accuracy: 0.4798

---------------------------------------------------------------------------

Step 80, Epoch 2/20, Batch 0/80
temp: 0.9130, beta: 0.0000, 
nri_train_loss: 0.0189, enc_train_loss: 7.2487, enc_train_entropy: 0.0891, dec_train_loss: 0.0189, train_edge_accuracy_pred: 0.4867train_edge_accuracy_matrix: 0.4850

Step 85, Epoch 2/20, Batch 5/80
temp: 0.9084, beta: 0.0000, 
nri_train_loss: 0.0185, enc_train_loss: 7.1650, enc_train_entropy: 0.0961, dec_train_loss: 0.0185, train_edge_accuracy_pred: 0.5033train_edge_accuracy_matrix: 0.5067

Step 90, Epoch 2/20, Batch 10/80
temp: 0.9039, beta: 0.0000, 
nri_train_loss: 0.0182, enc_train_loss: 7.2211, enc_train_entropy: 0.0914, dec_train_loss: 0.0182, train_edge_accuracy_pred: 0.4450train_edge_accuracy_matrix: 0.4433

Step 95, Epoch 2/20, Batch 15/80
temp: 0.8994, beta: 0.0000, 
nri_train_loss: 0.0178, enc_train_loss: 7.3374, enc_train_entropy: 0.0817, dec_train_loss: 0.0178, train_edge_accuracy_pred: 0.4667train_edge_accuracy_matrix: 0.4650

Step 100, Epoch 2/20, Batch 20/80
temp: 0.8949, beta: 0.0000, 
nri_train_loss: 0.0177, enc_train_loss: 7.0932, enc_train_entropy: 0.1021, dec_train_loss: 0.0177, train_edge_accuracy_pred: 0.4950train_edge_accuracy_matrix: 0.4983

Step 105, Epoch 2/20, Batch 25/80
temp: 0.8904, beta: 0.0000, 
nri_train_loss: 0.0175, enc_train_loss: 7.1147, enc_train_entropy: 0.1003, dec_train_loss: 0.0175, train_edge_accuracy_pred: 0.4767train_edge_accuracy_matrix: 0.4700

Step 110, Epoch 2/20, Batch 30/80
temp: 0.8860, beta: 0.0000, 
nri_train_loss: 0.0172, enc_train_loss: 7.0333, enc_train_entropy: 0.1070, dec_train_loss: 0.0172, train_edge_accuracy_pred: 0.4883train_edge_accuracy_matrix: 0.4783

Step 115, Epoch 2/20, Batch 35/80
temp: 0.8816, beta: 0.0000, 
nri_train_loss: 0.0171, enc_train_loss: 6.9377, enc_train_entropy: 0.1150, dec_train_loss: 0.0171, train_edge_accuracy_pred: 0.4733train_edge_accuracy_matrix: 0.4750

Step 120, Epoch 2/20, Batch 40/80
temp: 0.8772, beta: 0.0000, 
nri_train_loss: 0.0169, enc_train_loss: 7.0189, enc_train_entropy: 0.1082, dec_train_loss: 0.0169, train_edge_accuracy_pred: 0.4767train_edge_accuracy_matrix: 0.4767

Step 125, Epoch 2/20, Batch 45/80
temp: 0.8728, beta: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1750, enc_train_entropy: 0.0952, dec_train_loss: 0.0170, train_edge_accuracy_pred: 0.4600train_edge_accuracy_matrix: 0.4600

Step 130, Epoch 2/20, Batch 50/80
temp: 0.8684, beta: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 7.1209, enc_train_entropy: 0.0997, dec_train_loss: 0.0166, train_edge_accuracy_pred: 0.4350train_edge_accuracy_matrix: 0.4417

Step 135, Epoch 2/20, Batch 55/80
temp: 0.8641, beta: 0.0000, 
nri_train_loss: 0.0165, enc_train_loss: 7.0076, enc_train_entropy: 0.1092, dec_train_loss: 0.0165, train_edge_accuracy_pred: 0.4867train_edge_accuracy_matrix: 0.4750

Step 140, Epoch 2/20, Batch 60/80
temp: 0.8598, beta: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 6.8295, enc_train_entropy: 0.1240, dec_train_loss: 0.0163, train_edge_accuracy_pred: 0.4717train_edge_accuracy_matrix: 0.4683

Step 145, Epoch 2/20, Batch 65/80
temp: 0.8555, beta: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 6.8925, enc_train_entropy: 0.1188, dec_train_loss: 0.0166, train_edge_accuracy_pred: 0.4517train_edge_accuracy_matrix: 0.4633

Step 150, Epoch 2/20, Batch 70/80
temp: 0.8512, beta: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 6.7461, enc_train_entropy: 0.1310, dec_train_loss: 0.0163, train_edge_accuracy_pred: 0.4600train_edge_accuracy_matrix: 0.4517

Step 155, Epoch 2/20, Batch 75/80
temp: 0.8470, beta: 0.0000, 
nri_train_loss: 0.0160, enc_train_loss: 6.8964, enc_train_entropy: 0.1184, dec_train_loss: 0.0160, train_edge_accuracy_pred: 0.4183train_edge_accuracy_matrix: 0.4367


Epoch 2/20 completed, Global Step: 160
nri_train_loss: 0.0160, enc_train_loss: 6.8964, enc_train_entropy: 0.1184, dec_train_loss: 0.0160, train_edge_accuracy: 0.4183
nri_val_loss: 0.0160, enc_val_loss: 6.8226, enc_val_entropy: 0.1246, dec_val_loss: 0.0160, val_edge_accuracy: 0.4543

---------------------------------------------------------------------------

Step 160, Epoch 3/20, Batch 0/80
temp: 0.8344, beta: 0.0000, 
nri_train_loss: 0.0161, enc_train_loss: 6.7911, enc_train_entropy: 0.1272, dec_train_loss: 0.0161, train_edge_accuracy_pred: 0.4350train_edge_accuracy_matrix: 0.4383

Step 165, Epoch 3/20, Batch 5/80
temp: 0.8302, beta: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 6.8578, enc_train_entropy: 0.1217, dec_train_loss: 0.0159, train_edge_accuracy_pred: 0.4400train_edge_accuracy_matrix: 0.4383

Step 170, Epoch 3/20, Batch 10/80
temp: 0.8261, beta: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 6.8699, enc_train_entropy: 0.1207, dec_train_loss: 0.0158, train_edge_accuracy_pred: 0.4450train_edge_accuracy_matrix: 0.4600

Step 175, Epoch 3/20, Batch 15/80
temp: 0.8219, beta: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 6.8529, enc_train_entropy: 0.1221, dec_train_loss: 0.0158, train_edge_accuracy_pred: 0.4567train_edge_accuracy_matrix: 0.4517

Step 180, Epoch 3/20, Batch 20/80
temp: 0.8178, beta: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 6.7863, enc_train_entropy: 0.1276, dec_train_loss: 0.0155, train_edge_accuracy_pred: 0.4467train_edge_accuracy_matrix: 0.4583

Step 185, Epoch 3/20, Batch 25/80
temp: 0.8137, beta: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 6.6729, enc_train_entropy: 0.1371, dec_train_loss: 0.0155, train_edge_accuracy_pred: 0.4567train_edge_accuracy_matrix: 0.4617

Step 190, Epoch 3/20, Batch 30/80
temp: 0.8097, beta: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 6.7697, enc_train_entropy: 0.1290, dec_train_loss: 0.0152, train_edge_accuracy_pred: 0.4600train_edge_accuracy_matrix: 0.4617

Step 195, Epoch 3/20, Batch 35/80
temp: 0.8056, beta: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 6.7631, enc_train_entropy: 0.1296, dec_train_loss: 0.0152, train_edge_accuracy_pred: 0.4500train_edge_accuracy_matrix: 0.4317

Step 200, Epoch 3/20, Batch 40/80
temp: 0.8016, beta: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 6.8273, enc_train_entropy: 0.1242, dec_train_loss: 0.0155, train_edge_accuracy_pred: 0.4500train_edge_accuracy_matrix: 0.4483

Step 205, Epoch 3/20, Batch 45/80
temp: 0.7976, beta: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.0556, enc_train_entropy: 0.1052, dec_train_loss: 0.0153, train_edge_accuracy_pred: 0.4567train_edge_accuracy_matrix: 0.4617

Step 210, Epoch 3/20, Batch 50/80
temp: 0.7936, beta: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 6.9446, enc_train_entropy: 0.1144, dec_train_loss: 0.0151, train_edge_accuracy_pred: 0.4333train_edge_accuracy_matrix: 0.4117

Step 215, Epoch 3/20, Batch 55/80
temp: 0.7897, beta: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 6.8996, enc_train_entropy: 0.1182, dec_train_loss: 0.0150, train_edge_accuracy_pred: 0.4633train_edge_accuracy_matrix: 0.4583

Step 220, Epoch 3/20, Batch 60/80
temp: 0.7857, beta: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 6.9344, enc_train_entropy: 0.1153, dec_train_loss: 0.0151, train_edge_accuracy_pred: 0.4567train_edge_accuracy_matrix: 0.4600

Step 225, Epoch 3/20, Batch 65/80
temp: 0.7818, beta: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 6.8866, enc_train_entropy: 0.1193, dec_train_loss: 0.0149, train_edge_accuracy_pred: 0.4533train_edge_accuracy_matrix: 0.4500

Step 230, Epoch 3/20, Batch 70/80
temp: 0.7779, beta: 0.0000, 
nri_train_loss: 0.0147, enc_train_loss: 6.7852, enc_train_entropy: 0.1277, dec_train_loss: 0.0147, train_edge_accuracy_pred: 0.4783train_edge_accuracy_matrix: 0.4800

Step 235, Epoch 3/20, Batch 75/80
temp: 0.7740, beta: 0.0000, 
nri_train_loss: 0.0148, enc_train_loss: 6.8959, enc_train_entropy: 0.1185, dec_train_loss: 0.0148, train_edge_accuracy_pred: 0.4383train_edge_accuracy_matrix: 0.4333


Epoch 3/20 completed, Global Step: 240
nri_train_loss: 0.0148, enc_train_loss: 6.8959, enc_train_entropy: 0.1185, dec_train_loss: 0.0148, train_edge_accuracy: 0.4383
nri_val_loss: 0.0149, enc_val_loss: 6.9026, enc_val_entropy: 0.1179, dec_val_loss: 0.0149, val_edge_accuracy: 0.4620

---------------------------------------------------------------------------

Step 240, Epoch 4/20, Batch 0/80
temp: 0.7625, beta: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 6.8511, enc_train_entropy: 0.1222, dec_train_loss: 0.0151, train_edge_accuracy_pred: 0.4500train_edge_accuracy_matrix: 0.4400

Step 245, Epoch 4/20, Batch 5/80
temp: 0.7587, beta: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.1451, enc_train_entropy: 0.0977, dec_train_loss: 0.0150, train_edge_accuracy_pred: 0.4467train_edge_accuracy_matrix: 0.4467

Step 250, Epoch 4/20, Batch 10/80
temp: 0.7549, beta: 0.0000, 
nri_train_loss: 0.0144, enc_train_loss: 7.1484, enc_train_entropy: 0.0975, dec_train_loss: 0.0144, train_edge_accuracy_pred: 0.4633train_edge_accuracy_matrix: 0.4700

Step 255, Epoch 4/20, Batch 15/80
temp: 0.7512, beta: 0.0000, 
nri_train_loss: 0.0143, enc_train_loss: 6.9828, enc_train_entropy: 0.1112, dec_train_loss: 0.0143, train_edge_accuracy_pred: 0.4700train_edge_accuracy_matrix: 0.4883

Step 260, Epoch 4/20, Batch 20/80
temp: 0.7474, beta: 0.0000, 
nri_train_loss: 0.0144, enc_train_loss: 7.1233, enc_train_entropy: 0.0995, dec_train_loss: 0.0144, train_edge_accuracy_pred: 0.4733train_edge_accuracy_matrix: 0.4783

Step 265, Epoch 4/20, Batch 25/80
temp: 0.7437, beta: 0.0000, 
nri_train_loss: 0.0144, enc_train_loss: 7.1043, enc_train_entropy: 0.1011, dec_train_loss: 0.0144, train_edge_accuracy_pred: 0.4233train_edge_accuracy_matrix: 0.4317

Step 270, Epoch 4/20, Batch 30/80
temp: 0.7400, beta: 0.0000, 
nri_train_loss: 0.0142, enc_train_loss: 7.2801, enc_train_entropy: 0.0865, dec_train_loss: 0.0142, train_edge_accuracy_pred: 0.4800train_edge_accuracy_matrix: 0.4733

Step 275, Epoch 4/20, Batch 35/80
temp: 0.7363, beta: 0.0000, 
nri_train_loss: 0.0143, enc_train_loss: 7.1416, enc_train_entropy: 0.0980, dec_train_loss: 0.0143, train_edge_accuracy_pred: 0.4417train_edge_accuracy_matrix: 0.4533

Step 280, Epoch 4/20, Batch 40/80
temp: 0.7326, beta: 0.0000, 
nri_train_loss: 0.0144, enc_train_loss: 7.3273, enc_train_entropy: 0.0825, dec_train_loss: 0.0144, train_edge_accuracy_pred: 0.4317train_edge_accuracy_matrix: 0.4367

Step 285, Epoch 4/20, Batch 45/80
temp: 0.7289, beta: 0.0000, 
nri_train_loss: 0.0141, enc_train_loss: 7.2718, enc_train_entropy: 0.0872, dec_train_loss: 0.0141, train_edge_accuracy_pred: 0.4350train_edge_accuracy_matrix: 0.4317

Step 290, Epoch 4/20, Batch 50/80
temp: 0.7253, beta: 0.0000, 
nri_train_loss: 0.0140, enc_train_loss: 7.2223, enc_train_entropy: 0.0913, dec_train_loss: 0.0140, train_edge_accuracy_pred: 0.4550train_edge_accuracy_matrix: 0.4617

Step 295, Epoch 4/20, Batch 55/80
temp: 0.7217, beta: 0.0000, 
nri_train_loss: 0.0140, enc_train_loss: 7.1240, enc_train_entropy: 0.0995, dec_train_loss: 0.0140, train_edge_accuracy_pred: 0.4467train_edge_accuracy_matrix: 0.4467

Step 300, Epoch 4/20, Batch 60/80
temp: 0.7181, beta: 0.0000, 
nri_train_loss: 0.0145, enc_train_loss: 7.0871, enc_train_entropy: 0.1026, dec_train_loss: 0.0145, train_edge_accuracy_pred: 0.4617train_edge_accuracy_matrix: 0.4517

Step 305, Epoch 4/20, Batch 65/80
temp: 0.7145, beta: 0.0000, 
nri_train_loss: 0.0140, enc_train_loss: 7.1768, enc_train_entropy: 0.0951, dec_train_loss: 0.0140, train_edge_accuracy_pred: 0.4183train_edge_accuracy_matrix: 0.4250

Step 310, Epoch 4/20, Batch 70/80
temp: 0.7109, beta: 0.0000, 
nri_train_loss: 0.0139, enc_train_loss: 6.9959, enc_train_entropy: 0.1102, dec_train_loss: 0.0139, train_edge_accuracy_pred: 0.4683train_edge_accuracy_matrix: 0.4733

Step 315, Epoch 4/20, Batch 75/80
temp: 0.7074, beta: 0.0000, 
nri_train_loss: 0.0142, enc_train_loss: 7.1354, enc_train_entropy: 0.0985, dec_train_loss: 0.0142, train_edge_accuracy_pred: 0.4417train_edge_accuracy_matrix: 0.4450


Epoch 4/20 completed, Global Step: 320
nri_train_loss: 0.0142, enc_train_loss: 7.1354, enc_train_entropy: 0.0985, dec_train_loss: 0.0142, train_edge_accuracy: 0.4417
nri_val_loss: 0.0138, enc_val_loss: 7.2963, enc_val_entropy: 0.0851, dec_val_loss: 0.0138, val_edge_accuracy: 0.4542

---------------------------------------------------------------------------

Step 320, Epoch 5/20, Batch 0/80
temp: 0.6969, beta: 0.0000, 
nri_train_loss: 0.0136, enc_train_loss: 7.2330, enc_train_entropy: 0.0904, dec_train_loss: 0.0136, train_edge_accuracy_pred: 0.4583train_edge_accuracy_matrix: 0.4450

Step 325, Epoch 5/20, Batch 5/80
temp: 0.6934, beta: 0.0000, 
nri_train_loss: 0.0136, enc_train_loss: 7.3868, enc_train_entropy: 0.0776, dec_train_loss: 0.0136, train_edge_accuracy_pred: 0.4517train_edge_accuracy_matrix: 0.4417

Step 330, Epoch 5/20, Batch 10/80
temp: 0.6899, beta: 0.0000, 
nri_train_loss: 0.0136, enc_train_loss: 7.2610, enc_train_entropy: 0.0881, dec_train_loss: 0.0136, train_edge_accuracy_pred: 0.4517train_edge_accuracy_matrix: 0.4633

Step 335, Epoch 5/20, Batch 15/80
temp: 0.6865, beta: 0.0000, 
nri_train_loss: 0.0136, enc_train_loss: 7.1770, enc_train_entropy: 0.0951, dec_train_loss: 0.0136, train_edge_accuracy_pred: 0.4500train_edge_accuracy_matrix: 0.4417

Step 340, Epoch 5/20, Batch 20/80
temp: 0.6830, beta: 0.0000, 
nri_train_loss: 0.0135, enc_train_loss: 7.2133, enc_train_entropy: 0.0920, dec_train_loss: 0.0135, train_edge_accuracy_pred: 0.4367train_edge_accuracy_matrix: 0.4367

Step 345, Epoch 5/20, Batch 25/80
temp: 0.6796, beta: 0.0000, 
nri_train_loss: 0.0132, enc_train_loss: 7.4309, enc_train_entropy: 0.0739, dec_train_loss: 0.0132, train_edge_accuracy_pred: 0.4333train_edge_accuracy_matrix: 0.4300

Step 350, Epoch 5/20, Batch 30/80
temp: 0.6762, beta: 0.0000, 
nri_train_loss: 0.0130, enc_train_loss: 7.4250, enc_train_entropy: 0.0744, dec_train_loss: 0.0130, train_edge_accuracy_pred: 0.4317train_edge_accuracy_matrix: 0.4367

Step 355, Epoch 5/20, Batch 35/80
temp: 0.6729, beta: 0.0000, 
nri_train_loss: 0.0129, enc_train_loss: 7.8029, enc_train_entropy: 0.0429, dec_train_loss: 0.0129, train_edge_accuracy_pred: 0.4533train_edge_accuracy_matrix: 0.4500

Step 360, Epoch 5/20, Batch 40/80
temp: 0.6695, beta: 0.0000, 
nri_train_loss: 0.0127, enc_train_loss: 7.7592, enc_train_entropy: 0.0465, dec_train_loss: 0.0127, train_edge_accuracy_pred: 0.4383train_edge_accuracy_matrix: 0.4417

Step 365, Epoch 5/20, Batch 45/80
temp: 0.6662, beta: 0.0000, 
nri_train_loss: 0.0124, enc_train_loss: 7.7226, enc_train_entropy: 0.0496, dec_train_loss: 0.0124, train_edge_accuracy_pred: 0.4533train_edge_accuracy_matrix: 0.4517

Step 370, Epoch 5/20, Batch 50/80
temp: 0.6629, beta: 0.0000, 
nri_train_loss: 0.0121, enc_train_loss: 7.5408, enc_train_entropy: 0.0648, dec_train_loss: 0.0121, train_edge_accuracy_pred: 0.4367train_edge_accuracy_matrix: 0.4433

Step 375, Epoch 5/20, Batch 55/80
temp: 0.6595, beta: 0.0000, 
nri_train_loss: 0.0121, enc_train_loss: 7.6198, enc_train_entropy: 0.0582, dec_train_loss: 0.0121, train_edge_accuracy_pred: 0.4250train_edge_accuracy_matrix: 0.4417

Step 380, Epoch 5/20, Batch 60/80
temp: 0.6563, beta: 0.0000, 
nri_train_loss: 0.0115, enc_train_loss: 7.5332, enc_train_entropy: 0.0654, dec_train_loss: 0.0115, train_edge_accuracy_pred: 0.4350train_edge_accuracy_matrix: 0.4233

Step 385, Epoch 5/20, Batch 65/80
temp: 0.6530, beta: 0.0000, 
nri_train_loss: 0.0115, enc_train_loss: 7.4837, enc_train_entropy: 0.0695, dec_train_loss: 0.0115, train_edge_accuracy_pred: 0.4383train_edge_accuracy_matrix: 0.4383

Step 390, Epoch 5/20, Batch 70/80
temp: 0.6497, beta: 0.0000, 
nri_train_loss: 0.0110, enc_train_loss: 7.6733, enc_train_entropy: 0.0537, dec_train_loss: 0.0110, train_edge_accuracy_pred: 0.4300train_edge_accuracy_matrix: 0.4267

Step 395, Epoch 5/20, Batch 75/80
temp: 0.6465, beta: 0.0000, 
nri_train_loss: 0.0103, enc_train_loss: 7.7339, enc_train_entropy: 0.0487, dec_train_loss: 0.0103, train_edge_accuracy_pred: 0.4333train_edge_accuracy_matrix: 0.4233


Epoch 5/20 completed, Global Step: 400
nri_train_loss: 0.0103, enc_train_loss: 7.7339, enc_train_entropy: 0.0487, dec_train_loss: 0.0103, train_edge_accuracy: 0.4333
nri_val_loss: 0.0094, enc_val_loss: 7.7268, enc_val_entropy: 0.0493, dec_val_loss: 0.0094, val_edge_accuracy: 0.4310

---------------------------------------------------------------------------

Step 400, Epoch 6/20, Batch 0/80
temp: 0.6368, beta: 0.0000, 
nri_train_loss: 0.0096, enc_train_loss: 7.6001, enc_train_entropy: 0.0598, dec_train_loss: 0.0096, train_edge_accuracy_pred: 0.4300train_edge_accuracy_matrix: 0.4333

Step 405, Epoch 6/20, Batch 5/80
temp: 0.6337, beta: 0.0000, 
nri_train_loss: 0.0074, enc_train_loss: 7.5818, enc_train_entropy: 0.0613, dec_train_loss: 0.0074, train_edge_accuracy_pred: 0.4167train_edge_accuracy_matrix: 0.4067

Step 410, Epoch 6/20, Batch 10/80
temp: 0.6305, beta: 0.0000, 
nri_train_loss: 0.0068, enc_train_loss: 7.5815, enc_train_entropy: 0.0614, dec_train_loss: 0.0068, train_edge_accuracy_pred: 0.3733train_edge_accuracy_matrix: 0.3717

Step 415, Epoch 6/20, Batch 15/80
temp: 0.6274, beta: 0.0000, 
nri_train_loss: 0.0064, enc_train_loss: 7.5228, enc_train_entropy: 0.0662, dec_train_loss: 0.0064, train_edge_accuracy_pred: 0.4067train_edge_accuracy_matrix: 0.4017

Step 420, Epoch 6/20, Batch 20/80
temp: 0.6242, beta: 0.0000, 
nri_train_loss: 0.0058, enc_train_loss: 7.7228, enc_train_entropy: 0.0496, dec_train_loss: 0.0058, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3833

Step 425, Epoch 6/20, Batch 25/80
temp: 0.6211, beta: 0.0000, 
nri_train_loss: 0.0052, enc_train_loss: 7.7734, enc_train_entropy: 0.0454, dec_train_loss: 0.0052, train_edge_accuracy_pred: 0.3767train_edge_accuracy_matrix: 0.3867

Step 430, Epoch 6/20, Batch 30/80
temp: 0.6180, beta: 0.0000, 
nri_train_loss: 0.0050, enc_train_loss: 7.6508, enc_train_entropy: 0.0556, dec_train_loss: 0.0050, train_edge_accuracy_pred: 0.3883train_edge_accuracy_matrix: 0.3850

Step 435, Epoch 6/20, Batch 35/80
temp: 0.6149, beta: 0.0000, 
nri_train_loss: 0.0049, enc_train_loss: 8.0196, enc_train_entropy: 0.0248, dec_train_loss: 0.0049, train_edge_accuracy_pred: 0.3533train_edge_accuracy_matrix: 0.3500

Step 440, Epoch 6/20, Batch 40/80
temp: 0.6119, beta: 0.0000, 
nri_train_loss: 0.0047, enc_train_loss: 7.7518, enc_train_entropy: 0.0472, dec_train_loss: 0.0047, train_edge_accuracy_pred: 0.3683train_edge_accuracy_matrix: 0.3700

Step 445, Epoch 6/20, Batch 45/80
temp: 0.6088, beta: 0.0000, 
nri_train_loss: 0.0049, enc_train_loss: 7.8114, enc_train_entropy: 0.0422, dec_train_loss: 0.0049, train_edge_accuracy_pred: 0.3833train_edge_accuracy_matrix: 0.3817

Step 450, Epoch 6/20, Batch 50/80
temp: 0.6058, beta: 0.0000, 
nri_train_loss: 0.0044, enc_train_loss: 7.9665, enc_train_entropy: 0.0293, dec_train_loss: 0.0044, train_edge_accuracy_pred: 0.3683train_edge_accuracy_matrix: 0.3700

Step 455, Epoch 6/20, Batch 55/80
temp: 0.6027, beta: 0.0000, 
nri_train_loss: 0.0045, enc_train_loss: 7.8520, enc_train_entropy: 0.0388, dec_train_loss: 0.0045, train_edge_accuracy_pred: 0.3717train_edge_accuracy_matrix: 0.3633

Step 460, Epoch 6/20, Batch 60/80
temp: 0.5997, beta: 0.0000, 
nri_train_loss: 0.0048, enc_train_loss: 7.8681, enc_train_entropy: 0.0375, dec_train_loss: 0.0048, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3717

Step 465, Epoch 6/20, Batch 65/80
temp: 0.5967, beta: 0.0000, 
nri_train_loss: 0.0045, enc_train_loss: 7.8647, enc_train_entropy: 0.0378, dec_train_loss: 0.0045, train_edge_accuracy_pred: 0.3767train_edge_accuracy_matrix: 0.3783

Step 470, Epoch 6/20, Batch 70/80
temp: 0.5938, beta: 0.0000, 
nri_train_loss: 0.0045, enc_train_loss: 7.7796, enc_train_entropy: 0.0448, dec_train_loss: 0.0045, train_edge_accuracy_pred: 0.3850train_edge_accuracy_matrix: 0.3783

Step 475, Epoch 6/20, Batch 75/80
temp: 0.5908, beta: 0.0000, 
nri_train_loss: 0.0043, enc_train_loss: 7.8576, enc_train_entropy: 0.0383, dec_train_loss: 0.0043, train_edge_accuracy_pred: 0.3850train_edge_accuracy_matrix: 0.3767


Epoch 6/20 completed, Global Step: 480
nri_train_loss: 0.0043, enc_train_loss: 7.8576, enc_train_entropy: 0.0383, dec_train_loss: 0.0043, train_edge_accuracy: 0.3850
nri_val_loss: 0.0044, enc_val_loss: 7.7336, enc_val_entropy: 0.0487, dec_val_loss: 0.0044, val_edge_accuracy: 0.3767

---------------------------------------------------------------------------

Step 480, Epoch 7/20, Batch 0/80
temp: 0.5820, beta: 0.0000, 
nri_train_loss: 0.0045, enc_train_loss: 7.7545, enc_train_entropy: 0.0469, dec_train_loss: 0.0045, train_edge_accuracy_pred: 0.3867train_edge_accuracy_matrix: 0.3783

Step 485, Epoch 7/20, Batch 5/80
temp: 0.5791, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.7579, enc_train_entropy: 0.0467, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3783

Step 490, Epoch 7/20, Batch 10/80
temp: 0.5762, beta: 0.0000, 
nri_train_loss: 0.0045, enc_train_loss: 7.6921, enc_train_entropy: 0.0521, dec_train_loss: 0.0045, train_edge_accuracy_pred: 0.3717train_edge_accuracy_matrix: 0.3667

Step 495, Epoch 7/20, Batch 15/80
temp: 0.5733, beta: 0.0000, 
nri_train_loss: 0.0043, enc_train_loss: 7.8671, enc_train_entropy: 0.0376, dec_train_loss: 0.0043, train_edge_accuracy_pred: 0.3617train_edge_accuracy_matrix: 0.3667

Step 500, Epoch 7/20, Batch 20/80
temp: 0.5705, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9071, enc_train_entropy: 0.0342, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3783

Step 505, Epoch 7/20, Batch 25/80
temp: 0.5676, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.7851, enc_train_entropy: 0.0444, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3633

Step 510, Epoch 7/20, Batch 30/80
temp: 0.5648, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.6302, enc_train_entropy: 0.0573, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3800

Step 515, Epoch 7/20, Batch 35/80
temp: 0.5620, beta: 0.0000, 
nri_train_loss: 0.0052, enc_train_loss: 7.7359, enc_train_entropy: 0.0485, dec_train_loss: 0.0052, train_edge_accuracy_pred: 0.3833train_edge_accuracy_matrix: 0.3767

Step 520, Epoch 7/20, Batch 40/80
temp: 0.5592, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.8255, enc_train_entropy: 0.0410, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3783train_edge_accuracy_matrix: 0.3850

Step 525, Epoch 7/20, Batch 45/80
temp: 0.5564, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9594, enc_train_entropy: 0.0299, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3733

Step 530, Epoch 7/20, Batch 50/80
temp: 0.5536, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.7464, enc_train_entropy: 0.0476, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3767train_edge_accuracy_matrix: 0.3717

Step 535, Epoch 7/20, Batch 55/80
temp: 0.5508, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 7.9216, enc_train_entropy: 0.0330, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3717train_edge_accuracy_matrix: 0.3717

Step 540, Epoch 7/20, Batch 60/80
temp: 0.5481, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9670, enc_train_entropy: 0.0292, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3583train_edge_accuracy_matrix: 0.3633

Step 545, Epoch 7/20, Batch 65/80
temp: 0.5454, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.8282, enc_train_entropy: 0.0408, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3683train_edge_accuracy_matrix: 0.3633

Step 550, Epoch 7/20, Batch 70/80
temp: 0.5426, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.7894, enc_train_entropy: 0.0440, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3733train_edge_accuracy_matrix: 0.3783

Step 555, Epoch 7/20, Batch 75/80
temp: 0.5399, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.9037, enc_train_entropy: 0.0345, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3767train_edge_accuracy_matrix: 0.3767


Epoch 7/20 completed, Global Step: 560
nri_train_loss: 0.0040, enc_train_loss: 7.9037, enc_train_entropy: 0.0345, dec_train_loss: 0.0040, train_edge_accuracy: 0.3767
nri_val_loss: 0.0041, enc_val_loss: 7.9321, enc_val_entropy: 0.0321, dec_val_loss: 0.0041, val_edge_accuracy: 0.3740

---------------------------------------------------------------------------

Step 560, Epoch 8/20, Batch 0/80
temp: 0.5319, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.9032, enc_train_entropy: 0.0345, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3733train_edge_accuracy_matrix: 0.3700

Step 565, Epoch 8/20, Batch 5/80
temp: 0.5292, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.7186, enc_train_entropy: 0.0499, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3733train_edge_accuracy_matrix: 0.3817

Step 570, Epoch 8/20, Batch 10/80
temp: 0.5266, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.8275, enc_train_entropy: 0.0409, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3733train_edge_accuracy_matrix: 0.3767

Step 575, Epoch 8/20, Batch 15/80
temp: 0.5240, beta: 0.0000, 
nri_train_loss: 0.0043, enc_train_loss: 7.8594, enc_train_entropy: 0.0382, dec_train_loss: 0.0043, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3733

Step 580, Epoch 8/20, Batch 20/80
temp: 0.5214, beta: 0.0000, 
nri_train_loss: 0.0043, enc_train_loss: 7.8866, enc_train_entropy: 0.0359, dec_train_loss: 0.0043, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3717

Step 585, Epoch 8/20, Batch 25/80
temp: 0.5188, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 7.8117, enc_train_entropy: 0.0422, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3733train_edge_accuracy_matrix: 0.3767

Step 590, Epoch 8/20, Batch 30/80
temp: 0.5162, beta: 0.0000, 
nri_train_loss: 0.0043, enc_train_loss: 7.7547, enc_train_entropy: 0.0469, dec_train_loss: 0.0043, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3750

Step 595, Epoch 8/20, Batch 35/80
temp: 0.5136, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8131, enc_train_entropy: 0.0421, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3733

Step 600, Epoch 8/20, Batch 40/80
temp: 0.5110, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.7763, enc_train_entropy: 0.0451, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3783train_edge_accuracy_matrix: 0.3850

Step 605, Epoch 8/20, Batch 45/80
temp: 0.5085, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.7157, enc_train_entropy: 0.0502, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3733

Step 610, Epoch 8/20, Batch 50/80
temp: 0.5059, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 8.0269, enc_train_entropy: 0.0242, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3683

Step 615, Epoch 8/20, Batch 55/80
temp: 0.5034, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 8.0069, enc_train_entropy: 0.0259, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3633

Step 620, Epoch 8/20, Batch 60/80
temp: 0.5009, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9725, enc_train_entropy: 0.0288, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3717

Step 625, Epoch 8/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0043, enc_train_loss: 7.7796, enc_train_entropy: 0.0448, dec_train_loss: 0.0043, train_edge_accuracy_pred: 0.3833train_edge_accuracy_matrix: 0.3900

Step 630, Epoch 8/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 7.6898, enc_train_entropy: 0.0523, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3867train_edge_accuracy_matrix: 0.3900

Step 635, Epoch 8/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9073, enc_train_entropy: 0.0342, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3700


Epoch 8/20 completed, Global Step: 640
nri_train_loss: 0.0042, enc_train_loss: 7.9073, enc_train_entropy: 0.0342, dec_train_loss: 0.0042, train_edge_accuracy: 0.3700
nri_val_loss: 0.0041, enc_val_loss: 8.0490, enc_val_entropy: 0.0224, dec_val_loss: 0.0041, val_edge_accuracy: 0.3720

---------------------------------------------------------------------------

Step 640, Epoch 9/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9941, enc_train_entropy: 0.0270, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3733train_edge_accuracy_matrix: 0.3750

Step 645, Epoch 9/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 8.0421, enc_train_entropy: 0.0230, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3650

Step 650, Epoch 9/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9191, enc_train_entropy: 0.0332, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3717

Step 655, Epoch 9/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 8.1332, enc_train_entropy: 0.0154, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3650

Step 660, Epoch 9/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 8.0423, enc_train_entropy: 0.0230, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3617

Step 665, Epoch 9/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.9947, enc_train_entropy: 0.0269, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3717train_edge_accuracy_matrix: 0.3667

Step 670, Epoch 9/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9659, enc_train_entropy: 0.0293, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3617

Step 675, Epoch 9/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 8.0315, enc_train_entropy: 0.0239, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3600

Step 680, Epoch 9/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8210, enc_train_entropy: 0.0414, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3633

Step 685, Epoch 9/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.8430, enc_train_entropy: 0.0396, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3600train_edge_accuracy_matrix: 0.3567

Step 690, Epoch 9/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9119, enc_train_entropy: 0.0338, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3650

Step 695, Epoch 9/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0044, enc_train_loss: 7.9659, enc_train_entropy: 0.0293, dec_train_loss: 0.0044, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3650

Step 700, Epoch 9/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9440, enc_train_entropy: 0.0311, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3700

Step 705, Epoch 9/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9290, enc_train_entropy: 0.0324, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3683train_edge_accuracy_matrix: 0.3733

Step 710, Epoch 9/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9987, enc_train_entropy: 0.0266, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3700

Step 715, Epoch 9/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9577, enc_train_entropy: 0.0300, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3617train_edge_accuracy_matrix: 0.3650


Epoch 9/20 completed, Global Step: 720
nri_train_loss: 0.0036, enc_train_loss: 7.9577, enc_train_entropy: 0.0300, dec_train_loss: 0.0036, train_edge_accuracy: 0.3617
nri_val_loss: 0.0039, enc_val_loss: 7.8335, enc_val_entropy: 0.0404, dec_val_loss: 0.0039, val_edge_accuracy: 0.3608

---------------------------------------------------------------------------

Step 720, Epoch 10/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.8422, enc_train_entropy: 0.0396, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3500train_edge_accuracy_matrix: 0.3550

Step 725, Epoch 10/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.8632, enc_train_entropy: 0.0379, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3550

Step 730, Epoch 10/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9847, enc_train_entropy: 0.0278, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3733

Step 735, Epoch 10/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9757, enc_train_entropy: 0.0285, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3633

Step 740, Epoch 10/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.9348, enc_train_entropy: 0.0319, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3583train_edge_accuracy_matrix: 0.3617

Step 745, Epoch 10/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9594, enc_train_entropy: 0.0299, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3733

Step 750, Epoch 10/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.8991, enc_train_entropy: 0.0349, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3783train_edge_accuracy_matrix: 0.3767

Step 755, Epoch 10/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0043, enc_train_loss: 7.8262, enc_train_entropy: 0.0410, dec_train_loss: 0.0043, train_edge_accuracy_pred: 0.3583train_edge_accuracy_matrix: 0.3633

Step 760, Epoch 10/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9375, enc_train_entropy: 0.0317, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3550

Step 765, Epoch 10/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 8.0165, enc_train_entropy: 0.0251, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3683train_edge_accuracy_matrix: 0.3667

Step 770, Epoch 10/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.8817, enc_train_entropy: 0.0363, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3650

Step 775, Epoch 10/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 7.9614, enc_train_entropy: 0.0297, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3533

Step 780, Epoch 10/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8434, enc_train_entropy: 0.0395, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3783train_edge_accuracy_matrix: 0.3833

Step 785, Epoch 10/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0040, enc_train_loss: 7.9943, enc_train_entropy: 0.0270, dec_train_loss: 0.0040, train_edge_accuracy_pred: 0.3717train_edge_accuracy_matrix: 0.3700

Step 790, Epoch 10/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 8.0135, enc_train_entropy: 0.0254, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3717

Step 795, Epoch 10/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.8951, enc_train_entropy: 0.0352, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3650


Epoch 10/20 completed, Global Step: 800
nri_train_loss: 0.0036, enc_train_loss: 7.8951, enc_train_entropy: 0.0352, dec_train_loss: 0.0036, train_edge_accuracy: 0.3650
nri_val_loss: 0.0039, enc_val_loss: 8.0234, enc_val_entropy: 0.0245, dec_val_loss: 0.0039, val_edge_accuracy: 0.3680

---------------------------------------------------------------------------

Step 800, Epoch 11/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 8.1062, enc_train_entropy: 0.0176, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3633

Step 805, Epoch 11/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 8.1207, enc_train_entropy: 0.0164, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3800train_edge_accuracy_matrix: 0.3783

Step 810, Epoch 11/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9820, enc_train_entropy: 0.0280, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3633

Step 815, Epoch 11/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.8553, enc_train_entropy: 0.0385, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3750train_edge_accuracy_matrix: 0.3717

Step 820, Epoch 11/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 8.0039, enc_train_entropy: 0.0262, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3550

Step 825, Epoch 11/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 7.9962, enc_train_entropy: 0.0268, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3700train_edge_accuracy_matrix: 0.3700

Step 830, Epoch 11/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.8152, enc_train_entropy: 0.0419, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3500train_edge_accuracy_matrix: 0.3600

Step 835, Epoch 11/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9295, enc_train_entropy: 0.0324, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3600train_edge_accuracy_matrix: 0.3650

Step 840, Epoch 11/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.8925, enc_train_entropy: 0.0354, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3517

Step 845, Epoch 11/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 8.0273, enc_train_entropy: 0.0242, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3717train_edge_accuracy_matrix: 0.3683

Step 850, Epoch 11/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 8.0344, enc_train_entropy: 0.0236, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3717

Step 855, Epoch 11/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9206, enc_train_entropy: 0.0331, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3650

Step 860, Epoch 11/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9746, enc_train_entropy: 0.0286, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3583train_edge_accuracy_matrix: 0.3617

Step 865, Epoch 11/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8862, enc_train_entropy: 0.0360, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3633train_edge_accuracy_matrix: 0.3633

Step 870, Epoch 11/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9117, enc_train_entropy: 0.0338, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3617

Step 875, Epoch 11/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9617, enc_train_entropy: 0.0297, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3500


Epoch 11/20 completed, Global Step: 880
nri_train_loss: 0.0036, enc_train_loss: 7.9617, enc_train_entropy: 0.0297, dec_train_loss: 0.0036, train_edge_accuracy: 0.3550
nri_val_loss: 0.0037, enc_val_loss: 7.9486, enc_val_entropy: 0.0308, dec_val_loss: 0.0037, val_edge_accuracy: 0.3570

---------------------------------------------------------------------------

Step 880, Epoch 12/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8635, enc_train_entropy: 0.0379, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3600train_edge_accuracy_matrix: 0.3633

Step 885, Epoch 12/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9701, enc_train_entropy: 0.0290, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3617

Step 890, Epoch 12/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9262, enc_train_entropy: 0.0326, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3617train_edge_accuracy_matrix: 0.3617

Step 895, Epoch 12/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9223, enc_train_entropy: 0.0330, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3700

Step 900, Epoch 12/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8201, enc_train_entropy: 0.0415, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3450

Step 905, Epoch 12/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9073, enc_train_entropy: 0.0342, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3533

Step 910, Epoch 12/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 7.9986, enc_train_entropy: 0.0266, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3583train_edge_accuracy_matrix: 0.3633

Step 915, Epoch 12/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0041, enc_train_loss: 8.0085, enc_train_entropy: 0.0258, dec_train_loss: 0.0041, train_edge_accuracy_pred: 0.3600train_edge_accuracy_matrix: 0.3617

Step 920, Epoch 12/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.8757, enc_train_entropy: 0.0368, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3417train_edge_accuracy_matrix: 0.3417

Step 925, Epoch 12/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.8870, enc_train_entropy: 0.0359, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3483train_edge_accuracy_matrix: 0.3517

Step 930, Epoch 12/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9339, enc_train_entropy: 0.0320, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3717

Step 935, Epoch 12/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9673, enc_train_entropy: 0.0292, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3700

Step 940, Epoch 12/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.8782, enc_train_entropy: 0.0366, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3600

Step 945, Epoch 12/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 8.0511, enc_train_entropy: 0.0222, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3683train_edge_accuracy_matrix: 0.3667

Step 950, Epoch 12/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9227, enc_train_entropy: 0.0329, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3450

Step 955, Epoch 12/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9160, enc_train_entropy: 0.0335, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3600


Epoch 12/20 completed, Global Step: 960
nri_train_loss: 0.0038, enc_train_loss: 7.9160, enc_train_entropy: 0.0335, dec_train_loss: 0.0038, train_edge_accuracy: 0.3550
nri_val_loss: 0.0039, enc_val_loss: 7.9073, enc_val_entropy: 0.0342, dec_val_loss: 0.0039, val_edge_accuracy: 0.3512

---------------------------------------------------------------------------

Step 960, Epoch 13/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8864, enc_train_entropy: 0.0359, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3533train_edge_accuracy_matrix: 0.3600

Step 965, Epoch 13/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.9406, enc_train_entropy: 0.0314, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3500train_edge_accuracy_matrix: 0.3450

Step 970, Epoch 13/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 8.0288, enc_train_entropy: 0.0241, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3533train_edge_accuracy_matrix: 0.3517

Step 975, Epoch 13/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9486, enc_train_entropy: 0.0308, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3717train_edge_accuracy_matrix: 0.3650

Step 980, Epoch 13/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9259, enc_train_entropy: 0.0327, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3583

Step 985, Epoch 13/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.8732, enc_train_entropy: 0.0370, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3367train_edge_accuracy_matrix: 0.3433

Step 990, Epoch 13/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9757, enc_train_entropy: 0.0285, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3517

Step 995, Epoch 13/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9507, enc_train_entropy: 0.0306, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3517

Step 1000, Epoch 13/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 8.0225, enc_train_entropy: 0.0246, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3600

Step 1005, Epoch 13/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.8967, enc_train_entropy: 0.0351, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3683train_edge_accuracy_matrix: 0.3700

Step 1010, Epoch 13/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.9093, enc_train_entropy: 0.0340, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3583

Step 1015, Epoch 13/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.8834, enc_train_entropy: 0.0362, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3583train_edge_accuracy_matrix: 0.3583

Step 1020, Epoch 13/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9729, enc_train_entropy: 0.0287, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3600train_edge_accuracy_matrix: 0.3517

Step 1025, Epoch 13/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8874, enc_train_entropy: 0.0359, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3617

Step 1030, Epoch 13/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.9514, enc_train_entropy: 0.0305, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3450train_edge_accuracy_matrix: 0.3417

Step 1035, Epoch 13/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 8.0077, enc_train_entropy: 0.0258, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3583train_edge_accuracy_matrix: 0.3617


Epoch 13/20 completed, Global Step: 1040
nri_train_loss: 0.0035, enc_train_loss: 8.0077, enc_train_entropy: 0.0258, dec_train_loss: 0.0035, train_edge_accuracy: 0.3583
nri_val_loss: 0.0036, enc_val_loss: 7.9255, enc_val_entropy: 0.0327, dec_val_loss: 0.0036, val_edge_accuracy: 0.3535

---------------------------------------------------------------------------

Step 1040, Epoch 14/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.9284, enc_train_entropy: 0.0324, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3483train_edge_accuracy_matrix: 0.3533

Step 1045, Epoch 14/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9096, enc_train_entropy: 0.0340, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3467train_edge_accuracy_matrix: 0.3417

Step 1050, Epoch 14/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9559, enc_train_entropy: 0.0302, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3500

Step 1055, Epoch 14/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8317, enc_train_entropy: 0.0405, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3667train_edge_accuracy_matrix: 0.3667

Step 1060, Epoch 14/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 8.0319, enc_train_entropy: 0.0238, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3467train_edge_accuracy_matrix: 0.3500

Step 1065, Epoch 14/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.9041, enc_train_entropy: 0.0345, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3600

Step 1070, Epoch 14/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 8.0294, enc_train_entropy: 0.0240, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3500train_edge_accuracy_matrix: 0.3550

Step 1075, Epoch 14/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.8715, enc_train_entropy: 0.0372, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3450train_edge_accuracy_matrix: 0.3400

Step 1080, Epoch 14/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0039, enc_train_loss: 7.9868, enc_train_entropy: 0.0276, dec_train_loss: 0.0039, train_edge_accuracy_pred: 0.3450train_edge_accuracy_matrix: 0.3517

Step 1085, Epoch 14/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9768, enc_train_entropy: 0.0284, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3583

Step 1090, Epoch 14/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 8.0069, enc_train_entropy: 0.0259, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3500

Step 1095, Epoch 14/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 8.0016, enc_train_entropy: 0.0263, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3600train_edge_accuracy_matrix: 0.3600

Step 1100, Epoch 14/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.9760, enc_train_entropy: 0.0285, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3600

Step 1105, Epoch 14/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.9527, enc_train_entropy: 0.0304, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3483train_edge_accuracy_matrix: 0.3467

Step 1110, Epoch 14/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 8.0003, enc_train_entropy: 0.0265, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3500

Step 1115, Epoch 14/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9285, enc_train_entropy: 0.0324, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3450


Epoch 14/20 completed, Global Step: 1120
nri_train_loss: 0.0038, enc_train_loss: 7.9285, enc_train_entropy: 0.0324, dec_train_loss: 0.0038, train_edge_accuracy: 0.3517
nri_val_loss: 0.0035, enc_val_loss: 7.9248, enc_val_entropy: 0.0327, dec_val_loss: 0.0035, val_edge_accuracy: 0.3400

---------------------------------------------------------------------------

Step 1120, Epoch 15/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.9826, enc_train_entropy: 0.0279, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3400train_edge_accuracy_matrix: 0.3500

Step 1125, Epoch 15/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 8.0126, enc_train_entropy: 0.0254, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3483

Step 1130, Epoch 15/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.8938, enc_train_entropy: 0.0353, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3417train_edge_accuracy_matrix: 0.3500

Step 1135, Epoch 15/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.8124, enc_train_entropy: 0.0421, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3450train_edge_accuracy_matrix: 0.3483

Step 1140, Epoch 15/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.9595, enc_train_entropy: 0.0299, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3267train_edge_accuracy_matrix: 0.3317

Step 1145, Epoch 15/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.9713, enc_train_entropy: 0.0289, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3417train_edge_accuracy_matrix: 0.3417

Step 1150, Epoch 15/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.8804, enc_train_entropy: 0.0364, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3550train_edge_accuracy_matrix: 0.3550

Step 1155, Epoch 15/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.8526, enc_train_entropy: 0.0388, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3617train_edge_accuracy_matrix: 0.3633

Step 1160, Epoch 15/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.8604, enc_train_entropy: 0.0381, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3467train_edge_accuracy_matrix: 0.3500

Step 1165, Epoch 15/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.9593, enc_train_entropy: 0.0299, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3450train_edge_accuracy_matrix: 0.3467

Step 1170, Epoch 15/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 8.0010, enc_train_entropy: 0.0264, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3467train_edge_accuracy_matrix: 0.3517

Step 1175, Epoch 15/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.9739, enc_train_entropy: 0.0287, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3483

Step 1180, Epoch 15/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0032, enc_train_loss: 7.9465, enc_train_entropy: 0.0309, dec_train_loss: 0.0032, train_edge_accuracy_pred: 0.3467train_edge_accuracy_matrix: 0.3467

Step 1185, Epoch 15/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0042, enc_train_loss: 7.9419, enc_train_entropy: 0.0313, dec_train_loss: 0.0042, train_edge_accuracy_pred: 0.3533train_edge_accuracy_matrix: 0.3533

Step 1190, Epoch 15/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 8.0716, enc_train_entropy: 0.0205, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3533

Step 1195, Epoch 15/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9569, enc_train_entropy: 0.0301, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3600train_edge_accuracy_matrix: 0.3567


Epoch 15/20 completed, Global Step: 1200
nri_train_loss: 0.0036, enc_train_loss: 7.9569, enc_train_entropy: 0.0301, dec_train_loss: 0.0036, train_edge_accuracy: 0.3600
nri_val_loss: 0.0035, enc_val_loss: 7.9273, enc_val_entropy: 0.0325, dec_val_loss: 0.0035, val_edge_accuracy: 0.3540

---------------------------------------------------------------------------

Step 1200, Epoch 16/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0030, enc_train_loss: 7.9139, enc_train_entropy: 0.0337, dec_train_loss: 0.0030, train_edge_accuracy_pred: 0.3650train_edge_accuracy_matrix: 0.3700

Step 1205, Epoch 16/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0037, enc_train_loss: 7.8852, enc_train_entropy: 0.0360, dec_train_loss: 0.0037, train_edge_accuracy_pred: 0.3433train_edge_accuracy_matrix: 0.3433

Step 1210, Epoch 16/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.9264, enc_train_entropy: 0.0326, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3400train_edge_accuracy_matrix: 0.3517

Step 1215, Epoch 16/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9010, enc_train_entropy: 0.0347, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3567train_edge_accuracy_matrix: 0.3533

Step 1220, Epoch 16/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.9682, enc_train_entropy: 0.0291, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3467train_edge_accuracy_matrix: 0.3500

Step 1225, Epoch 16/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 8.0161, enc_train_entropy: 0.0251, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3433train_edge_accuracy_matrix: 0.3417

Step 1230, Epoch 16/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.9815, enc_train_entropy: 0.0280, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3333train_edge_accuracy_matrix: 0.3283

Step 1235, Epoch 16/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.9887, enc_train_entropy: 0.0274, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3383train_edge_accuracy_matrix: 0.3350

Step 1240, Epoch 16/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0035, enc_train_loss: 7.7688, enc_train_entropy: 0.0458, dec_train_loss: 0.0035, train_edge_accuracy_pred: 0.3500train_edge_accuracy_matrix: 0.3500

Step 1245, Epoch 16/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0031, enc_train_loss: 7.9849, enc_train_entropy: 0.0277, dec_train_loss: 0.0031, train_edge_accuracy_pred: 0.3450train_edge_accuracy_matrix: 0.3467

Step 1250, Epoch 16/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0032, enc_train_loss: 7.8933, enc_train_entropy: 0.0354, dec_train_loss: 0.0032, train_edge_accuracy_pred: 0.3517train_edge_accuracy_matrix: 0.3500

Step 1255, Epoch 16/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.9154, enc_train_entropy: 0.0335, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3433train_edge_accuracy_matrix: 0.3467

Step 1260, Epoch 16/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.9526, enc_train_entropy: 0.0304, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3400train_edge_accuracy_matrix: 0.3450

Step 1265, Epoch 16/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.9858, enc_train_entropy: 0.0277, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3467train_edge_accuracy_matrix: 0.3450

Step 1270, Epoch 16/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.9464, enc_train_entropy: 0.0309, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3417train_edge_accuracy_matrix: 0.3450

Step 1275, Epoch 16/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0033, enc_train_loss: 7.8241, enc_train_entropy: 0.0411, dec_train_loss: 0.0033, train_edge_accuracy_pred: 0.3367train_edge_accuracy_matrix: 0.3417


Epoch 16/20 completed, Global Step: 1280
nri_train_loss: 0.0033, enc_train_loss: 7.8241, enc_train_entropy: 0.0411, dec_train_loss: 0.0033, train_edge_accuracy: 0.3367
nri_val_loss: 0.0037, enc_val_loss: 7.9183, enc_val_entropy: 0.0333, dec_val_loss: 0.0037, val_edge_accuracy: 0.3265

---------------------------------------------------------------------------

Step 1280, Epoch 17/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0038, enc_train_loss: 7.9324, enc_train_entropy: 0.0321, dec_train_loss: 0.0038, train_edge_accuracy_pred: 0.3350train_edge_accuracy_matrix: 0.3350

Step 1285, Epoch 17/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.8274, enc_train_entropy: 0.0409, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3317train_edge_accuracy_matrix: 0.3367

Step 1290, Epoch 17/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0031, enc_train_loss: 7.8791, enc_train_entropy: 0.0366, dec_train_loss: 0.0031, train_edge_accuracy_pred: 0.3283train_edge_accuracy_matrix: 0.3317

Step 1295, Epoch 17/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0031, enc_train_loss: 7.8184, enc_train_entropy: 0.0416, dec_train_loss: 0.0031, train_edge_accuracy_pred: 0.3133train_edge_accuracy_matrix: 0.3183

Step 1300, Epoch 17/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0032, enc_train_loss: 7.8926, enc_train_entropy: 0.0354, dec_train_loss: 0.0032, train_edge_accuracy_pred: 0.3150train_edge_accuracy_matrix: 0.3200

Step 1305, Epoch 17/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0036, enc_train_loss: 7.8256, enc_train_entropy: 0.0410, dec_train_loss: 0.0036, train_edge_accuracy_pred: 0.3217train_edge_accuracy_matrix: 0.3300

Step 1310, Epoch 17/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0029, enc_train_loss: 7.8628, enc_train_entropy: 0.0379, dec_train_loss: 0.0029, train_edge_accuracy_pred: 0.3350train_edge_accuracy_matrix: 0.3333

Step 1315, Epoch 17/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0030, enc_train_loss: 7.9351, enc_train_entropy: 0.0319, dec_train_loss: 0.0030, train_edge_accuracy_pred: 0.3233train_edge_accuracy_matrix: 0.3233

Step 1320, Epoch 17/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0029, enc_train_loss: 7.8176, enc_train_entropy: 0.0417, dec_train_loss: 0.0029, train_edge_accuracy_pred: 0.3217train_edge_accuracy_matrix: 0.3217

Step 1325, Epoch 17/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0029, enc_train_loss: 7.8672, enc_train_entropy: 0.0376, dec_train_loss: 0.0029, train_edge_accuracy_pred: 0.3300train_edge_accuracy_matrix: 0.3233

Step 1330, Epoch 17/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0031, enc_train_loss: 7.9117, enc_train_entropy: 0.0338, dec_train_loss: 0.0031, train_edge_accuracy_pred: 0.3050train_edge_accuracy_matrix: 0.3150

Step 1335, Epoch 17/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0027, enc_train_loss: 7.9542, enc_train_entropy: 0.0303, dec_train_loss: 0.0027, train_edge_accuracy_pred: 0.3250train_edge_accuracy_matrix: 0.3200

Step 1340, Epoch 17/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0029, enc_train_loss: 7.8472, enc_train_entropy: 0.0392, dec_train_loss: 0.0029, train_edge_accuracy_pred: 0.3217train_edge_accuracy_matrix: 0.3183

Step 1345, Epoch 17/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0030, enc_train_loss: 7.8846, enc_train_entropy: 0.0361, dec_train_loss: 0.0030, train_edge_accuracy_pred: 0.3250train_edge_accuracy_matrix: 0.3150

Step 1350, Epoch 17/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0027, enc_train_loss: 7.9063, enc_train_entropy: 0.0343, dec_train_loss: 0.0027, train_edge_accuracy_pred: 0.3083train_edge_accuracy_matrix: 0.3117

Step 1355, Epoch 17/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0027, enc_train_loss: 7.8610, enc_train_entropy: 0.0381, dec_train_loss: 0.0027, train_edge_accuracy_pred: 0.3017train_edge_accuracy_matrix: 0.3083


Epoch 17/20 completed, Global Step: 1360
nri_train_loss: 0.0027, enc_train_loss: 7.8610, enc_train_entropy: 0.0381, dec_train_loss: 0.0027, train_edge_accuracy: 0.3017
nri_val_loss: 0.0029, enc_val_loss: 7.8945, enc_val_entropy: 0.0353, dec_val_loss: 0.0029, val_edge_accuracy: 0.3148

---------------------------------------------------------------------------

Step 1360, Epoch 18/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0029, enc_train_loss: 7.8683, enc_train_entropy: 0.0375, dec_train_loss: 0.0029, train_edge_accuracy_pred: 0.3200train_edge_accuracy_matrix: 0.3183

Step 1365, Epoch 18/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0030, enc_train_loss: 7.9449, enc_train_entropy: 0.0311, dec_train_loss: 0.0030, train_edge_accuracy_pred: 0.3100train_edge_accuracy_matrix: 0.3150

Step 1370, Epoch 18/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0034, enc_train_loss: 7.8622, enc_train_entropy: 0.0380, dec_train_loss: 0.0034, train_edge_accuracy_pred: 0.3100train_edge_accuracy_matrix: 0.3117

Step 1375, Epoch 18/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0031, enc_train_loss: 7.9021, enc_train_entropy: 0.0346, dec_train_loss: 0.0031, train_edge_accuracy_pred: 0.3150train_edge_accuracy_matrix: 0.3217

Step 1380, Epoch 18/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0026, enc_train_loss: 7.9806, enc_train_entropy: 0.0281, dec_train_loss: 0.0026, train_edge_accuracy_pred: 0.3150train_edge_accuracy_matrix: 0.3133

Step 1385, Epoch 18/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0029, enc_train_loss: 7.8845, enc_train_entropy: 0.0361, dec_train_loss: 0.0029, train_edge_accuracy_pred: 0.3000train_edge_accuracy_matrix: 0.3017

Step 1390, Epoch 18/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0027, enc_train_loss: 7.9038, enc_train_entropy: 0.0345, dec_train_loss: 0.0027, train_edge_accuracy_pred: 0.2967train_edge_accuracy_matrix: 0.2950

Step 1395, Epoch 18/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0026, enc_train_loss: 7.7911, enc_train_entropy: 0.0439, dec_train_loss: 0.0026, train_edge_accuracy_pred: 0.2883train_edge_accuracy_matrix: 0.2933

Step 1400, Epoch 18/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0025, enc_train_loss: 7.9442, enc_train_entropy: 0.0311, dec_train_loss: 0.0025, train_edge_accuracy_pred: 0.3033train_edge_accuracy_matrix: 0.3033

Step 1405, Epoch 18/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0025, enc_train_loss: 7.8952, enc_train_entropy: 0.0352, dec_train_loss: 0.0025, train_edge_accuracy_pred: 0.2900train_edge_accuracy_matrix: 0.2967

Step 1410, Epoch 18/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0021, enc_train_loss: 7.9890, enc_train_entropy: 0.0274, dec_train_loss: 0.0021, train_edge_accuracy_pred: 0.2950train_edge_accuracy_matrix: 0.2917

Step 1415, Epoch 18/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0025, enc_train_loss: 7.9165, enc_train_entropy: 0.0334, dec_train_loss: 0.0025, train_edge_accuracy_pred: 0.2900train_edge_accuracy_matrix: 0.2933

Step 1420, Epoch 18/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0024, enc_train_loss: 7.9219, enc_train_entropy: 0.0330, dec_train_loss: 0.0024, train_edge_accuracy_pred: 0.3050train_edge_accuracy_matrix: 0.3067

Step 1425, Epoch 18/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0022, enc_train_loss: 7.8965, enc_train_entropy: 0.0351, dec_train_loss: 0.0022, train_edge_accuracy_pred: 0.3083train_edge_accuracy_matrix: 0.3017

Step 1430, Epoch 18/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0020, enc_train_loss: 7.9250, enc_train_entropy: 0.0327, dec_train_loss: 0.0020, train_edge_accuracy_pred: 0.2900train_edge_accuracy_matrix: 0.2950

Step 1435, Epoch 18/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0021, enc_train_loss: 7.9691, enc_train_entropy: 0.0291, dec_train_loss: 0.0021, train_edge_accuracy_pred: 0.2967train_edge_accuracy_matrix: 0.3000


Epoch 18/20 completed, Global Step: 1440
nri_train_loss: 0.0021, enc_train_loss: 7.9691, enc_train_entropy: 0.0291, dec_train_loss: 0.0021, train_edge_accuracy: 0.2967
nri_val_loss: 0.0021, enc_val_loss: 7.9060, enc_val_entropy: 0.0343, dec_val_loss: 0.0021, val_edge_accuracy: 0.2898

---------------------------------------------------------------------------

Step 1440, Epoch 19/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0020, enc_train_loss: 8.0189, enc_train_entropy: 0.0249, dec_train_loss: 0.0020, train_edge_accuracy_pred: 0.2950train_edge_accuracy_matrix: 0.2933

Step 1445, Epoch 19/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0021, enc_train_loss: 7.8687, enc_train_entropy: 0.0374, dec_train_loss: 0.0021, train_edge_accuracy_pred: 0.2850train_edge_accuracy_matrix: 0.2883

Step 1450, Epoch 19/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0025, enc_train_loss: 7.8625, enc_train_entropy: 0.0379, dec_train_loss: 0.0025, train_edge_accuracy_pred: 0.3050train_edge_accuracy_matrix: 0.3117

Step 1455, Epoch 19/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0020, enc_train_loss: 7.9491, enc_train_entropy: 0.0307, dec_train_loss: 0.0020, train_edge_accuracy_pred: 0.2950train_edge_accuracy_matrix: 0.2950

Step 1460, Epoch 19/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0022, enc_train_loss: 7.9269, enc_train_entropy: 0.0326, dec_train_loss: 0.0022, train_edge_accuracy_pred: 0.2950train_edge_accuracy_matrix: 0.3067

Step 1465, Epoch 19/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0019, enc_train_loss: 7.9381, enc_train_entropy: 0.0316, dec_train_loss: 0.0019, train_edge_accuracy_pred: 0.2767train_edge_accuracy_matrix: 0.2800

Step 1470, Epoch 19/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0022, enc_train_loss: 7.7583, enc_train_entropy: 0.0466, dec_train_loss: 0.0022, train_edge_accuracy_pred: 0.2733train_edge_accuracy_matrix: 0.2750

Step 1475, Epoch 19/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0019, enc_train_loss: 7.7440, enc_train_entropy: 0.0478, dec_train_loss: 0.0019, train_edge_accuracy_pred: 0.2783train_edge_accuracy_matrix: 0.2800

Step 1480, Epoch 19/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0021, enc_train_loss: 7.7592, enc_train_entropy: 0.0465, dec_train_loss: 0.0021, train_edge_accuracy_pred: 0.2767train_edge_accuracy_matrix: 0.2800

Step 1485, Epoch 19/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0022, enc_train_loss: 7.7581, enc_train_entropy: 0.0466, dec_train_loss: 0.0022, train_edge_accuracy_pred: 0.2567train_edge_accuracy_matrix: 0.2583

Step 1490, Epoch 19/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0021, enc_train_loss: 7.8244, enc_train_entropy: 0.0411, dec_train_loss: 0.0021, train_edge_accuracy_pred: 0.2833train_edge_accuracy_matrix: 0.2817

Step 1495, Epoch 19/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0021, enc_train_loss: 7.7857, enc_train_entropy: 0.0443, dec_train_loss: 0.0021, train_edge_accuracy_pred: 0.2767train_edge_accuracy_matrix: 0.2800

Step 1500, Epoch 19/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0019, enc_train_loss: 7.8014, enc_train_entropy: 0.0430, dec_train_loss: 0.0019, train_edge_accuracy_pred: 0.2667train_edge_accuracy_matrix: 0.2717

Step 1505, Epoch 19/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0021, enc_train_loss: 7.8979, enc_train_entropy: 0.0350, dec_train_loss: 0.0021, train_edge_accuracy_pred: 0.2667train_edge_accuracy_matrix: 0.2733

Step 1510, Epoch 19/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0018, enc_train_loss: 7.9046, enc_train_entropy: 0.0344, dec_train_loss: 0.0018, train_edge_accuracy_pred: 0.2633train_edge_accuracy_matrix: 0.2683

Step 1515, Epoch 19/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0019, enc_train_loss: 7.9022, enc_train_entropy: 0.0346, dec_train_loss: 0.0019, train_edge_accuracy_pred: 0.2667train_edge_accuracy_matrix: 0.2617


Epoch 19/20 completed, Global Step: 1520
nri_train_loss: 0.0019, enc_train_loss: 7.9022, enc_train_entropy: 0.0346, dec_train_loss: 0.0019, train_edge_accuracy: 0.2667
nri_val_loss: 0.0029, enc_val_loss: 7.8817, enc_val_entropy: 0.0363, dec_val_loss: 0.0029, val_edge_accuracy: 0.2632

---------------------------------------------------------------------------

Step 1520, Epoch 20/20, Batch 0/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0031, enc_train_loss: 7.8955, enc_train_entropy: 0.0352, dec_train_loss: 0.0031, train_edge_accuracy_pred: 0.2650train_edge_accuracy_matrix: 0.2633

Step 1525, Epoch 20/20, Batch 5/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0019, enc_train_loss: 7.8691, enc_train_entropy: 0.0374, dec_train_loss: 0.0019, train_edge_accuracy_pred: 0.2533train_edge_accuracy_matrix: 0.2583

Step 1530, Epoch 20/20, Batch 10/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0018, enc_train_loss: 7.7581, enc_train_entropy: 0.0466, dec_train_loss: 0.0018, train_edge_accuracy_pred: 0.2817train_edge_accuracy_matrix: 0.2817

Step 1535, Epoch 20/20, Batch 15/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0016, enc_train_loss: 7.8460, enc_train_entropy: 0.0393, dec_train_loss: 0.0016, train_edge_accuracy_pred: 0.2733train_edge_accuracy_matrix: 0.2633

Step 1540, Epoch 20/20, Batch 20/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0018, enc_train_loss: 7.9027, enc_train_entropy: 0.0346, dec_train_loss: 0.0018, train_edge_accuracy_pred: 0.2833train_edge_accuracy_matrix: 0.2817

Step 1545, Epoch 20/20, Batch 25/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0018, enc_train_loss: 7.9073, enc_train_entropy: 0.0342, dec_train_loss: 0.0018, train_edge_accuracy_pred: 0.2917train_edge_accuracy_matrix: 0.2983

Step 1550, Epoch 20/20, Batch 30/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0030, enc_train_loss: 7.8765, enc_train_entropy: 0.0368, dec_train_loss: 0.0030, train_edge_accuracy_pred: 0.2883train_edge_accuracy_matrix: 0.2833

Step 1555, Epoch 20/20, Batch 35/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0015, enc_train_loss: 7.8805, enc_train_entropy: 0.0364, dec_train_loss: 0.0015, train_edge_accuracy_pred: 0.2767train_edge_accuracy_matrix: 0.2800

Step 1560, Epoch 20/20, Batch 40/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0018, enc_train_loss: 7.8449, enc_train_entropy: 0.0394, dec_train_loss: 0.0018, train_edge_accuracy_pred: 0.2733train_edge_accuracy_matrix: 0.2800

Step 1565, Epoch 20/20, Batch 45/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0015, enc_train_loss: 7.8518, enc_train_entropy: 0.0388, dec_train_loss: 0.0015, train_edge_accuracy_pred: 0.2633train_edge_accuracy_matrix: 0.2650

Step 1570, Epoch 20/20, Batch 50/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0015, enc_train_loss: 7.9433, enc_train_entropy: 0.0312, dec_train_loss: 0.0015, train_edge_accuracy_pred: 0.2700train_edge_accuracy_matrix: 0.2717

Step 1575, Epoch 20/20, Batch 55/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0017, enc_train_loss: 8.0276, enc_train_entropy: 0.0242, dec_train_loss: 0.0017, train_edge_accuracy_pred: 0.2650train_edge_accuracy_matrix: 0.2683

Step 1580, Epoch 20/20, Batch 60/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0018, enc_train_loss: 7.8069, enc_train_entropy: 0.0426, dec_train_loss: 0.0018, train_edge_accuracy_pred: 0.2767train_edge_accuracy_matrix: 0.2850

Step 1585, Epoch 20/20, Batch 65/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0016, enc_train_loss: 7.9283, enc_train_entropy: 0.0325, dec_train_loss: 0.0016, train_edge_accuracy_pred: 0.2750train_edge_accuracy_matrix: 0.2767

Step 1590, Epoch 20/20, Batch 70/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0016, enc_train_loss: 7.9548, enc_train_entropy: 0.0302, dec_train_loss: 0.0016, train_edge_accuracy_pred: 0.2717train_edge_accuracy_matrix: 0.2750

Step 1595, Epoch 20/20, Batch 75/80
temp: 0.5000, beta: 0.0000, 
nri_train_loss: 0.0015, enc_train_loss: 8.0106, enc_train_entropy: 0.0256, dec_train_loss: 0.0015, train_edge_accuracy_pred: 0.2683train_edge_accuracy_matrix: 0.2700


Epoch 20/20 completed, Global Step: 1600
nri_train_loss: 0.0015, enc_train_loss: 8.0106, enc_train_entropy: 0.0256, dec_train_loss: 0.0015, train_edge_accuracy: 0.2683
nri_val_loss: 0.0014, enc_val_loss: 8.0210, enc_val_entropy: 0.0247, dec_val_loss: 0.0014, val_edge_accuracy: 0.2858

---------------------------------------------------------------------------


Training completed in 520.27 seconds or 8.67 minutes or 0.14451891005039214 hours.
Total training steps: 1600

Training completed for model '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10


<<<<<<<<<<<< ENCODER EDGE ACCURACY PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating encoder edge accuracy plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10...

Encoder edge accuracy (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10


<<<<<<<<<<<< ENCODER EDGE ENTROPY PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating encoder edge entropy plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10...

Encoder edge entropy (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.232' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10...

Decoder output plot for rep '1001.2316' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.292' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10...

Decoder output plot for rep '1001.2923' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10


---------------------------------------------------------------------------

TESTING TRAINED NRI MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10\checkpoints:

['best-model-epoch=19-val_loss=0.0000.ckpt']

Trained NRI Model Loaded for testing.

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10\test
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Testing: |                                                                                           | 0/? [00:00<?, ?it/s]
Initializing input processors for encoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Testing:   0%|                                                                                      | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|                                                                         | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|######5                                                          | 1/10 [00:00<00:01,  8.81it/s]Testing DataLoader 0:  20%|#############                                                    | 2/10 [00:00<00:00,  9.26it/s]Testing DataLoader 0:  30%|###################5                                             | 3/10 [00:00<00:00,  9.70it/s]Testing DataLoader 0:  40%|##########################                                       | 4/10 [00:00<00:00,  9.97it/s]Testing DataLoader 0:  50%|################################5                                | 5/10 [00:00<00:00, 10.09it/s]Testing DataLoader 0:  60%|#######################################                          | 6/10 [00:00<00:00,  9.93it/s]Testing DataLoader 0:  70%|#############################################5                   | 7/10 [00:00<00:00,  9.67it/s]Testing DataLoader 0:  80%|####################################################             | 8/10 [00:00<00:00,  9.44it/s]Testing DataLoader 0:  90%|##########################################################5      | 9/10 [00:00<00:00,  9.26it/s]Testing DataLoader 0: 100%|################################################################| 10/10 [00:01<00:00,  9.16it/s]
Testing completed in 1.09 seconds or 0.02 minutes or 0.0003040322330262926 hours.

nri_test_loss: 7.9947, enc_test_loss: 7.9932, dec_test_loss: 0.0015, test_edge_accuracy: 0.2867

Edge predictions are as follows (showing probabilities for each edge type):

Rep 1,001.372:
[[6.49969152e-04 9.99350011e-01]
 [1.01694198e-07 9.99999881e-01]
 [1.45651882e-07 9.99999881e-01]
 [1.00000000e+00 6.69840156e-11]
 [9.99891877e-01 1.08053464e-04]
 [7.28880821e-07 9.99999285e-01]
 [1.00000000e+00 2.11195384e-12]
 [1.00000000e+00 2.94631992e-11]
 [2.77505876e-07 9.99999762e-01]
 [4.51638261e-05 9.99954820e-01]
 [3.61158163e-05 9.99963880e-01]
 [2.60768729e-06 9.99997377e-01]]

Rep 1,001.477:
[[2.71965284e-03 9.97280359e-01]
 [6.97790412e-08 9.99999881e-01]
 [4.26391381e-07 9.99999523e-01]
 [1.00000000e+00 6.71757150e-10]
 [9.99038458e-01 9.61525715e-04]
 [6.89712124e-07 9.99999285e-01]
 [1.00000000e+00 2.66586336e-11]
 [1.00000000e+00 1.53675475e-10]
 [2.56027818e-07 9.99999762e-01]
 [8.08922341e-05 9.99919057e-01]
 [1.19085795e-04 9.99880910e-01]
 [3.22962046e-06 9.99996781e-01]]

Rep 1,001.120:
[[4.6395818e-03 9.9536043e-01]
 [3.6635695e-06 9.9999630e-01]
 [7.9678443e-07 9.9999917e-01]
 [1.0000000e+00 2.1926631e-10]
 [9.9999988e-01 1.3998110e-07]
 [6.4086800e-07 9.9999940e-01]
 [1.0000000e+00 3.9640891e-10]
 [1.0000000e+00 8.9010677e-10]
 [3.3246562e-07 9.9999964e-01]
 [8.7685276e-06 9.9999118e-01]
 [2.7596627e-05 9.9997234e-01]
 [1.9188271e-06 9.9999809e-01]]

Rep 1,001.446:
[[4.3614969e-01 5.6385028e-01]
 [3.3813595e-07 9.9999964e-01]
 [1.8437668e-05 9.9998152e-01]
 [1.0000000e+00 2.8754339e-08]
 [9.9997556e-01 2.4469535e-05]
 [3.7087773e-06 9.9999630e-01]
 [1.0000000e+00 2.1122281e-08]
 [1.0000000e+00 3.1262275e-09]
 [2.5741526e-06 9.9999738e-01]
 [3.4413701e-05 9.9996555e-01]
 [3.2531639e-04 9.9967468e-01]
 [3.7351656e-05 9.9996269e-01]]

Rep 1,001.360:
[[9.7846699e-01 2.1532968e-02]
 [5.7293546e-05 9.9994266e-01]
 [8.2765936e-08 9.9999988e-01]
 [1.0000000e+00 6.4720707e-12]
 [9.9996579e-01 3.4268658e-05]
 [5.6324367e-08 1.0000000e+00]
 [1.0000000e+00 9.3081681e-11]
 [1.0000000e+00 6.1264355e-10]
 [7.4113402e-08 9.9999988e-01]
 [3.2776697e-07 9.9999964e-01]
 [3.5686253e-07 9.9999964e-01]
 [1.2414684e-04 9.9987578e-01]]

Rep 1,001.372:
[[5.8963531e-01 4.1036466e-01]
 [6.1471013e-07 9.9999940e-01]
 [5.4384745e-06 9.9999452e-01]
 [9.9999988e-01 1.0634475e-07]
 [9.9998820e-01 1.1816898e-05]
 [6.0856205e-06 9.9999392e-01]
 [9.9999702e-01 2.9499402e-06]
 [9.9998951e-01 1.0441845e-05]
 [1.4539873e-05 9.9998546e-01]
 [1.8152361e-05 9.9998188e-01]
 [8.2518636e-05 9.9991751e-01]
 [2.3941325e-04 9.9976057e-01]]

Rep 1,001.174:
[[8.9384961e-01 1.0615042e-01]
 [3.1997857e-05 9.9996805e-01]
 [3.9531642e-05 9.9996042e-01]
 [1.0000000e+00 8.1825213e-09]
 [9.9999774e-01 2.2301761e-06]
 [4.7717012e-06 9.9999523e-01]
 [1.0000000e+00 2.6518896e-09]
 [1.0000000e+00 3.6180308e-09]
 [5.3860085e-06 9.9999464e-01]
 [3.6398164e-05 9.9996364e-01]
 [4.0614479e-05 9.9995935e-01]
 [1.7335742e-05 9.9998271e-01]]

Rep 1,001.357:
[[1.5057643e-01 8.4942353e-01]
 [3.3624612e-05 9.9996638e-01]
 [6.2516733e-07 9.9999940e-01]
 [1.0000000e+00 1.2291370e-11]
 [9.9999988e-01 1.4599412e-07]
 [4.9431333e-07 9.9999952e-01]
 [1.0000000e+00 2.9502020e-10]
 [1.0000000e+00 5.3370561e-09]
 [2.9590987e-07 9.9999976e-01]
 [1.0452746e-06 9.9999893e-01]
 [3.6971412e-06 9.9999630e-01]
 [2.1282057e-04 9.9978715e-01]]

Rep 1,001.137:
[[9.9723792e-01 2.7620699e-03]
 [2.7469608e-03 9.9725300e-01]
 [2.4859721e-06 9.9999750e-01]
 [1.0000000e+00 4.0789558e-09]
 [9.9983609e-01 1.6393860e-04]
 [1.7461929e-06 9.9999821e-01]
 [1.0000000e+00 3.8995418e-11]
 [1.0000000e+00 4.2549239e-10]
 [8.0086630e-07 9.9999917e-01]
 [2.6220662e-06 9.9999738e-01]
 [1.4250941e-05 9.9998569e-01]
 [6.2975851e-06 9.9999368e-01]]

Rep 1,001.481:
[[2.4060830e-02 9.7593915e-01]
 [3.9489743e-07 9.9999964e-01]
 [1.8990584e-06 9.9999809e-01]
 [1.0000000e+00 8.8428269e-09]
 [9.9470115e-01 5.2989312e-03]
 [1.2396700e-06 9.9999881e-01]
 [1.0000000e+00 7.4913444e-11]
 [1.0000000e+00 3.6988437e-10]
 [7.1906646e-07 9.9999928e-01]
 [7.0630173e-05 9.9992931e-01]
 [5.1164130e-05 9.9994886e-01]
 [5.9120771e-06 9.9999404e-01]]

Rep 1,001.390:
[[6.7830968e-01 3.2169026e-01]
 [7.1723953e-05 9.9992824e-01]
 [4.7830827e-07 9.9999952e-01]
 [1.0000000e+00 2.5438635e-10]
 [9.9999928e-01 7.1957265e-07]
 [3.8035304e-07 9.9999964e-01]
 [1.0000000e+00 1.0972332e-09]
 [1.0000000e+00 1.6089309e-09]
 [2.2054040e-07 9.9999976e-01]
 [1.0505344e-06 9.9999893e-01]
 [5.6663680e-06 9.9999428e-01]
 [5.7254441e-05 9.9994278e-01]]

Rep 1,001.132:
[[9.9569976e-01 4.3002064e-03]
 [4.0110401e-03 9.9598891e-01]
 [1.3691944e-07 9.9999988e-01]
 [1.0000000e+00 4.2016141e-11]
 [9.9999344e-01 6.5520621e-06]
 [6.5393877e-08 9.9999988e-01]
 [1.0000000e+00 5.1072306e-11]
 [1.0000000e+00 3.4677639e-10]
 [9.8494176e-08 9.9999988e-01]
 [3.2089508e-07 9.9999964e-01]
 [4.3816959e-07 9.9999952e-01]
 [4.1219279e-05 9.9995875e-01]]

Rep 1,001.135:
[[2.7693921e-01 7.2306073e-01]
 [1.7556908e-05 9.9998248e-01]
 [3.3953347e-05 9.9996603e-01]
 [1.0000000e+00 2.1786457e-08]
 [9.9999809e-01 1.8686412e-06]
 [2.4830199e-06 9.9999750e-01]
 [1.0000000e+00 5.2116738e-09]
 [1.0000000e+00 1.2990238e-09]
 [2.7845597e-06 9.9999726e-01]
 [4.0854196e-05 9.9995911e-01]
 [2.0483843e-05 9.9997950e-01]
 [9.1570882e-06 9.9999082e-01]]

Rep 1,001.387:
[[9.4115072e-01 5.8849275e-02]
 [3.3929703e-06 9.9999666e-01]
 [7.3576034e-06 9.9999261e-01]
 [1.0000000e+00 9.2104262e-09]
 [9.9998593e-01 1.4069665e-05]
 [1.0968646e-05 9.9998903e-01]
 [1.0000000e+00 1.3122854e-08]
 [1.0000000e+00 1.8319705e-08]
 [2.2913235e-05 9.9997711e-01]
 [1.4318579e-05 9.9998569e-01]
 [2.0115935e-05 9.9997985e-01]
 [2.3338600e-05 9.9997663e-01]]

Rep 1,001.428:
[[9.3862313e-01 6.1376859e-02]
 [7.0971004e-03 9.9290293e-01]
 [6.1953722e-07 9.9999940e-01]
 [1.0000000e+00 3.7716891e-10]
 [9.9999857e-01 1.4372126e-06]
 [1.7495253e-07 9.9999988e-01]
 [1.0000000e+00 5.4441004e-11]
 [1.0000000e+00 1.8915318e-10]
 [1.8860547e-07 9.9999976e-01]
 [1.0523663e-06 9.9999893e-01]
 [4.8985089e-07 9.9999952e-01]
 [4.7387371e-06 9.9999523e-01]]

Rep 1,001.438:
[[8.6161608e-01 1.3838390e-01]
 [3.8539406e-07 9.9999964e-01]
 [8.9905334e-06 9.9999106e-01]
 [9.9999988e-01 7.9287489e-08]
 [9.9999571e-01 4.2899101e-06]
 [2.0109728e-06 9.9999797e-01]
 [9.9999940e-01 6.2695489e-07]
 [9.9999988e-01 1.4100205e-07]
 [1.7768115e-06 9.9999821e-01]
 [1.6182026e-05 9.9998379e-01]
 [3.7307912e-04 9.9962687e-01]
 [1.2959290e-04 9.9987042e-01]]

Rep 1,001.718:
[[9.22639906e-01 7.73601234e-02]
 [1.78266982e-05 9.99982119e-01]
 [7.69209549e-08 9.99999881e-01]
 [1.00000000e+00 9.10371171e-13]
 [9.99956608e-01 4.33709138e-05]
 [1.44217708e-07 9.99999881e-01]
 [1.00000000e+00 1.65526291e-11]
 [1.00000000e+00 5.86212079e-10]
 [1.03681735e-07 9.99999881e-01]
 [1.26776376e-06 9.99998689e-01]
 [6.96459381e-07 9.99999285e-01]
 [1.26802333e-05 9.99987364e-01]]

Rep 1,001.233:
[[9.9115598e-01 8.8439975e-03]
 [1.6478962e-05 9.9998355e-01]
 [1.7320655e-05 9.9998271e-01]
 [1.0000000e+00 2.7984344e-08]
 [9.9999475e-01 5.2845658e-06]
 [1.8486701e-06 9.9999809e-01]
 [9.9999988e-01 9.2882296e-08]
 [9.9999893e-01 1.0137403e-06]
 [2.5931943e-06 9.9999738e-01]
 [2.4406749e-05 9.9997556e-01]
 [7.3515796e-05 9.9992645e-01]
 [3.5162440e-05 9.9996483e-01]]

Rep 1,001.454:
[[8.1140208e-01 1.8859798e-01]
 [7.6176906e-07 9.9999928e-01]
 [3.3584030e-08 1.0000000e+00]
 [1.0000000e+00 1.3541044e-12]
 [9.9574476e-01 4.2552170e-03]
 [9.9273294e-08 9.9999988e-01]
 [1.0000000e+00 4.7876384e-13]
 [1.0000000e+00 4.1589100e-11]
 [5.1341271e-08 1.0000000e+00]
 [7.1002296e-06 9.9999285e-01]
 [9.3053910e-07 9.9999905e-01]
 [2.2955514e-06 9.9999774e-01]]

Rep 1,001.319:
[[5.4087859e-01 4.5912141e-01]
 [5.0538056e-06 9.9999499e-01]
 [1.3276104e-07 9.9999988e-01]
 [1.0000000e+00 1.4172648e-10]
 [9.9515897e-01 4.8410436e-03]
 [1.4231870e-07 9.9999988e-01]
 [1.0000000e+00 3.7569808e-12]
 [1.0000000e+00 5.9433930e-11]
 [4.9741928e-08 1.0000000e+00]
 [7.5291887e-06 9.9999249e-01]
 [2.8588561e-06 9.9999714e-01]
 [1.0257016e-06 9.9999893e-01]]

Rep 1,001.203:
[[9.9796486e-01 2.0351724e-03]
 [1.6109018e-02 9.8389095e-01]
 [3.0384192e-06 9.9999702e-01]
 [1.0000000e+00 2.0649795e-08]
 [9.9933833e-01 6.6168583e-04]
 [1.1914686e-06 9.9999881e-01]
 [1.0000000e+00 5.1483574e-11]
 [1.0000000e+00 6.3769803e-09]
 [3.2249591e-07 9.9999964e-01]
 [3.0015144e-06 9.9999702e-01]
 [8.2779643e-06 9.9999177e-01]
 [2.5699443e-05 9.9997425e-01]]

Rep 1,001.175:
[[8.7249005e-01 1.2750994e-01]
 [3.6406936e-06 9.9999630e-01]
 [6.2885110e-06 9.9999368e-01]
 [9.9999988e-01 9.3837450e-08]
 [9.9999988e-01 1.2990175e-07]
 [2.0836933e-06 9.9999797e-01]
 [9.9999821e-01 1.7454270e-06]
 [1.0000000e+00 4.6649582e-08]
 [1.7677725e-06 9.9999821e-01]
 [9.7400898e-06 9.9999022e-01]
 [2.3869959e-04 9.9976128e-01]
 [2.6427160e-05 9.9997354e-01]]

Rep 1,001.455:
[[1.7631918e-01 8.2368076e-01]
 [6.6619828e-07 9.9999928e-01]
 [1.9196748e-06 9.9999809e-01]
 [1.0000000e+00 6.1639933e-09]
 [9.9999964e-01 3.6075792e-07]
 [5.3023400e-06 9.9999475e-01]
 [9.9999976e-01 2.1626815e-07]
 [9.9999964e-01 2.9948100e-07]
 [3.3072995e-06 9.9999666e-01]
 [9.7436759e-06 9.9999022e-01]
 [3.8846923e-05 9.9996114e-01]
 [9.0268426e-05 9.9990976e-01]]

Rep 1,001.484:
[[6.1912793e-01 3.8087204e-01]
 [5.5780663e-07 9.9999940e-01]
 [4.0987616e-05 9.9995899e-01]
 [1.0000000e+00 3.4357313e-08]
 [9.9993169e-01 6.8279340e-05]
 [7.2263060e-06 9.9999273e-01]
 [1.0000000e+00 1.3703930e-08]
 [1.0000000e+00 7.2581119e-09]
 [8.0549044e-06 9.9999189e-01]
 [5.5064695e-05 9.9994493e-01]
 [1.4146772e-04 9.9985850e-01]
 [7.4318363e-05 9.9992573e-01]]

Rep 1,001.178:
[[1.5637510e-02 9.8436248e-01]
 [2.3410023e-05 9.9997663e-01]
 [1.0947556e-06 9.9999893e-01]
 [1.0000000e+00 1.3622154e-08]
 [1.0000000e+00 7.4059536e-09]
 [9.3600397e-07 9.9999905e-01]
 [9.9999928e-01 6.8701866e-07]
 [9.9999988e-01 1.3627226e-07]
 [7.6140009e-07 9.9999928e-01]
 [6.7866713e-06 9.9999321e-01]
 [2.4235582e-05 9.9997580e-01]
 [3.4452940e-05 9.9996555e-01]]

Rep 1,001.362:
[[9.9344009e-01 6.5599252e-03]
 [2.3141529e-05 9.9997687e-01]
 [6.4296096e-08 9.9999988e-01]
 [1.0000000e+00 6.8197991e-12]
 [9.9959463e-01 4.0539465e-04]
 [6.0118431e-08 9.9999988e-01]
 [1.0000000e+00 4.8875959e-11]
 [1.0000000e+00 2.9399111e-10]
 [5.1194007e-08 1.0000000e+00]
 [3.5482438e-07 9.9999964e-01]
 [2.8388442e-07 9.9999976e-01]
 [6.7830057e-05 9.9993217e-01]]

Rep 1,001.001:
[[4.70673025e-01 5.29326975e-01]
 [6.15716306e-03 9.93842840e-01]
 [3.07435300e-07 9.99999642e-01]
 [1.00000000e+00 5.32773714e-10]
 [9.99998093e-01 1.85958709e-06]
 [3.27499805e-07 9.99999642e-01]
 [1.00000000e+00 1.28421351e-12]
 [1.00000000e+00 5.33884548e-10]
 [1.55467745e-07 9.99999881e-01]
 [3.76874141e-07 9.99999642e-01]
 [1.79977087e-05 9.99981999e-01]
 [1.38023925e-05 9.99986172e-01]]

Rep 1,001.321:
[[3.9413646e-02 9.6058643e-01]
 [5.1538478e-07 9.9999952e-01]
 [1.5680587e-08 1.0000000e+00]
 [1.0000000e+00 4.6723731e-13]
 [9.9990678e-01 9.3245108e-05]
 [4.2548717e-08 1.0000000e+00]
 [1.0000000e+00 2.3056494e-13]
 [1.0000000e+00 1.4487526e-11]
 [2.1017552e-08 1.0000000e+00]
 [3.2478190e-06 9.9999678e-01]
 [3.6833633e-06 9.9999630e-01]
 [4.0677037e-06 9.9999595e-01]]

Rep 1,001.168:
[[7.8595724e-05 9.9992144e-01]
 [8.1087272e-08 9.9999988e-01]
 [3.4550844e-08 1.0000000e+00]
 [1.0000000e+00 6.0601817e-13]
 [9.9999952e-01 4.8253617e-07]
 [9.3942397e-08 9.9999988e-01]
 [1.0000000e+00 5.7700867e-12]
 [1.0000000e+00 6.2838110e-11]
 [3.8056129e-08 1.0000000e+00]
 [1.6178568e-05 9.9998379e-01]
 [5.7123983e-05 9.9994290e-01]
 [4.3572081e-06 9.9999559e-01]]

Rep 1,001.147:
[[9.9628085e-01 3.7192116e-03]
 [1.7821143e-03 9.9821788e-01]
 [7.7150460e-08 9.9999988e-01]
 [1.0000000e+00 5.7501657e-09]
 [9.3662828e-01 6.3371696e-02]
 [4.7413408e-08 1.0000000e+00]
 [1.0000000e+00 5.6076384e-13]
 [1.0000000e+00 5.1285694e-11]
 [1.5443284e-08 1.0000000e+00]
 [2.8977461e-06 9.9999714e-01]
 [2.6364992e-06 9.9999738e-01]
 [1.3042651e-06 9.9999869e-01]]

Rep 1,001.434:
[[9.53923941e-01 4.60760742e-02]
 [4.95864356e-07 9.99999523e-01]
 [5.27205657e-06 9.99994755e-01]
 [1.00000000e+00 3.63332227e-08]
 [9.99515533e-01 4.84480639e-04]
 [1.64998928e-05 9.99983549e-01]
 [1.00000000e+00 5.62769031e-08]
 [1.00000000e+00 2.92500246e-08]
 [2.68074000e-05 9.99973178e-01]
 [1.37847655e-05 9.99986172e-01]
 [3.41089908e-05 9.99965906e-01]
 [4.68913568e-05 9.99953151e-01]]

Rep 1,001.415:
[[3.0848539e-01 6.9151461e-01]
 [1.7597657e-06 9.9999821e-01]
 [7.7261927e-07 9.9999928e-01]
 [1.0000000e+00 2.2888475e-08]
 [9.9964380e-01 3.5622614e-04]
 [1.2847190e-07 9.9999988e-01]
 [1.0000000e+00 8.7505798e-09]
 [1.0000000e+00 9.3286703e-09]
 [5.0181420e-08 1.0000000e+00]
 [2.4843062e-05 9.9997520e-01]
 [1.3879738e-04 9.9986124e-01]
 [3.4350483e-06 9.9999654e-01]]

Rep 1,001.144:
[[9.8180503e-01 1.8194931e-02]
 [1.3975721e-05 9.9998605e-01]
 [5.8963110e-06 9.9999416e-01]
 [9.9999976e-01 2.0617294e-07]
 [9.9790323e-01 2.0967247e-03]
 [5.3781050e-06 9.9999464e-01]
 [9.9999785e-01 2.1119195e-06]
 [9.9999726e-01 2.6822299e-06]
 [3.8813969e-06 9.9999607e-01]
 [2.5212383e-05 9.9997473e-01]
 [8.3233783e-05 9.9991679e-01]
 [5.0536026e-05 9.9994946e-01]]

Rep 1,001.313:
[[8.08178261e-02 9.19182181e-01]
 [9.12229382e-07 9.99999046e-01]
 [2.62115600e-06 9.99997377e-01]
 [9.99999881e-01 6.05344965e-08]
 [9.99999881e-01 1.14824786e-07]
 [4.75727029e-06 9.99995232e-01]
 [9.99998331e-01 1.65453525e-06]
 [9.99999046e-01 9.19350782e-07]
 [7.70174120e-06 9.99992251e-01]
 [9.72373709e-06 9.99990225e-01]
 [9.49828973e-05 9.99904990e-01]
 [9.02828033e-05 9.99909759e-01]]

Rep 1,001.136:
[[3.3783785e-01 6.6216218e-01]
 [1.1839495e-03 9.9881613e-01]
 [2.6853834e-07 9.9999976e-01]
 [1.0000000e+00 7.6012903e-12]
 [9.9999952e-01 4.9088584e-07]
 [5.4760540e-07 9.9999940e-01]
 [1.0000000e+00 9.3562354e-13]
 [1.0000000e+00 5.1084509e-10]
 [2.5578598e-07 9.9999976e-01]
 [1.3128133e-06 9.9999869e-01]
 [2.9954749e-06 9.9999702e-01]
 [3.8700096e-06 9.9999619e-01]]

Rep 1,001.231:
[[9.95235384e-01 4.76460578e-03]
 [1.18324815e-05 9.99988198e-01]
 [1.04621786e-05 9.99989510e-01]
 [1.00000000e+00 3.08376045e-08]
 [9.99692082e-01 3.07983748e-04]
 [4.83096937e-06 9.99995112e-01]
 [1.00000000e+00 4.12171381e-08]
 [9.99999642e-01 3.16105712e-07]
 [3.17887066e-06 9.99996781e-01]
 [2.24848300e-05 9.99977469e-01]
 [3.47096757e-05 9.99965310e-01]
 [4.75796332e-05 9.99952435e-01]]

Rep 1,001.404:
[[4.5932084e-02 9.5406795e-01]
 [5.9289141e-07 9.9999940e-01]
 [2.3390035e-06 9.9999762e-01]
 [1.0000000e+00 6.6893056e-09]
 [9.9996102e-01 3.9005252e-05]
 [6.7731759e-07 9.9999928e-01]
 [1.0000000e+00 1.5812790e-09]
 [1.0000000e+00 1.7517868e-08]
 [6.9323357e-07 9.9999928e-01]
 [1.7381366e-05 9.9998260e-01]
 [2.6507574e-05 9.9997354e-01]
 [4.7588810e-06 9.9999523e-01]]

Rep 1,001.165:
[[9.9809605e-01 1.9039423e-03]
 [2.0313276e-04 9.9979693e-01]
 [8.4548518e-08 9.9999988e-01]
 [1.0000000e+00 1.0395560e-11]
 [9.9925798e-01 7.4197492e-04]
 [7.4224168e-08 9.9999988e-01]
 [1.0000000e+00 1.3331720e-12]
 [1.0000000e+00 4.0844484e-11]
 [5.9262295e-08 1.0000000e+00]
 [1.7600225e-06 9.9999821e-01]
 [6.8798641e-07 9.9999928e-01]
 [6.0801212e-06 9.9999392e-01]]

Rep 1,001.192:
[[9.7622824e-01 2.3771750e-02]
 [4.8818688e-06 9.9999511e-01]
 [5.3155895e-06 9.9999464e-01]
 [9.9999988e-01 6.3519522e-08]
 [9.9991691e-01 8.3096900e-05]
 [3.2540941e-06 9.9999678e-01]
 [9.9999821e-01 1.8395095e-06]
 [9.9999738e-01 2.6186176e-06]
 [1.7912597e-06 9.9999821e-01]
 [2.4711453e-05 9.9997532e-01]
 [9.5789874e-05 9.9990416e-01]
 [6.8867339e-05 9.9993110e-01]]

Rep 1,001.944:
[[9.9883646e-01 1.1636148e-03]
 [2.8890159e-04 9.9971110e-01]
 [3.7773443e-05 9.9996221e-01]
 [1.0000000e+00 2.5431433e-08]
 [9.9974114e-01 2.5891181e-04]
 [5.2826003e-06 9.9999475e-01]
 [1.0000000e+00 5.4829364e-08]
 [9.9999833e-01 1.6982598e-06]
 [7.0832075e-06 9.9999297e-01]
 [5.8657177e-05 9.9994135e-01]
 [2.1874372e-05 9.9997818e-01]
 [2.9472654e-05 9.9997056e-01]]

Rep 1,001.323:
[[6.20342910e-01 3.79657090e-01]
 [8.91949583e-07 9.99999166e-01]
 [4.20811966e-06 9.99995828e-01]
 [9.99999881e-01 1.10420444e-07]
 [9.99997258e-01 2.78312586e-06]
 [5.35370054e-06 9.99994636e-01]
 [9.99997735e-01 2.31742456e-06]
 [9.99998569e-01 1.41799035e-06]
 [9.27273595e-06 9.99990702e-01]
 [1.15198045e-05 9.99988437e-01]
 [1.44021687e-04 9.99855995e-01]
 [1.23288031e-04 9.99876738e-01]]

Rep 1,001.186:
[[8.2476735e-01 1.7523263e-01]
 [5.9222203e-04 9.9940777e-01]
 [1.1943685e-06 9.9999881e-01]
 [1.0000000e+00 5.3388455e-10]
 [9.9981600e-01 1.8397655e-04]
 [5.3241672e-07 9.9999952e-01]
 [1.0000000e+00 3.4114443e-13]
 [1.0000000e+00 5.9377581e-12]
 [2.6452636e-07 9.9999976e-01]
 [1.2666371e-05 9.9998736e-01]
 [3.1505165e-06 9.9999690e-01]
 [8.9628469e-07 9.9999905e-01]]

Rep 1,001.481:
[[3.0814251e-01 6.9185752e-01]
 [3.9663777e-07 9.9999964e-01]
 [2.0366986e-08 1.0000000e+00]
 [1.0000000e+00 3.6757401e-14]
 [9.9996936e-01 3.0607331e-05]
 [1.0480451e-07 9.9999988e-01]
 [1.0000000e+00 1.0921531e-13]
 [1.0000000e+00 2.4479207e-11]
 [5.5115819e-08 1.0000000e+00]
 [6.2965769e-06 9.9999368e-01]
 [5.7923717e-07 9.9999940e-01]
 [2.6374271e-06 9.9999738e-01]]

Rep 1,001.263:
[[2.5768214e-01 7.4231786e-01]
 [3.4627683e-05 9.9996543e-01]
 [2.5767808e-06 9.9999738e-01]
 [1.0000000e+00 7.3057199e-10]
 [1.0000000e+00 1.6302028e-08]
 [1.7925857e-06 9.9999821e-01]
 [9.9999988e-01 1.4506536e-07]
 [9.9999332e-01 6.6414696e-06]
 [1.3224096e-06 9.9999869e-01]
 [9.6797721e-06 9.9999034e-01]
 [3.1816722e-05 9.9996817e-01]
 [3.4710203e-05 9.9996531e-01]]

Rep 1,001.262:
[[1.1232863e-01 8.8767135e-01]
 [5.8908932e-07 9.9999940e-01]
 [5.2504811e-06 9.9999475e-01]
 [1.0000000e+00 5.6658354e-09]
 [1.0000000e+00 3.8216154e-08]
 [1.8482083e-06 9.9999809e-01]
 [1.0000000e+00 4.3994664e-08]
 [1.0000000e+00 1.0220440e-08]
 [1.3494245e-06 9.9999869e-01]
 [6.9573580e-06 9.9999309e-01]
 [6.6439017e-05 9.9993360e-01]
 [7.4335703e-06 9.9999261e-01]]

Rep 1,001.200:
[[9.85840559e-01 1.41594140e-02]
 [2.55576288e-03 9.97444272e-01]
 [5.99624570e-07 9.99999404e-01]
 [1.00000000e+00 4.14117352e-10]
 [9.99998093e-01 1.94182530e-06]
 [4.47043988e-07 9.99999523e-01]
 [1.00000000e+00 3.80947451e-09]
 [1.00000000e+00 6.29587360e-09]
 [2.84434549e-07 9.99999762e-01]
 [1.01268631e-06 9.99999046e-01]
 [1.81181622e-06 9.99998212e-01]
 [1.30185945e-05 9.99987006e-01]]

Rep 1,001.206:
[[5.5309659e-01 4.4690341e-01]
 [1.3804512e-05 9.9998617e-01]
 [1.1667172e-05 9.9998832e-01]
 [1.0000000e+00 3.4323779e-08]
 [9.9458140e-01 5.4185642e-03]
 [1.4367713e-06 9.9999857e-01]
 [1.0000000e+00 2.3920416e-10]
 [1.0000000e+00 7.0057971e-10]
 [7.9531242e-07 9.9999917e-01]
 [8.4689258e-05 9.9991536e-01]
 [3.7897778e-05 9.9996209e-01]
 [5.0201038e-06 9.9999499e-01]]

Rep 1,001.331:
[[4.8319039e-01 5.1680958e-01]
 [1.2066736e-04 9.9987936e-01]
 [2.8527779e-07 9.9999976e-01]
 [1.0000000e+00 9.3496400e-12]
 [9.9999988e-01 1.4989649e-07]
 [2.0363807e-07 9.9999976e-01]
 [1.0000000e+00 5.0366024e-11]
 [1.0000000e+00 3.5457767e-10]
 [2.0380477e-07 9.9999976e-01]
 [4.3873911e-07 9.9999952e-01]
 [1.3022565e-06 9.9999869e-01]
 [1.2922159e-04 9.9987078e-01]]

Rep 1,001.742:
[[9.48761880e-01 5.12381755e-02]
 [7.70348357e-03 9.92296517e-01]
 [2.03558266e-05 9.99979615e-01]
 [1.00000000e+00 7.02335079e-10]
 [1.00000000e+00 1.27944375e-08]
 [4.88179012e-06 9.99995112e-01]
 [1.00000000e+00 6.44568043e-09]
 [9.99999642e-01 4.12878023e-07]
 [8.36871459e-06 9.99991655e-01]
 [3.13534656e-05 9.99968648e-01]
 [2.20666243e-05 9.99977946e-01]
 [1.62464439e-05 9.99983788e-01]]

Rep 1,001.610:
[[9.9341351e-01 6.5865545e-03]
 [2.8248576e-03 9.9717510e-01]
 [6.1356332e-07 9.9999940e-01]
 [1.0000000e+00 9.4108462e-12]
 [9.9999583e-01 4.2029415e-06]
 [3.1727663e-07 9.9999964e-01]
 [1.0000000e+00 2.0555530e-11]
 [1.0000000e+00 6.5714528e-10]
 [1.3463327e-07 9.9999988e-01]
 [1.5155904e-06 9.9999845e-01]
 [5.4714133e-07 9.9999940e-01]
 [3.7364935e-06 9.9999630e-01]]

Adjacency matrix from edge pred is as follows:

Rep 1,001.372:
[[[0.00000000e+00 0.00000000e+00]
  [6.49969152e-04 9.99350011e-01]
  [1.01694198e-07 9.99999881e-01]
  [1.45651882e-07 9.99999881e-01]]

 [[1.00000000e+00 6.69840156e-11]
  [0.00000000e+00 0.00000000e+00]
  [9.99891877e-01 1.08053464e-04]
  [7.28880821e-07 9.99999285e-01]]

 [[1.00000000e+00 2.11195384e-12]
  [1.00000000e+00 2.94631992e-11]
  [0.00000000e+00 0.00000000e+00]
  [2.77505876e-07 9.99999762e-01]]

 [[4.51638261e-05 9.99954820e-01]
  [3.61158163e-05 9.99963880e-01]
  [2.60768729e-06 9.99997377e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.477:
[[[0.00000000e+00 0.00000000e+00]
  [2.71965284e-03 9.97280359e-01]
  [6.97790412e-08 9.99999881e-01]
  [4.26391381e-07 9.99999523e-01]]

 [[1.00000000e+00 6.71757150e-10]
  [0.00000000e+00 0.00000000e+00]
  [9.99038458e-01 9.61525715e-04]
  [6.89712124e-07 9.99999285e-01]]

 [[1.00000000e+00 2.66586336e-11]
  [1.00000000e+00 1.53675475e-10]
  [0.00000000e+00 0.00000000e+00]
  [2.56027818e-07 9.99999762e-01]]

 [[8.08922341e-05 9.99919057e-01]
  [1.19085795e-04 9.99880910e-01]
  [3.22962046e-06 9.99996781e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.120:
[[[0.0000000e+00 0.0000000e+00]
  [4.6395818e-03 9.9536043e-01]
  [3.6635695e-06 9.9999630e-01]
  [7.9678443e-07 9.9999917e-01]]

 [[1.0000000e+00 2.1926631e-10]
  [0.0000000e+00 0.0000000e+00]
  [9.9999988e-01 1.3998110e-07]
  [6.4086800e-07 9.9999940e-01]]

 [[1.0000000e+00 3.9640891e-10]
  [1.0000000e+00 8.9010677e-10]
  [0.0000000e+00 0.0000000e+00]
  [3.3246562e-07 9.9999964e-01]]

 [[8.7685276e-06 9.9999118e-01]
  [2.7596627e-05 9.9997234e-01]
  [1.9188271e-06 9.9999809e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.446:
[[[0.0000000e+00 0.0000000e+00]
  [4.3614969e-01 5.6385028e-01]
  [3.3813595e-07 9.9999964e-01]
  [1.8437668e-05 9.9998152e-01]]

 [[1.0000000e+00 2.8754339e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9997556e-01 2.4469535e-05]
  [3.7087773e-06 9.9999630e-01]]

 [[1.0000000e+00 2.1122281e-08]
  [1.0000000e+00 3.1262275e-09]
  [0.0000000e+00 0.0000000e+00]
  [2.5741526e-06 9.9999738e-01]]

 [[3.4413701e-05 9.9996555e-01]
  [3.2531639e-04 9.9967468e-01]
  [3.7351656e-05 9.9996269e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.360:
[[[0.0000000e+00 0.0000000e+00]
  [9.7846699e-01 2.1532968e-02]
  [5.7293546e-05 9.9994266e-01]
  [8.2765936e-08 9.9999988e-01]]

 [[1.0000000e+00 6.4720707e-12]
  [0.0000000e+00 0.0000000e+00]
  [9.9996579e-01 3.4268658e-05]
  [5.6324367e-08 1.0000000e+00]]

 [[1.0000000e+00 9.3081681e-11]
  [1.0000000e+00 6.1264355e-10]
  [0.0000000e+00 0.0000000e+00]
  [7.4113402e-08 9.9999988e-01]]

 [[3.2776697e-07 9.9999964e-01]
  [3.5686253e-07 9.9999964e-01]
  [1.2414684e-04 9.9987578e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.372:
[[[0.0000000e+00 0.0000000e+00]
  [5.8963531e-01 4.1036466e-01]
  [6.1471013e-07 9.9999940e-01]
  [5.4384745e-06 9.9999452e-01]]

 [[9.9999988e-01 1.0634475e-07]
  [0.0000000e+00 0.0000000e+00]
  [9.9998820e-01 1.1816898e-05]
  [6.0856205e-06 9.9999392e-01]]

 [[9.9999702e-01 2.9499402e-06]
  [9.9998951e-01 1.0441845e-05]
  [0.0000000e+00 0.0000000e+00]
  [1.4539873e-05 9.9998546e-01]]

 [[1.8152361e-05 9.9998188e-01]
  [8.2518636e-05 9.9991751e-01]
  [2.3941325e-04 9.9976057e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.174:
[[[0.0000000e+00 0.0000000e+00]
  [8.9384961e-01 1.0615042e-01]
  [3.1997857e-05 9.9996805e-01]
  [3.9531642e-05 9.9996042e-01]]

 [[1.0000000e+00 8.1825213e-09]
  [0.0000000e+00 0.0000000e+00]
  [9.9999774e-01 2.2301761e-06]
  [4.7717012e-06 9.9999523e-01]]

 [[1.0000000e+00 2.6518896e-09]
  [1.0000000e+00 3.6180308e-09]
  [0.0000000e+00 0.0000000e+00]
  [5.3860085e-06 9.9999464e-01]]

 [[3.6398164e-05 9.9996364e-01]
  [4.0614479e-05 9.9995935e-01]
  [1.7335742e-05 9.9998271e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.357:
[[[0.0000000e+00 0.0000000e+00]
  [1.5057643e-01 8.4942353e-01]
  [3.3624612e-05 9.9996638e-01]
  [6.2516733e-07 9.9999940e-01]]

 [[1.0000000e+00 1.2291370e-11]
  [0.0000000e+00 0.0000000e+00]
  [9.9999988e-01 1.4599412e-07]
  [4.9431333e-07 9.9999952e-01]]

 [[1.0000000e+00 2.9502020e-10]
  [1.0000000e+00 5.3370561e-09]
  [0.0000000e+00 0.0000000e+00]
  [2.9590987e-07 9.9999976e-01]]

 [[1.0452746e-06 9.9999893e-01]
  [3.6971412e-06 9.9999630e-01]
  [2.1282057e-04 9.9978715e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.137:
[[[0.0000000e+00 0.0000000e+00]
  [9.9723792e-01 2.7620699e-03]
  [2.7469608e-03 9.9725300e-01]
  [2.4859721e-06 9.9999750e-01]]

 [[1.0000000e+00 4.0789558e-09]
  [0.0000000e+00 0.0000000e+00]
  [9.9983609e-01 1.6393860e-04]
  [1.7461929e-06 9.9999821e-01]]

 [[1.0000000e+00 3.8995418e-11]
  [1.0000000e+00 4.2549239e-10]
  [0.0000000e+00 0.0000000e+00]
  [8.0086630e-07 9.9999917e-01]]

 [[2.6220662e-06 9.9999738e-01]
  [1.4250941e-05 9.9998569e-01]
  [6.2975851e-06 9.9999368e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.481:
[[[0.0000000e+00 0.0000000e+00]
  [2.4060830e-02 9.7593915e-01]
  [3.9489743e-07 9.9999964e-01]
  [1.8990584e-06 9.9999809e-01]]

 [[1.0000000e+00 8.8428269e-09]
  [0.0000000e+00 0.0000000e+00]
  [9.9470115e-01 5.2989312e-03]
  [1.2396700e-06 9.9999881e-01]]

 [[1.0000000e+00 7.4913444e-11]
  [1.0000000e+00 3.6988437e-10]
  [0.0000000e+00 0.0000000e+00]
  [7.1906646e-07 9.9999928e-01]]

 [[7.0630173e-05 9.9992931e-01]
  [5.1164130e-05 9.9994886e-01]
  [5.9120771e-06 9.9999404e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.390:
[[[0.0000000e+00 0.0000000e+00]
  [6.7830968e-01 3.2169026e-01]
  [7.1723953e-05 9.9992824e-01]
  [4.7830827e-07 9.9999952e-01]]

 [[1.0000000e+00 2.5438635e-10]
  [0.0000000e+00 0.0000000e+00]
  [9.9999928e-01 7.1957265e-07]
  [3.8035304e-07 9.9999964e-01]]

 [[1.0000000e+00 1.0972332e-09]
  [1.0000000e+00 1.6089309e-09]
  [0.0000000e+00 0.0000000e+00]
  [2.2054040e-07 9.9999976e-01]]

 [[1.0505344e-06 9.9999893e-01]
  [5.6663680e-06 9.9999428e-01]
  [5.7254441e-05 9.9994278e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.132:
[[[0.0000000e+00 0.0000000e+00]
  [9.9569976e-01 4.3002064e-03]
  [4.0110401e-03 9.9598891e-01]
  [1.3691944e-07 9.9999988e-01]]

 [[1.0000000e+00 4.2016141e-11]
  [0.0000000e+00 0.0000000e+00]
  [9.9999344e-01 6.5520621e-06]
  [6.5393877e-08 9.9999988e-01]]

 [[1.0000000e+00 5.1072306e-11]
  [1.0000000e+00 3.4677639e-10]
  [0.0000000e+00 0.0000000e+00]
  [9.8494176e-08 9.9999988e-01]]

 [[3.2089508e-07 9.9999964e-01]
  [4.3816959e-07 9.9999952e-01]
  [4.1219279e-05 9.9995875e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.135:
[[[0.0000000e+00 0.0000000e+00]
  [2.7693921e-01 7.2306073e-01]
  [1.7556908e-05 9.9998248e-01]
  [3.3953347e-05 9.9996603e-01]]

 [[1.0000000e+00 2.1786457e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9999809e-01 1.8686412e-06]
  [2.4830199e-06 9.9999750e-01]]

 [[1.0000000e+00 5.2116738e-09]
  [1.0000000e+00 1.2990238e-09]
  [0.0000000e+00 0.0000000e+00]
  [2.7845597e-06 9.9999726e-01]]

 [[4.0854196e-05 9.9995911e-01]
  [2.0483843e-05 9.9997950e-01]
  [9.1570882e-06 9.9999082e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.387:
[[[0.0000000e+00 0.0000000e+00]
  [9.4115072e-01 5.8849275e-02]
  [3.3929703e-06 9.9999666e-01]
  [7.3576034e-06 9.9999261e-01]]

 [[1.0000000e+00 9.2104262e-09]
  [0.0000000e+00 0.0000000e+00]
  [9.9998593e-01 1.4069665e-05]
  [1.0968646e-05 9.9998903e-01]]

 [[1.0000000e+00 1.3122854e-08]
  [1.0000000e+00 1.8319705e-08]
  [0.0000000e+00 0.0000000e+00]
  [2.2913235e-05 9.9997711e-01]]

 [[1.4318579e-05 9.9998569e-01]
  [2.0115935e-05 9.9997985e-01]
  [2.3338600e-05 9.9997663e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.428:
[[[0.0000000e+00 0.0000000e+00]
  [9.3862313e-01 6.1376859e-02]
  [7.0971004e-03 9.9290293e-01]
  [6.1953722e-07 9.9999940e-01]]

 [[1.0000000e+00 3.7716891e-10]
  [0.0000000e+00 0.0000000e+00]
  [9.9999857e-01 1.4372126e-06]
  [1.7495253e-07 9.9999988e-01]]

 [[1.0000000e+00 5.4441004e-11]
  [1.0000000e+00 1.8915318e-10]
  [0.0000000e+00 0.0000000e+00]
  [1.8860547e-07 9.9999976e-01]]

 [[1.0523663e-06 9.9999893e-01]
  [4.8985089e-07 9.9999952e-01]
  [4.7387371e-06 9.9999523e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.438:
[[[0.0000000e+00 0.0000000e+00]
  [8.6161608e-01 1.3838390e-01]
  [3.8539406e-07 9.9999964e-01]
  [8.9905334e-06 9.9999106e-01]]

 [[9.9999988e-01 7.9287489e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9999571e-01 4.2899101e-06]
  [2.0109728e-06 9.9999797e-01]]

 [[9.9999940e-01 6.2695489e-07]
  [9.9999988e-01 1.4100205e-07]
  [0.0000000e+00 0.0000000e+00]
  [1.7768115e-06 9.9999821e-01]]

 [[1.6182026e-05 9.9998379e-01]
  [3.7307912e-04 9.9962687e-01]
  [1.2959290e-04 9.9987042e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.718:
[[[0.00000000e+00 0.00000000e+00]
  [9.22639906e-01 7.73601234e-02]
  [1.78266982e-05 9.99982119e-01]
  [7.69209549e-08 9.99999881e-01]]

 [[1.00000000e+00 9.10371171e-13]
  [0.00000000e+00 0.00000000e+00]
  [9.99956608e-01 4.33709138e-05]
  [1.44217708e-07 9.99999881e-01]]

 [[1.00000000e+00 1.65526291e-11]
  [1.00000000e+00 5.86212079e-10]
  [0.00000000e+00 0.00000000e+00]
  [1.03681735e-07 9.99999881e-01]]

 [[1.26776376e-06 9.99998689e-01]
  [6.96459381e-07 9.99999285e-01]
  [1.26802333e-05 9.99987364e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.233:
[[[0.0000000e+00 0.0000000e+00]
  [9.9115598e-01 8.8439975e-03]
  [1.6478962e-05 9.9998355e-01]
  [1.7320655e-05 9.9998271e-01]]

 [[1.0000000e+00 2.7984344e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9999475e-01 5.2845658e-06]
  [1.8486701e-06 9.9999809e-01]]

 [[9.9999988e-01 9.2882296e-08]
  [9.9999893e-01 1.0137403e-06]
  [0.0000000e+00 0.0000000e+00]
  [2.5931943e-06 9.9999738e-01]]

 [[2.4406749e-05 9.9997556e-01]
  [7.3515796e-05 9.9992645e-01]
  [3.5162440e-05 9.9996483e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.454:
[[[0.0000000e+00 0.0000000e+00]
  [8.1140208e-01 1.8859798e-01]
  [7.6176906e-07 9.9999928e-01]
  [3.3584030e-08 1.0000000e+00]]

 [[1.0000000e+00 1.3541044e-12]
  [0.0000000e+00 0.0000000e+00]
  [9.9574476e-01 4.2552170e-03]
  [9.9273294e-08 9.9999988e-01]]

 [[1.0000000e+00 4.7876384e-13]
  [1.0000000e+00 4.1589100e-11]
  [0.0000000e+00 0.0000000e+00]
  [5.1341271e-08 1.0000000e+00]]

 [[7.1002296e-06 9.9999285e-01]
  [9.3053910e-07 9.9999905e-01]
  [2.2955514e-06 9.9999774e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.319:
[[[0.0000000e+00 0.0000000e+00]
  [5.4087859e-01 4.5912141e-01]
  [5.0538056e-06 9.9999499e-01]
  [1.3276104e-07 9.9999988e-01]]

 [[1.0000000e+00 1.4172648e-10]
  [0.0000000e+00 0.0000000e+00]
  [9.9515897e-01 4.8410436e-03]
  [1.4231870e-07 9.9999988e-01]]

 [[1.0000000e+00 3.7569808e-12]
  [1.0000000e+00 5.9433930e-11]
  [0.0000000e+00 0.0000000e+00]
  [4.9741928e-08 1.0000000e+00]]

 [[7.5291887e-06 9.9999249e-01]
  [2.8588561e-06 9.9999714e-01]
  [1.0257016e-06 9.9999893e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.203:
[[[0.0000000e+00 0.0000000e+00]
  [9.9796486e-01 2.0351724e-03]
  [1.6109018e-02 9.8389095e-01]
  [3.0384192e-06 9.9999702e-01]]

 [[1.0000000e+00 2.0649795e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9933833e-01 6.6168583e-04]
  [1.1914686e-06 9.9999881e-01]]

 [[1.0000000e+00 5.1483574e-11]
  [1.0000000e+00 6.3769803e-09]
  [0.0000000e+00 0.0000000e+00]
  [3.2249591e-07 9.9999964e-01]]

 [[3.0015144e-06 9.9999702e-01]
  [8.2779643e-06 9.9999177e-01]
  [2.5699443e-05 9.9997425e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.175:
[[[0.0000000e+00 0.0000000e+00]
  [8.7249005e-01 1.2750994e-01]
  [3.6406936e-06 9.9999630e-01]
  [6.2885110e-06 9.9999368e-01]]

 [[9.9999988e-01 9.3837450e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9999988e-01 1.2990175e-07]
  [2.0836933e-06 9.9999797e-01]]

 [[9.9999821e-01 1.7454270e-06]
  [1.0000000e+00 4.6649582e-08]
  [0.0000000e+00 0.0000000e+00]
  [1.7677725e-06 9.9999821e-01]]

 [[9.7400898e-06 9.9999022e-01]
  [2.3869959e-04 9.9976128e-01]
  [2.6427160e-05 9.9997354e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.455:
[[[0.0000000e+00 0.0000000e+00]
  [1.7631918e-01 8.2368076e-01]
  [6.6619828e-07 9.9999928e-01]
  [1.9196748e-06 9.9999809e-01]]

 [[1.0000000e+00 6.1639933e-09]
  [0.0000000e+00 0.0000000e+00]
  [9.9999964e-01 3.6075792e-07]
  [5.3023400e-06 9.9999475e-01]]

 [[9.9999976e-01 2.1626815e-07]
  [9.9999964e-01 2.9948100e-07]
  [0.0000000e+00 0.0000000e+00]
  [3.3072995e-06 9.9999666e-01]]

 [[9.7436759e-06 9.9999022e-01]
  [3.8846923e-05 9.9996114e-01]
  [9.0268426e-05 9.9990976e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.484:
[[[0.0000000e+00 0.0000000e+00]
  [6.1912793e-01 3.8087204e-01]
  [5.5780663e-07 9.9999940e-01]
  [4.0987616e-05 9.9995899e-01]]

 [[1.0000000e+00 3.4357313e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9993169e-01 6.8279340e-05]
  [7.2263060e-06 9.9999273e-01]]

 [[1.0000000e+00 1.3703930e-08]
  [1.0000000e+00 7.2581119e-09]
  [0.0000000e+00 0.0000000e+00]
  [8.0549044e-06 9.9999189e-01]]

 [[5.5064695e-05 9.9994493e-01]
  [1.4146772e-04 9.9985850e-01]
  [7.4318363e-05 9.9992573e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.178:
[[[0.0000000e+00 0.0000000e+00]
  [1.5637510e-02 9.8436248e-01]
  [2.3410023e-05 9.9997663e-01]
  [1.0947556e-06 9.9999893e-01]]

 [[1.0000000e+00 1.3622154e-08]
  [0.0000000e+00 0.0000000e+00]
  [1.0000000e+00 7.4059536e-09]
  [9.3600397e-07 9.9999905e-01]]

 [[9.9999928e-01 6.8701866e-07]
  [9.9999988e-01 1.3627226e-07]
  [0.0000000e+00 0.0000000e+00]
  [7.6140009e-07 9.9999928e-01]]

 [[6.7866713e-06 9.9999321e-01]
  [2.4235582e-05 9.9997580e-01]
  [3.4452940e-05 9.9996555e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.362:
[[[0.0000000e+00 0.0000000e+00]
  [9.9344009e-01 6.5599252e-03]
  [2.3141529e-05 9.9997687e-01]
  [6.4296096e-08 9.9999988e-01]]

 [[1.0000000e+00 6.8197991e-12]
  [0.0000000e+00 0.0000000e+00]
  [9.9959463e-01 4.0539465e-04]
  [6.0118431e-08 9.9999988e-01]]

 [[1.0000000e+00 4.8875959e-11]
  [1.0000000e+00 2.9399111e-10]
  [0.0000000e+00 0.0000000e+00]
  [5.1194007e-08 1.0000000e+00]]

 [[3.5482438e-07 9.9999964e-01]
  [2.8388442e-07 9.9999976e-01]
  [6.7830057e-05 9.9993217e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.001:
[[[0.00000000e+00 0.00000000e+00]
  [4.70673025e-01 5.29326975e-01]
  [6.15716306e-03 9.93842840e-01]
  [3.07435300e-07 9.99999642e-01]]

 [[1.00000000e+00 5.32773714e-10]
  [0.00000000e+00 0.00000000e+00]
  [9.99998093e-01 1.85958709e-06]
  [3.27499805e-07 9.99999642e-01]]

 [[1.00000000e+00 1.28421351e-12]
  [1.00000000e+00 5.33884548e-10]
  [0.00000000e+00 0.00000000e+00]
  [1.55467745e-07 9.99999881e-01]]

 [[3.76874141e-07 9.99999642e-01]
  [1.79977087e-05 9.99981999e-01]
  [1.38023925e-05 9.99986172e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.321:
[[[0.0000000e+00 0.0000000e+00]
  [3.9413646e-02 9.6058643e-01]
  [5.1538478e-07 9.9999952e-01]
  [1.5680587e-08 1.0000000e+00]]

 [[1.0000000e+00 4.6723731e-13]
  [0.0000000e+00 0.0000000e+00]
  [9.9990678e-01 9.3245108e-05]
  [4.2548717e-08 1.0000000e+00]]

 [[1.0000000e+00 2.3056494e-13]
  [1.0000000e+00 1.4487526e-11]
  [0.0000000e+00 0.0000000e+00]
  [2.1017552e-08 1.0000000e+00]]

 [[3.2478190e-06 9.9999678e-01]
  [3.6833633e-06 9.9999630e-01]
  [4.0677037e-06 9.9999595e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.168:
[[[0.0000000e+00 0.0000000e+00]
  [7.8595724e-05 9.9992144e-01]
  [8.1087272e-08 9.9999988e-01]
  [3.4550844e-08 1.0000000e+00]]

 [[1.0000000e+00 6.0601817e-13]
  [0.0000000e+00 0.0000000e+00]
  [9.9999952e-01 4.8253617e-07]
  [9.3942397e-08 9.9999988e-01]]

 [[1.0000000e+00 5.7700867e-12]
  [1.0000000e+00 6.2838110e-11]
  [0.0000000e+00 0.0000000e+00]
  [3.8056129e-08 1.0000000e+00]]

 [[1.6178568e-05 9.9998379e-01]
  [5.7123983e-05 9.9994290e-01]
  [4.3572081e-06 9.9999559e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.147:
[[[0.0000000e+00 0.0000000e+00]
  [9.9628085e-01 3.7192116e-03]
  [1.7821143e-03 9.9821788e-01]
  [7.7150460e-08 9.9999988e-01]]

 [[1.0000000e+00 5.7501657e-09]
  [0.0000000e+00 0.0000000e+00]
  [9.3662828e-01 6.3371696e-02]
  [4.7413408e-08 1.0000000e+00]]

 [[1.0000000e+00 5.6076384e-13]
  [1.0000000e+00 5.1285694e-11]
  [0.0000000e+00 0.0000000e+00]
  [1.5443284e-08 1.0000000e+00]]

 [[2.8977461e-06 9.9999714e-01]
  [2.6364992e-06 9.9999738e-01]
  [1.3042651e-06 9.9999869e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.434:
[[[0.00000000e+00 0.00000000e+00]
  [9.53923941e-01 4.60760742e-02]
  [4.95864356e-07 9.99999523e-01]
  [5.27205657e-06 9.99994755e-01]]

 [[1.00000000e+00 3.63332227e-08]
  [0.00000000e+00 0.00000000e+00]
  [9.99515533e-01 4.84480639e-04]
  [1.64998928e-05 9.99983549e-01]]

 [[1.00000000e+00 5.62769031e-08]
  [1.00000000e+00 2.92500246e-08]
  [0.00000000e+00 0.00000000e+00]
  [2.68074000e-05 9.99973178e-01]]

 [[1.37847655e-05 9.99986172e-01]
  [3.41089908e-05 9.99965906e-01]
  [4.68913568e-05 9.99953151e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.415:
[[[0.0000000e+00 0.0000000e+00]
  [3.0848539e-01 6.9151461e-01]
  [1.7597657e-06 9.9999821e-01]
  [7.7261927e-07 9.9999928e-01]]

 [[1.0000000e+00 2.2888475e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9964380e-01 3.5622614e-04]
  [1.2847190e-07 9.9999988e-01]]

 [[1.0000000e+00 8.7505798e-09]
  [1.0000000e+00 9.3286703e-09]
  [0.0000000e+00 0.0000000e+00]
  [5.0181420e-08 1.0000000e+00]]

 [[2.4843062e-05 9.9997520e-01]
  [1.3879738e-04 9.9986124e-01]
  [3.4350483e-06 9.9999654e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.144:
[[[0.0000000e+00 0.0000000e+00]
  [9.8180503e-01 1.8194931e-02]
  [1.3975721e-05 9.9998605e-01]
  [5.8963110e-06 9.9999416e-01]]

 [[9.9999976e-01 2.0617294e-07]
  [0.0000000e+00 0.0000000e+00]
  [9.9790323e-01 2.0967247e-03]
  [5.3781050e-06 9.9999464e-01]]

 [[9.9999785e-01 2.1119195e-06]
  [9.9999726e-01 2.6822299e-06]
  [0.0000000e+00 0.0000000e+00]
  [3.8813969e-06 9.9999607e-01]]

 [[2.5212383e-05 9.9997473e-01]
  [8.3233783e-05 9.9991679e-01]
  [5.0536026e-05 9.9994946e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.313:
[[[0.00000000e+00 0.00000000e+00]
  [8.08178261e-02 9.19182181e-01]
  [9.12229382e-07 9.99999046e-01]
  [2.62115600e-06 9.99997377e-01]]

 [[9.99999881e-01 6.05344965e-08]
  [0.00000000e+00 0.00000000e+00]
  [9.99999881e-01 1.14824786e-07]
  [4.75727029e-06 9.99995232e-01]]

 [[9.99998331e-01 1.65453525e-06]
  [9.99999046e-01 9.19350782e-07]
  [0.00000000e+00 0.00000000e+00]
  [7.70174120e-06 9.99992251e-01]]

 [[9.72373709e-06 9.99990225e-01]
  [9.49828973e-05 9.99904990e-01]
  [9.02828033e-05 9.99909759e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.136:
[[[0.0000000e+00 0.0000000e+00]
  [3.3783785e-01 6.6216218e-01]
  [1.1839495e-03 9.9881613e-01]
  [2.6853834e-07 9.9999976e-01]]

 [[1.0000000e+00 7.6012903e-12]
  [0.0000000e+00 0.0000000e+00]
  [9.9999952e-01 4.9088584e-07]
  [5.4760540e-07 9.9999940e-01]]

 [[1.0000000e+00 9.3562354e-13]
  [1.0000000e+00 5.1084509e-10]
  [0.0000000e+00 0.0000000e+00]
  [2.5578598e-07 9.9999976e-01]]

 [[1.3128133e-06 9.9999869e-01]
  [2.9954749e-06 9.9999702e-01]
  [3.8700096e-06 9.9999619e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.231:
[[[0.00000000e+00 0.00000000e+00]
  [9.95235384e-01 4.76460578e-03]
  [1.18324815e-05 9.99988198e-01]
  [1.04621786e-05 9.99989510e-01]]

 [[1.00000000e+00 3.08376045e-08]
  [0.00000000e+00 0.00000000e+00]
  [9.99692082e-01 3.07983748e-04]
  [4.83096937e-06 9.99995112e-01]]

 [[1.00000000e+00 4.12171381e-08]
  [9.99999642e-01 3.16105712e-07]
  [0.00000000e+00 0.00000000e+00]
  [3.17887066e-06 9.99996781e-01]]

 [[2.24848300e-05 9.99977469e-01]
  [3.47096757e-05 9.99965310e-01]
  [4.75796332e-05 9.99952435e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.404:
[[[0.0000000e+00 0.0000000e+00]
  [4.5932084e-02 9.5406795e-01]
  [5.9289141e-07 9.9999940e-01]
  [2.3390035e-06 9.9999762e-01]]

 [[1.0000000e+00 6.6893056e-09]
  [0.0000000e+00 0.0000000e+00]
  [9.9996102e-01 3.9005252e-05]
  [6.7731759e-07 9.9999928e-01]]

 [[1.0000000e+00 1.5812790e-09]
  [1.0000000e+00 1.7517868e-08]
  [0.0000000e+00 0.0000000e+00]
  [6.9323357e-07 9.9999928e-01]]

 [[1.7381366e-05 9.9998260e-01]
  [2.6507574e-05 9.9997354e-01]
  [4.7588810e-06 9.9999523e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.165:
[[[0.0000000e+00 0.0000000e+00]
  [9.9809605e-01 1.9039423e-03]
  [2.0313276e-04 9.9979693e-01]
  [8.4548518e-08 9.9999988e-01]]

 [[1.0000000e+00 1.0395560e-11]
  [0.0000000e+00 0.0000000e+00]
  [9.9925798e-01 7.4197492e-04]
  [7.4224168e-08 9.9999988e-01]]

 [[1.0000000e+00 1.3331720e-12]
  [1.0000000e+00 4.0844484e-11]
  [0.0000000e+00 0.0000000e+00]
  [5.9262295e-08 1.0000000e+00]]

 [[1.7600225e-06 9.9999821e-01]
  [6.8798641e-07 9.9999928e-01]
  [6.0801212e-06 9.9999392e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.192:
[[[0.0000000e+00 0.0000000e+00]
  [9.7622824e-01 2.3771750e-02]
  [4.8818688e-06 9.9999511e-01]
  [5.3155895e-06 9.9999464e-01]]

 [[9.9999988e-01 6.3519522e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9991691e-01 8.3096900e-05]
  [3.2540941e-06 9.9999678e-01]]

 [[9.9999821e-01 1.8395095e-06]
  [9.9999738e-01 2.6186176e-06]
  [0.0000000e+00 0.0000000e+00]
  [1.7912597e-06 9.9999821e-01]]

 [[2.4711453e-05 9.9997532e-01]
  [9.5789874e-05 9.9990416e-01]
  [6.8867339e-05 9.9993110e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.944:
[[[0.0000000e+00 0.0000000e+00]
  [9.9883646e-01 1.1636148e-03]
  [2.8890159e-04 9.9971110e-01]
  [3.7773443e-05 9.9996221e-01]]

 [[1.0000000e+00 2.5431433e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9974114e-01 2.5891181e-04]
  [5.2826003e-06 9.9999475e-01]]

 [[1.0000000e+00 5.4829364e-08]
  [9.9999833e-01 1.6982598e-06]
  [0.0000000e+00 0.0000000e+00]
  [7.0832075e-06 9.9999297e-01]]

 [[5.8657177e-05 9.9994135e-01]
  [2.1874372e-05 9.9997818e-01]
  [2.9472654e-05 9.9997056e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.323:
[[[0.00000000e+00 0.00000000e+00]
  [6.20342910e-01 3.79657090e-01]
  [8.91949583e-07 9.99999166e-01]
  [4.20811966e-06 9.99995828e-01]]

 [[9.99999881e-01 1.10420444e-07]
  [0.00000000e+00 0.00000000e+00]
  [9.99997258e-01 2.78312586e-06]
  [5.35370054e-06 9.99994636e-01]]

 [[9.99997735e-01 2.31742456e-06]
  [9.99998569e-01 1.41799035e-06]
  [0.00000000e+00 0.00000000e+00]
  [9.27273595e-06 9.99990702e-01]]

 [[1.15198045e-05 9.99988437e-01]
  [1.44021687e-04 9.99855995e-01]
  [1.23288031e-04 9.99876738e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.186:
[[[0.0000000e+00 0.0000000e+00]
  [8.2476735e-01 1.7523263e-01]
  [5.9222203e-04 9.9940777e-01]
  [1.1943685e-06 9.9999881e-01]]

 [[1.0000000e+00 5.3388455e-10]
  [0.0000000e+00 0.0000000e+00]
  [9.9981600e-01 1.8397655e-04]
  [5.3241672e-07 9.9999952e-01]]

 [[1.0000000e+00 3.4114443e-13]
  [1.0000000e+00 5.9377581e-12]
  [0.0000000e+00 0.0000000e+00]
  [2.6452636e-07 9.9999976e-01]]

 [[1.2666371e-05 9.9998736e-01]
  [3.1505165e-06 9.9999690e-01]
  [8.9628469e-07 9.9999905e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.481:
[[[0.0000000e+00 0.0000000e+00]
  [3.0814251e-01 6.9185752e-01]
  [3.9663777e-07 9.9999964e-01]
  [2.0366986e-08 1.0000000e+00]]

 [[1.0000000e+00 3.6757401e-14]
  [0.0000000e+00 0.0000000e+00]
  [9.9996936e-01 3.0607331e-05]
  [1.0480451e-07 9.9999988e-01]]

 [[1.0000000e+00 1.0921531e-13]
  [1.0000000e+00 2.4479207e-11]
  [0.0000000e+00 0.0000000e+00]
  [5.5115819e-08 1.0000000e+00]]

 [[6.2965769e-06 9.9999368e-01]
  [5.7923717e-07 9.9999940e-01]
  [2.6374271e-06 9.9999738e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.263:
[[[0.0000000e+00 0.0000000e+00]
  [2.5768214e-01 7.4231786e-01]
  [3.4627683e-05 9.9996543e-01]
  [2.5767808e-06 9.9999738e-01]]

 [[1.0000000e+00 7.3057199e-10]
  [0.0000000e+00 0.0000000e+00]
  [1.0000000e+00 1.6302028e-08]
  [1.7925857e-06 9.9999821e-01]]

 [[9.9999988e-01 1.4506536e-07]
  [9.9999332e-01 6.6414696e-06]
  [0.0000000e+00 0.0000000e+00]
  [1.3224096e-06 9.9999869e-01]]

 [[9.6797721e-06 9.9999034e-01]
  [3.1816722e-05 9.9996817e-01]
  [3.4710203e-05 9.9996531e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.262:
[[[0.0000000e+00 0.0000000e+00]
  [1.1232863e-01 8.8767135e-01]
  [5.8908932e-07 9.9999940e-01]
  [5.2504811e-06 9.9999475e-01]]

 [[1.0000000e+00 5.6658354e-09]
  [0.0000000e+00 0.0000000e+00]
  [1.0000000e+00 3.8216154e-08]
  [1.8482083e-06 9.9999809e-01]]

 [[1.0000000e+00 4.3994664e-08]
  [1.0000000e+00 1.0220440e-08]
  [0.0000000e+00 0.0000000e+00]
  [1.3494245e-06 9.9999869e-01]]

 [[6.9573580e-06 9.9999309e-01]
  [6.6439017e-05 9.9993360e-01]
  [7.4335703e-06 9.9999261e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.200:
[[[0.00000000e+00 0.00000000e+00]
  [9.85840559e-01 1.41594140e-02]
  [2.55576288e-03 9.97444272e-01]
  [5.99624570e-07 9.99999404e-01]]

 [[1.00000000e+00 4.14117352e-10]
  [0.00000000e+00 0.00000000e+00]
  [9.99998093e-01 1.94182530e-06]
  [4.47043988e-07 9.99999523e-01]]

 [[1.00000000e+00 3.80947451e-09]
  [1.00000000e+00 6.29587360e-09]
  [0.00000000e+00 0.00000000e+00]
  [2.84434549e-07 9.99999762e-01]]

 [[1.01268631e-06 9.99999046e-01]
  [1.81181622e-06 9.99998212e-01]
  [1.30185945e-05 9.99987006e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.206:
[[[0.0000000e+00 0.0000000e+00]
  [5.5309659e-01 4.4690341e-01]
  [1.3804512e-05 9.9998617e-01]
  [1.1667172e-05 9.9998832e-01]]

 [[1.0000000e+00 3.4323779e-08]
  [0.0000000e+00 0.0000000e+00]
  [9.9458140e-01 5.4185642e-03]
  [1.4367713e-06 9.9999857e-01]]

 [[1.0000000e+00 2.3920416e-10]
  [1.0000000e+00 7.0057971e-10]
  [0.0000000e+00 0.0000000e+00]
  [7.9531242e-07 9.9999917e-01]]

 [[8.4689258e-05 9.9991536e-01]
  [3.7897778e-05 9.9996209e-01]
  [5.0201038e-06 9.9999499e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.331:
[[[0.0000000e+00 0.0000000e+00]
  [4.8319039e-01 5.1680958e-01]
  [1.2066736e-04 9.9987936e-01]
  [2.8527779e-07 9.9999976e-01]]

 [[1.0000000e+00 9.3496400e-12]
  [0.0000000e+00 0.0000000e+00]
  [9.9999988e-01 1.4989649e-07]
  [2.0363807e-07 9.9999976e-01]]

 [[1.0000000e+00 5.0366024e-11]
  [1.0000000e+00 3.5457767e-10]
  [0.0000000e+00 0.0000000e+00]
  [2.0380477e-07 9.9999976e-01]]

 [[4.3873911e-07 9.9999952e-01]
  [1.3022565e-06 9.9999869e-01]
  [1.2922159e-04 9.9987078e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.742:
[[[0.00000000e+00 0.00000000e+00]
  [9.48761880e-01 5.12381755e-02]
  [7.70348357e-03 9.92296517e-01]
  [2.03558266e-05 9.99979615e-01]]

 [[1.00000000e+00 7.02335079e-10]
  [0.00000000e+00 0.00000000e+00]
  [1.00000000e+00 1.27944375e-08]
  [4.88179012e-06 9.99995112e-01]]

 [[1.00000000e+00 6.44568043e-09]
  [9.99999642e-01 4.12878023e-07]
  [0.00000000e+00 0.00000000e+00]
  [8.36871459e-06 9.99991655e-01]]

 [[3.13534656e-05 9.99968648e-01]
  [2.20666243e-05 9.99977946e-01]
  [1.62464439e-05 9.99983788e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.610:
[[[0.0000000e+00 0.0000000e+00]
  [9.9341351e-01 6.5865545e-03]
  [2.8248576e-03 9.9717510e-01]
  [6.1356332e-07 9.9999940e-01]]

 [[1.0000000e+00 9.4108462e-12]
  [0.0000000e+00 0.0000000e+00]
  [9.9999583e-01 4.2029415e-06]
  [3.1727663e-07 9.9999964e-01]]

 [[1.0000000e+00 2.0555530e-11]
  [1.0000000e+00 6.5714528e-10]
  [0.0000000e+00 0.0000000e+00]
  [1.3463327e-07 9.9999988e-01]]

 [[1.5155904e-06 9.9999845e-01]
  [5.4714133e-07 9.9999940e-01]
  [3.7364935e-06 9.9999630e-01]
  [0.0000000e+00 0.0000000e+00]]]

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.372' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10...

Decoder output plot for rep '1001.372' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10\test

Testing DataLoader 0: 100%|#########################################################| 10/10 [00:02<00:00,  3.37it/s]       

        Test metric               DataLoader 0        

       dec/test_loss          0.0014601516304537654   
  enc/test_edge_accuracy       0.2866666615009308     
     enc/test_entropy          0.02704564295709133    
       enc/test_loss            7.993218898773193     
       nri/test_loss            7.994678497314453     


===========================================================================

Nri model '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.10' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-17 10:40:59
