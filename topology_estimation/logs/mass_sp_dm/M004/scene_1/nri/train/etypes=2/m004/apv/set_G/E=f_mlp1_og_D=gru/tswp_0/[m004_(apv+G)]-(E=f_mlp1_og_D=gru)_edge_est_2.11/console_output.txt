=== SCRIPT EXECUTION LOG ===
Script: topology_estimation.train.py
Base Name: [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11
Start Time: 2025-09-17 11:47:02
End Time: 2025-09-17 11:56:06

CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel (Cores: 20), Max Frequency: 2300.00 MHz
GPUs Detected: 1
GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, Memory: 4.00 GB
OS: Windows 11 (10.0.26100)

Python Version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]
========================================================================================================================


Starting nri model training...

'Train' type dataset selected:


Dataset selections:
---------------------------------------------
*_(<ds_subtype_num>) <ds_subtype> : [<augments>]_*

- **Healthy configs**
  (1) series_tp    : [OG]

- **Unhealthy configs**

- **Unknown configs**


Node and signal types:
---------------------------------------------
*_(<node_num>) <node_type> : [<signal_types>]_*

  (1) mass_1   : [acc, pos, vel]
  (2) mass_2   : [acc, pos, vel]
  (3) mass_3   : [acc, pos, vel]
  (4) mass_4   : [acc, pos, vel]

Node group name: m004
Signal group name: apv


For ds_type 'OK' and others....
---------------------------------------------
Maximum timesteps across all node types: 500,001

No data interpolation applied.

'fs' is updated in data_config as given in loaded healthy (or unknown) data.
New fs:
[[500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.],
 [500., 500., 500.]]

No exclusive rep numbers found in keys of hfd5 file. Hence, using default rep numbers.


[1 sample = (n_nodes, n_timesteps (window_length), n_dims)]
------------------------------------------------------------
Total samples: 5000 
Train: 4000/4000 [OK=4000, NOK=0, UK=0], Test: 500/500 [OK=500, NOK=0, UK=0], Val: 500/500 [OK=500, NOK=0, UK=0],
Remainder: 0 [OK=0, NOK=0, UK=0]

train_data_loader statistics:
Number of batches: 80
torch.Size([50, 4, 100, 3])  => (batch_size, n_nodes, n_timesteps, n_dims)

test_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

val_data_loader statistics:
Number of batches: 10
torch.Size([50, 4, 100, 3]) 

---------------------------------------------------------------------------

Loading Relation Matrices...

Relation Matrices loaded successfully.

## Relation Matrices Summary 

**Adjacency matrix for input** => shape: (4, 4)
     n1   n2   n3   n4
n1  0.0  1.0  1.0  1.0
n2  1.0  0.0  1.0  1.0
n3  1.0  1.0  0.0  1.0
n4  1.0  1.0  1.0  0.0


**Receiver relation matrix** => shape: (12, 4)
      n1   n2   n3   n4
e12  0.0  1.0  0.0  0.0
e13  0.0  0.0  1.0  0.0
e14  0.0  0.0  0.0  1.0
e21  1.0  0.0  0.0  0.0
e23  0.0  0.0  1.0  0.0
e24  0.0  0.0  0.0  1.0
e31  1.0  0.0  0.0  0.0
e32  0.0  1.0  0.0  0.0
e34  0.0  0.0  0.0  1.0
e41  1.0  0.0  0.0  0.0
e42  0.0  1.0  0.0  0.0
e43  0.0  0.0  1.0  0.0


**Sender relation matrix:** => shape: (12, 4)
      n1   n2   n3   n4
e12  1.0  0.0  0.0  0.0
e13  1.0  0.0  0.0  0.0
e14  1.0  0.0  0.0  0.0
e21  0.0  1.0  0.0  0.0
e23  0.0  1.0  0.0  0.0
e24  0.0  1.0  0.0  0.0
e31  0.0  0.0  1.0  0.0
e32  0.0  0.0  1.0  0.0
e34  0.0  0.0  1.0  0.0
e41  0.0  0.0  0.0  1.0
e42  0.0  0.0  0.0  1.0
e43  0.0  0.0  0.0  1.0


---------------------------------------------------------------------------

<<<<<< ENCODER PARAMETERS >>>>>>
Encoder model parameters:
-------------------------
n_edge_types: 2
is_residual_connection: False
do_prob: {'mlp': 0.0, 'cnn': 0.0}
is_batch_norm: {'mlp': True, 'cnn': False}
is_xavier_weights: True
attention_output_size: 5
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
reduc_config: None
feat_norm: None
pipeline: [['1/node_emd.1', 'mlp'], ['1/pairwise_op', 'concat'], ['1/edge_emd.1.@', 'mlp'], ['2/aggregate', 'mean'], ['2/node_emd.1', 'mlp'], ['2/pairwise_op', 'concat'], ['2/edge_emd.1', 'mlp']]
edge_emb_configs: {'mlp': [[512, 'elu'], [512, 'elu']], 'cnn': [[5, 2, 64], [8]]}
node_emb_configs: {'mlp': [[512, 'elu'], [512, 'elu']], 'cnn': [[5, 2, 64], [8]]}
n_comps: 100
n_dims: 3

<<<<<< DECODER PARAMETERS >>>>>>

Decoder model parameters:
-------------------------
n_edge_types: 2
msg_out_size: 64
edge_mlp_config: [[64, 'tanh'], [64, 'tanh']]
out_mlp_config: [[64, 'relu'], [64, 'relu']]
do_prob: 0
is_batch_norm: False
is_xavier_weights: False
recur_emb_type: gru
domain_config: {'type': 'time', 'cutoff_freq': 0}
raw_data_norm: min_max
feat_configs: []
feat_norm: None
reduc_config: None
n_dims: 3

Decoder run parameters:
-------------------------
is_hard: False
skip_first_edge_type: True
pred_steps: 10
is_burn_in: True
final_pred_steps: 30
is_dynamic_graph: False
show_conf_band: False

Training parameters set to: 
lr_enc=0.0002, 
lr_dec=0.0001, 
final_beta=0.0, 
warmup_frac=0.9, 
optimizer=adam, 
loss_type_encoder=kld, 
loss_type_decoder=mse, 
prior=tensor([0.5000, 0.5000]), 
add_const_kld=True
is_enc_warmup: True, 
warmup_acc_cutoff: 0.85
, 
final_gamma: 0.5, 
warmup_frac_gamma: 0.3


---------------------------------------------------------------------------

NRI Model Initialized with the following configurations:
----- NRI Model Summary -----
-- Encoder Summary
Encoder(
  (emb_fn_dict): ModuleDict(
    (1/node_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=300, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (1/edge_emd1@): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (2/node_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
    (2/edge_emd1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ELU(alpha=1.0)
        (3): Dropout(p=0.0, inplace=False)
        (4): Linear(in_features=512, out_features=512, bias=True)
        (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ELU(alpha=1.0)
      )
    )
  )
  (attention_layer_dict): ModuleDict()
  (output_layer): Linear(in_features=512, out_features=2, bias=True)
)
-- Decoder Summary
Decoder(
  (edge_mlp_fn): ModuleList(
    (0-1): 2 x MLP(
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): Tanh()
        (2): Dropout(p=0, inplace=False)
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): Tanh()
      )
    )
  )
  (recurrent_emb_fn): GRU(
    (input_u): Linear(in_features=3, out_features=64, bias=True)
    (hidden_u): Linear(in_features=64, out_features=64, bias=True)
    (input_r): Linear(in_features=3, out_features=64, bias=True)
    (hidden_r): Linear(in_features=64, out_features=64, bias=True)
    (input_h): Linear(in_features=3, out_features=64, bias=True)
    (hidden_h): Linear(in_features=64, out_features=64, bias=True)
  )
  (mean_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (var_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): ReLU()
    )
  )
  (mean_output_layer): Linear(in_features=64, out_features=3, bias=True)
  (var_output_layer): Linear(in_features=64, out_features=3, bias=True)
)

---------------------------------------------------------------------------

'[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11' already exists in the log path 'C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11'.
(a) Overwrite exsiting version, (b) create new version, (c) stop training (Choose 'a', 'b' or 'c'):  Are you sure you want to remove the '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11? (y/n): Overwrote '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11' from the log path C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11.
Model parameters saved to C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11.

Training environment set. Training will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11

---------------------------------------------------------------------------
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.

Initializing input processors for encoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
Step 0, Epoch 1/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.7248, 
nri_train_loss: 1.7376, enc_train_loss: 2.2042, enc_train_entropy: 0.5095, dec_train_loss: 1.7376, train_edge_accuracy: 0.5067, 

Step 5, Epoch 1/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0052, 
enc_train_warmup_loss: 0.7959, 
nri_train_loss: 1.2336, enc_train_loss: 7.3511, enc_train_entropy: 0.0806, dec_train_loss: 1.2295, train_edge_accuracy: 0.5067, 

Step 10, Epoch 1/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0104, 
enc_train_warmup_loss: 0.8001, 
nri_train_loss: 0.8618, enc_train_loss: 7.8499, enc_train_entropy: 0.0390, dec_train_loss: 0.8535, train_edge_accuracy: 0.5100, 

Step 15, Epoch 1/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0156, 
enc_train_warmup_loss: 0.8072, 
nri_train_loss: 0.5788, enc_train_loss: 7.9038, enc_train_entropy: 0.0345, dec_train_loss: 0.5662, train_edge_accuracy: 0.5017, 

Step 20, Epoch 1/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0208, 
enc_train_warmup_loss: 0.8103, 
nri_train_loss: 0.4000, enc_train_loss: 8.1301, enc_train_entropy: 0.0156, dec_train_loss: 0.3831, train_edge_accuracy: 0.5017, 

Step 25, Epoch 1/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0260, 
enc_train_warmup_loss: 0.7910, 
nri_train_loss: 0.2772, enc_train_loss: 7.5444, enc_train_entropy: 0.0644, dec_train_loss: 0.2566, train_edge_accuracy: 0.5100, 

Step 30, Epoch 1/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0312, 
enc_train_warmup_loss: 0.7370, 
nri_train_loss: 0.1937, enc_train_loss: 7.1285, enc_train_entropy: 0.0991, dec_train_loss: 0.1707, train_edge_accuracy: 0.5633, 

Step 35, Epoch 1/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0365, 
enc_train_warmup_loss: 0.7751, 
nri_train_loss: 0.1458, enc_train_loss: 7.8222, enc_train_entropy: 0.0413, dec_train_loss: 0.1176, train_edge_accuracy: 0.5350, 

Step 40, Epoch 1/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0417, 
enc_train_warmup_loss: 0.7596, 
nri_train_loss: 0.1151, enc_train_loss: 7.4937, enc_train_entropy: 0.0687, dec_train_loss: 0.0834, train_edge_accuracy: 0.5433, 

Step 45, Epoch 1/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0469, 
enc_train_warmup_loss: 0.6935, 
nri_train_loss: 0.0990, enc_train_loss: 7.2621, enc_train_entropy: 0.0880, dec_train_loss: 0.0665, train_edge_accuracy: 0.6083, 

Step 50, Epoch 1/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0521, 
enc_train_warmup_loss: 0.6510, 
nri_train_loss: 0.0849, enc_train_loss: 7.2719, enc_train_entropy: 0.0872, dec_train_loss: 0.0510, train_edge_accuracy: 0.6617, 

Step 55, Epoch 1/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0573, 
enc_train_warmup_loss: 0.6630, 
nri_train_loss: 0.0771, enc_train_loss: 6.9573, enc_train_entropy: 0.1134, dec_train_loss: 0.0391, train_edge_accuracy: 0.6367, 

Step 60, Epoch 1/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0625, 
enc_train_warmup_loss: 0.6113, 
nri_train_loss: 0.0722, enc_train_loss: 7.1826, enc_train_entropy: 0.0946, dec_train_loss: 0.0339, train_edge_accuracy: 0.6983, 

Step 65, Epoch 1/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0677, 
enc_train_warmup_loss: 0.6170, 
nri_train_loss: 0.0726, enc_train_loss: 7.2608, enc_train_entropy: 0.0881, dec_train_loss: 0.0308, train_edge_accuracy: 0.6800, 

Step 70, Epoch 1/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0729, 
enc_train_warmup_loss: 0.5735, 
nri_train_loss: 0.0714, enc_train_loss: 7.3250, enc_train_entropy: 0.0827, dec_train_loss: 0.0296, train_edge_accuracy: 0.7283, 

Step 75, Epoch 1/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0781, 
enc_train_warmup_loss: 0.5546, 
nri_train_loss: 0.0715, enc_train_loss: 7.5038, enc_train_entropy: 0.0678, dec_train_loss: 0.0281, train_edge_accuracy: 0.7633, 


Epoch 1/20 completed, Global Step: 80
nri_train_loss: 0.0715, enc_train_loss: 7.5038, enc_train_warmup_loss: 0.5546, enc_train_entropy: 0.0678, dec_train_loss: 0.0281, train_edge_accuracy: 0.7633
nri_val_loss: 0.0724, enc_val_loss: 7.3817, enc_val_warmup_loss: 0.5315, enc_val_entropy: 0.0780, dec_val_loss: 0.0281, val_edge_accuracy: 0.7817

---------------------------------------------------------------------------

Step 80, Epoch 2/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0833, 
enc_train_warmup_loss: 0.5196, 
nri_train_loss: 0.0718, enc_train_loss: 7.4944, enc_train_entropy: 0.0686, dec_train_loss: 0.0285, train_edge_accuracy: 0.7967, 

Step 85, Epoch 2/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0885, 
enc_train_warmup_loss: 0.4966, 
nri_train_loss: 0.0718, enc_train_loss: 7.1694, enc_train_entropy: 0.0957, dec_train_loss: 0.0278, train_edge_accuracy: 0.8167, 

Step 90, Epoch 2/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0938, 
enc_train_warmup_loss: 0.4760, 
nri_train_loss: 0.0724, enc_train_loss: 7.2331, enc_train_entropy: 0.0904, dec_train_loss: 0.0277, train_edge_accuracy: 0.8350, 

Step 95, Epoch 2/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0990, 
enc_train_warmup_loss: 0.4062, 
nri_train_loss: 0.0670, enc_train_loss: 6.8948, enc_train_entropy: 0.1186, dec_train_loss: 0.0268, train_edge_accuracy: 0.9150, 


Encoder warmup completed at step 96. Encoder warmup disabled for the rest of training.

Step 100, Epoch 2/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0263, enc_train_loss: 6.7299, enc_train_entropy: 0.1323, dec_train_loss: 0.0263, train_edge_accuracy: 0.8967, 

Step 105, Epoch 2/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0250, enc_train_loss: 6.7961, enc_train_entropy: 0.1268, dec_train_loss: 0.0250, train_edge_accuracy: 0.8117, 

Step 110, Epoch 2/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0250, enc_train_loss: 6.8503, enc_train_entropy: 0.1223, dec_train_loss: 0.0250, train_edge_accuracy: 0.7633, 

Step 115, Epoch 2/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0244, enc_train_loss: 7.0457, enc_train_entropy: 0.1060, dec_train_loss: 0.0244, train_edge_accuracy: 0.7400, 

Step 120, Epoch 2/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0239, enc_train_loss: 7.1004, enc_train_entropy: 0.1014, dec_train_loss: 0.0239, train_edge_accuracy: 0.7383, 

Step 125, Epoch 2/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0239, enc_train_loss: 7.0406, enc_train_entropy: 0.1064, dec_train_loss: 0.0239, train_edge_accuracy: 0.7550, 

Step 130, Epoch 2/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0240, enc_train_loss: 7.0833, enc_train_entropy: 0.1029, dec_train_loss: 0.0240, train_edge_accuracy: 0.7483, 

Step 135, Epoch 2/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0234, enc_train_loss: 7.0311, enc_train_entropy: 0.1072, dec_train_loss: 0.0234, train_edge_accuracy: 0.7567, 

Step 140, Epoch 2/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0234, enc_train_loss: 7.0523, enc_train_entropy: 0.1055, dec_train_loss: 0.0234, train_edge_accuracy: 0.7650, 

Step 145, Epoch 2/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0233, enc_train_loss: 7.0756, enc_train_entropy: 0.1035, dec_train_loss: 0.0233, train_edge_accuracy: 0.7417, 

Step 150, Epoch 2/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0229, enc_train_loss: 7.0087, enc_train_entropy: 0.1091, dec_train_loss: 0.0229, train_edge_accuracy: 0.7483, 

Step 155, Epoch 2/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0227, enc_train_loss: 6.9167, enc_train_entropy: 0.1168, dec_train_loss: 0.0227, train_edge_accuracy: 0.7817, 


Epoch 2/20 completed, Global Step: 160
nri_train_loss: 0.0227, enc_train_loss: 6.9167, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.1168, dec_train_loss: 0.0227, train_edge_accuracy: 0.7817
nri_val_loss: 0.0227, enc_val_loss: 7.0164, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.1084, dec_val_loss: 0.0227, val_edge_accuracy: 0.7758

---------------------------------------------------------------------------

Step 160, Epoch 3/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0227, enc_train_loss: 7.0062, enc_train_entropy: 0.1093, dec_train_loss: 0.0227, train_edge_accuracy: 0.7783, 

Step 165, Epoch 3/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0222, enc_train_loss: 7.1044, enc_train_entropy: 0.1011, dec_train_loss: 0.0222, train_edge_accuracy: 0.7683, 

Step 170, Epoch 3/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0224, enc_train_loss: 6.8377, enc_train_entropy: 0.1233, dec_train_loss: 0.0224, train_edge_accuracy: 0.7700, 

Step 175, Epoch 3/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0223, enc_train_loss: 7.0562, enc_train_entropy: 0.1051, dec_train_loss: 0.0223, train_edge_accuracy: 0.7700, 

Step 180, Epoch 3/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0224, enc_train_loss: 7.0591, enc_train_entropy: 0.1049, dec_train_loss: 0.0224, train_edge_accuracy: 0.7667, 

Step 185, Epoch 3/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0219, enc_train_loss: 7.0673, enc_train_entropy: 0.1042, dec_train_loss: 0.0219, train_edge_accuracy: 0.7383, 

Step 190, Epoch 3/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0220, enc_train_loss: 7.0738, enc_train_entropy: 0.1037, dec_train_loss: 0.0220, train_edge_accuracy: 0.7650, 

Step 195, Epoch 3/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0221, enc_train_loss: 6.9763, enc_train_entropy: 0.1118, dec_train_loss: 0.0221, train_edge_accuracy: 0.7500, 

Step 200, Epoch 3/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0218, enc_train_loss: 7.0273, enc_train_entropy: 0.1075, dec_train_loss: 0.0218, train_edge_accuracy: 0.7850, 

Step 205, Epoch 3/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0217, enc_train_loss: 7.0144, enc_train_entropy: 0.1086, dec_train_loss: 0.0217, train_edge_accuracy: 0.7633, 

Step 210, Epoch 3/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0216, enc_train_loss: 6.9621, enc_train_entropy: 0.1130, dec_train_loss: 0.0216, train_edge_accuracy: 0.7350, 

Step 215, Epoch 3/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0216, enc_train_loss: 6.9097, enc_train_entropy: 0.1173, dec_train_loss: 0.0216, train_edge_accuracy: 0.7517, 

Step 220, Epoch 3/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0216, enc_train_loss: 7.0390, enc_train_entropy: 0.1066, dec_train_loss: 0.0216, train_edge_accuracy: 0.7233, 

Step 225, Epoch 3/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0212, enc_train_loss: 6.8806, enc_train_entropy: 0.1198, dec_train_loss: 0.0212, train_edge_accuracy: 0.7450, 

Step 230, Epoch 3/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0212, enc_train_loss: 7.0199, enc_train_entropy: 0.1082, dec_train_loss: 0.0212, train_edge_accuracy: 0.7183, 

Step 235, Epoch 3/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0212, enc_train_loss: 6.9483, enc_train_entropy: 0.1141, dec_train_loss: 0.0212, train_edge_accuracy: 0.7433, 


Epoch 3/20 completed, Global Step: 240
nri_train_loss: 0.0212, enc_train_loss: 6.9483, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.1141, dec_train_loss: 0.0212, train_edge_accuracy: 0.7433
nri_val_loss: 0.0211, enc_val_loss: 7.0017, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.1097, dec_val_loss: 0.0211, val_edge_accuracy: 0.7265

---------------------------------------------------------------------------

Step 240, Epoch 4/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0210, enc_train_loss: 6.9295, enc_train_entropy: 0.1157, dec_train_loss: 0.0210, train_edge_accuracy: 0.7267, 

Step 245, Epoch 4/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0208, enc_train_loss: 7.0070, enc_train_entropy: 0.1092, dec_train_loss: 0.0208, train_edge_accuracy: 0.7017, 

Step 250, Epoch 4/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0211, enc_train_loss: 7.1440, enc_train_entropy: 0.0978, dec_train_loss: 0.0211, train_edge_accuracy: 0.7183, 

Step 255, Epoch 4/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0207, enc_train_loss: 6.8346, enc_train_entropy: 0.1236, dec_train_loss: 0.0207, train_edge_accuracy: 0.7267, 

Step 260, Epoch 4/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0207, enc_train_loss: 6.9496, enc_train_entropy: 0.1140, dec_train_loss: 0.0207, train_edge_accuracy: 0.7017, 

Step 265, Epoch 4/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0208, enc_train_loss: 6.9702, enc_train_entropy: 0.1123, dec_train_loss: 0.0208, train_edge_accuracy: 0.7067, 

Step 270, Epoch 4/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0205, enc_train_loss: 6.9528, enc_train_entropy: 0.1137, dec_train_loss: 0.0205, train_edge_accuracy: 0.6883, 

Step 275, Epoch 4/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0205, enc_train_loss: 7.0564, enc_train_entropy: 0.1051, dec_train_loss: 0.0205, train_edge_accuracy: 0.7000, 

Step 280, Epoch 4/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0203, enc_train_loss: 7.0468, enc_train_entropy: 0.1059, dec_train_loss: 0.0203, train_edge_accuracy: 0.6850, 

Step 285, Epoch 4/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0200, enc_train_loss: 7.0612, enc_train_entropy: 0.1047, dec_train_loss: 0.0200, train_edge_accuracy: 0.7133, 

Step 290, Epoch 4/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0202, enc_train_loss: 7.0605, enc_train_entropy: 0.1048, dec_train_loss: 0.0202, train_edge_accuracy: 0.7083, 

Step 295, Epoch 4/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0201, enc_train_loss: 7.1154, enc_train_entropy: 0.1002, dec_train_loss: 0.0201, train_edge_accuracy: 0.7117, 

Step 300, Epoch 4/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0201, enc_train_loss: 7.0193, enc_train_entropy: 0.1082, dec_train_loss: 0.0201, train_edge_accuracy: 0.7050, 

Step 305, Epoch 4/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0202, enc_train_loss: 6.9963, enc_train_entropy: 0.1101, dec_train_loss: 0.0202, train_edge_accuracy: 0.6750, 

Step 310, Epoch 4/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0200, enc_train_loss: 7.1002, enc_train_entropy: 0.1015, dec_train_loss: 0.0200, train_edge_accuracy: 0.6900, 

Step 315, Epoch 4/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0198, enc_train_loss: 7.1245, enc_train_entropy: 0.0994, dec_train_loss: 0.0198, train_edge_accuracy: 0.6933, 


Epoch 4/20 completed, Global Step: 320
nri_train_loss: 0.0198, enc_train_loss: 7.1245, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0994, dec_train_loss: 0.0198, train_edge_accuracy: 0.6933
nri_val_loss: 0.0199, enc_val_loss: 7.0208, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.1081, dec_val_loss: 0.0199, val_edge_accuracy: 0.6907

---------------------------------------------------------------------------

Step 320, Epoch 5/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0198, enc_train_loss: 7.0642, enc_train_entropy: 0.1045, dec_train_loss: 0.0198, train_edge_accuracy: 0.7033, 

Step 325, Epoch 5/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0198, enc_train_loss: 7.1531, enc_train_entropy: 0.0971, dec_train_loss: 0.0198, train_edge_accuracy: 0.7050, 

Step 330, Epoch 5/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0199, enc_train_loss: 7.0038, enc_train_entropy: 0.1095, dec_train_loss: 0.0199, train_edge_accuracy: 0.6683, 

Step 335, Epoch 5/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0197, enc_train_loss: 6.9414, enc_train_entropy: 0.1147, dec_train_loss: 0.0197, train_edge_accuracy: 0.6950, 

Step 340, Epoch 5/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0197, enc_train_loss: 7.1218, enc_train_entropy: 0.0997, dec_train_loss: 0.0197, train_edge_accuracy: 0.7033, 

Step 345, Epoch 5/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0195, enc_train_loss: 7.1438, enc_train_entropy: 0.0978, dec_train_loss: 0.0195, train_edge_accuracy: 0.6850, 

Step 350, Epoch 5/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0195, enc_train_loss: 6.9212, enc_train_entropy: 0.1164, dec_train_loss: 0.0195, train_edge_accuracy: 0.6700, 

Step 355, Epoch 5/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0193, enc_train_loss: 6.9200, enc_train_entropy: 0.1165, dec_train_loss: 0.0193, train_edge_accuracy: 0.7133, 

Step 360, Epoch 5/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0194, enc_train_loss: 7.0983, enc_train_entropy: 0.1016, dec_train_loss: 0.0194, train_edge_accuracy: 0.6967, 

Step 365, Epoch 5/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0195, enc_train_loss: 7.1215, enc_train_entropy: 0.0997, dec_train_loss: 0.0195, train_edge_accuracy: 0.6883, 

Step 370, Epoch 5/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0195, enc_train_loss: 6.9237, enc_train_entropy: 0.1162, dec_train_loss: 0.0195, train_edge_accuracy: 0.6683, 

Step 375, Epoch 5/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0194, enc_train_loss: 7.1018, enc_train_entropy: 0.1013, dec_train_loss: 0.0194, train_edge_accuracy: 0.7067, 

Step 380, Epoch 5/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0192, enc_train_loss: 7.0926, enc_train_entropy: 0.1021, dec_train_loss: 0.0192, train_edge_accuracy: 0.6783, 

Step 385, Epoch 5/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0191, enc_train_loss: 6.9565, enc_train_entropy: 0.1134, dec_train_loss: 0.0191, train_edge_accuracy: 0.6833, 

Step 390, Epoch 5/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0191, enc_train_loss: 7.0529, enc_train_entropy: 0.1054, dec_train_loss: 0.0191, train_edge_accuracy: 0.6583, 

Step 395, Epoch 5/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0192, enc_train_loss: 6.9808, enc_train_entropy: 0.1114, dec_train_loss: 0.0192, train_edge_accuracy: 0.6833, 


Epoch 5/20 completed, Global Step: 400
nri_train_loss: 0.0192, enc_train_loss: 6.9808, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.1114, dec_train_loss: 0.0192, train_edge_accuracy: 0.6833
nri_val_loss: 0.0191, enc_val_loss: 7.0777, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.1033, dec_val_loss: 0.0191, val_edge_accuracy: 0.6643

---------------------------------------------------------------------------

Step 400, Epoch 6/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0191, enc_train_loss: 7.0152, enc_train_entropy: 0.1085, dec_train_loss: 0.0191, train_edge_accuracy: 0.6950, 

Step 405, Epoch 6/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0190, enc_train_loss: 7.1522, enc_train_entropy: 0.0971, dec_train_loss: 0.0190, train_edge_accuracy: 0.6850, 

Step 410, Epoch 6/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0191, enc_train_loss: 7.0256, enc_train_entropy: 0.1077, dec_train_loss: 0.0191, train_edge_accuracy: 0.6883, 

Step 415, Epoch 6/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0189, enc_train_loss: 7.1918, enc_train_entropy: 0.0938, dec_train_loss: 0.0189, train_edge_accuracy: 0.6767, 

Step 420, Epoch 6/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0188, enc_train_loss: 7.1218, enc_train_entropy: 0.0997, dec_train_loss: 0.0188, train_edge_accuracy: 0.6833, 

Step 425, Epoch 6/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0188, enc_train_loss: 7.0333, enc_train_entropy: 0.1070, dec_train_loss: 0.0188, train_edge_accuracy: 0.6767, 

Step 430, Epoch 6/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0188, enc_train_loss: 7.0715, enc_train_entropy: 0.1039, dec_train_loss: 0.0188, train_edge_accuracy: 0.6650, 

Step 435, Epoch 6/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0186, enc_train_loss: 7.1475, enc_train_entropy: 0.0975, dec_train_loss: 0.0186, train_edge_accuracy: 0.6700, 

Step 440, Epoch 6/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0189, enc_train_loss: 7.1467, enc_train_entropy: 0.0976, dec_train_loss: 0.0189, train_edge_accuracy: 0.6600, 

Step 445, Epoch 6/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0186, enc_train_loss: 7.1285, enc_train_entropy: 0.0991, dec_train_loss: 0.0186, train_edge_accuracy: 0.6667, 

Step 450, Epoch 6/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0184, enc_train_loss: 7.0990, enc_train_entropy: 0.1016, dec_train_loss: 0.0184, train_edge_accuracy: 0.6567, 

Step 455, Epoch 6/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0187, enc_train_loss: 7.0661, enc_train_entropy: 0.1043, dec_train_loss: 0.0187, train_edge_accuracy: 0.6800, 

Step 460, Epoch 6/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0184, enc_train_loss: 7.0706, enc_train_entropy: 0.1039, dec_train_loss: 0.0184, train_edge_accuracy: 0.6750, 

Step 465, Epoch 6/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0185, enc_train_loss: 7.1740, enc_train_entropy: 0.0953, dec_train_loss: 0.0185, train_edge_accuracy: 0.6800, 

Step 470, Epoch 6/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0184, enc_train_loss: 7.0276, enc_train_entropy: 0.1075, dec_train_loss: 0.0184, train_edge_accuracy: 0.6633, 

Step 475, Epoch 6/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0183, enc_train_loss: 7.0505, enc_train_entropy: 0.1056, dec_train_loss: 0.0183, train_edge_accuracy: 0.6750, 


Epoch 6/20 completed, Global Step: 480
nri_train_loss: 0.0183, enc_train_loss: 7.0505, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.1056, dec_train_loss: 0.0183, train_edge_accuracy: 0.6750
nri_val_loss: 0.0183, enc_val_loss: 7.1088, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.1007, dec_val_loss: 0.0183, val_edge_accuracy: 0.6597

---------------------------------------------------------------------------

Step 480, Epoch 7/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0184, enc_train_loss: 7.2141, enc_train_entropy: 0.0920, dec_train_loss: 0.0184, train_edge_accuracy: 0.6483, 

Step 485, Epoch 7/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0183, enc_train_loss: 7.0855, enc_train_entropy: 0.1027, dec_train_loss: 0.0183, train_edge_accuracy: 0.6633, 

Step 490, Epoch 7/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0182, enc_train_loss: 7.1342, enc_train_entropy: 0.0986, dec_train_loss: 0.0182, train_edge_accuracy: 0.6800, 

Step 495, Epoch 7/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0183, enc_train_loss: 7.1161, enc_train_entropy: 0.1001, dec_train_loss: 0.0183, train_edge_accuracy: 0.6617, 

Step 500, Epoch 7/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0178, enc_train_loss: 7.1974, enc_train_entropy: 0.0934, dec_train_loss: 0.0178, train_edge_accuracy: 0.6867, 

Step 505, Epoch 7/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0181, enc_train_loss: 7.2411, enc_train_entropy: 0.0897, dec_train_loss: 0.0181, train_edge_accuracy: 0.6950, 

Step 510, Epoch 7/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0181, enc_train_loss: 7.1689, enc_train_entropy: 0.0957, dec_train_loss: 0.0181, train_edge_accuracy: 0.6567, 

Step 515, Epoch 7/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0180, enc_train_loss: 7.0444, enc_train_entropy: 0.1061, dec_train_loss: 0.0180, train_edge_accuracy: 0.6583, 

Step 520, Epoch 7/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0180, enc_train_loss: 7.1058, enc_train_entropy: 0.1010, dec_train_loss: 0.0180, train_edge_accuracy: 0.6650, 

Step 525, Epoch 7/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0180, enc_train_loss: 7.0897, enc_train_entropy: 0.1023, dec_train_loss: 0.0180, train_edge_accuracy: 0.6867, 

Step 530, Epoch 7/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0181, enc_train_loss: 7.0938, enc_train_entropy: 0.1020, dec_train_loss: 0.0181, train_edge_accuracy: 0.6800, 

Step 535, Epoch 7/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0181, enc_train_loss: 7.1338, enc_train_entropy: 0.0987, dec_train_loss: 0.0181, train_edge_accuracy: 0.6667, 

Step 540, Epoch 7/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0180, enc_train_loss: 7.1342, enc_train_entropy: 0.0986, dec_train_loss: 0.0180, train_edge_accuracy: 0.6617, 

Step 545, Epoch 7/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0181, enc_train_loss: 7.0270, enc_train_entropy: 0.1076, dec_train_loss: 0.0181, train_edge_accuracy: 0.6650, 

Step 550, Epoch 7/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0179, enc_train_loss: 7.1034, enc_train_entropy: 0.1012, dec_train_loss: 0.0179, train_edge_accuracy: 0.6850, 

Step 555, Epoch 7/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0177, enc_train_loss: 7.2339, enc_train_entropy: 0.0903, dec_train_loss: 0.0177, train_edge_accuracy: 0.7050, 


Epoch 7/20 completed, Global Step: 560
nri_train_loss: 0.0177, enc_train_loss: 7.2339, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0903, dec_train_loss: 0.0177, train_edge_accuracy: 0.7050
nri_val_loss: 0.0178, enc_val_loss: 7.1592, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0965, dec_val_loss: 0.0178, val_edge_accuracy: 0.6653

---------------------------------------------------------------------------

Step 560, Epoch 8/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0176, enc_train_loss: 7.1801, enc_train_entropy: 0.0948, dec_train_loss: 0.0176, train_edge_accuracy: 0.6700, 

Step 565, Epoch 8/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0176, enc_train_loss: 7.2569, enc_train_entropy: 0.0884, dec_train_loss: 0.0176, train_edge_accuracy: 0.6883, 

Step 570, Epoch 8/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0175, enc_train_loss: 7.0659, enc_train_entropy: 0.1043, dec_train_loss: 0.0175, train_edge_accuracy: 0.6733, 

Step 575, Epoch 8/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0176, enc_train_loss: 6.9945, enc_train_entropy: 0.1103, dec_train_loss: 0.0176, train_edge_accuracy: 0.6533, 

Step 580, Epoch 8/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0178, enc_train_loss: 7.1676, enc_train_entropy: 0.0959, dec_train_loss: 0.0178, train_edge_accuracy: 0.6867, 

Step 585, Epoch 8/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0175, enc_train_loss: 7.1026, enc_train_entropy: 0.1013, dec_train_loss: 0.0175, train_edge_accuracy: 0.6650, 

Step 590, Epoch 8/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0174, enc_train_loss: 7.1487, enc_train_entropy: 0.0974, dec_train_loss: 0.0174, train_edge_accuracy: 0.6883, 

Step 595, Epoch 8/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0177, enc_train_loss: 7.0975, enc_train_entropy: 0.1017, dec_train_loss: 0.0177, train_edge_accuracy: 0.6850, 

Step 600, Epoch 8/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0176, enc_train_loss: 7.0552, enc_train_entropy: 0.1052, dec_train_loss: 0.0176, train_edge_accuracy: 0.6867, 

Step 605, Epoch 8/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0174, enc_train_loss: 7.1711, enc_train_entropy: 0.0956, dec_train_loss: 0.0174, train_edge_accuracy: 0.6767, 

Step 610, Epoch 8/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0173, enc_train_loss: 7.2361, enc_train_entropy: 0.0901, dec_train_loss: 0.0173, train_edge_accuracy: 0.6950, 

Step 615, Epoch 8/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0176, enc_train_loss: 7.1672, enc_train_entropy: 0.0959, dec_train_loss: 0.0176, train_edge_accuracy: 0.6667, 

Step 620, Epoch 8/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0173, enc_train_loss: 7.0936, enc_train_entropy: 0.1020, dec_train_loss: 0.0173, train_edge_accuracy: 0.6700, 

Step 625, Epoch 8/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0176, enc_train_loss: 7.1905, enc_train_entropy: 0.0939, dec_train_loss: 0.0176, train_edge_accuracy: 0.6833, 

Step 630, Epoch 8/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0175, enc_train_loss: 7.1203, enc_train_entropy: 0.0998, dec_train_loss: 0.0175, train_edge_accuracy: 0.6767, 

Step 635, Epoch 8/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0174, enc_train_loss: 7.2147, enc_train_entropy: 0.0919, dec_train_loss: 0.0174, train_edge_accuracy: 0.6700, 


Epoch 8/20 completed, Global Step: 640
nri_train_loss: 0.0174, enc_train_loss: 7.2147, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0919, dec_train_loss: 0.0174, train_edge_accuracy: 0.6700
nri_val_loss: 0.0173, enc_val_loss: 7.1749, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0952, dec_val_loss: 0.0173, val_edge_accuracy: 0.6767

---------------------------------------------------------------------------

Step 640, Epoch 9/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0171, enc_train_loss: 7.0682, enc_train_entropy: 0.1041, dec_train_loss: 0.0171, train_edge_accuracy: 0.6767, 

Step 645, Epoch 9/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0173, enc_train_loss: 7.2077, enc_train_entropy: 0.0925, dec_train_loss: 0.0173, train_edge_accuracy: 0.6883, 

Step 650, Epoch 9/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1310, enc_train_entropy: 0.0989, dec_train_loss: 0.0170, train_edge_accuracy: 0.6883, 

Step 655, Epoch 9/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0172, enc_train_loss: 7.0271, enc_train_entropy: 0.1076, dec_train_loss: 0.0172, train_edge_accuracy: 0.6667, 

Step 660, Epoch 9/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0171, enc_train_loss: 7.0635, enc_train_entropy: 0.1045, dec_train_loss: 0.0171, train_edge_accuracy: 0.6833, 

Step 665, Epoch 9/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0173, enc_train_loss: 7.1043, enc_train_entropy: 0.1011, dec_train_loss: 0.0173, train_edge_accuracy: 0.7033, 

Step 670, Epoch 9/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0172, enc_train_loss: 7.0916, enc_train_entropy: 0.1022, dec_train_loss: 0.0172, train_edge_accuracy: 0.6817, 

Step 675, Epoch 9/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0172, enc_train_loss: 7.1744, enc_train_entropy: 0.0953, dec_train_loss: 0.0172, train_edge_accuracy: 0.6717, 

Step 680, Epoch 9/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0171, enc_train_loss: 7.2496, enc_train_entropy: 0.0890, dec_train_loss: 0.0171, train_edge_accuracy: 0.6667, 

Step 685, Epoch 9/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0171, enc_train_loss: 7.1833, enc_train_entropy: 0.0945, dec_train_loss: 0.0171, train_edge_accuracy: 0.6717, 

Step 690, Epoch 9/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.0984, enc_train_entropy: 0.1016, dec_train_loss: 0.0170, train_edge_accuracy: 0.6967, 

Step 695, Epoch 9/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0172, enc_train_loss: 7.1674, enc_train_entropy: 0.0959, dec_train_loss: 0.0172, train_edge_accuracy: 0.6950, 

Step 700, Epoch 9/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1245, enc_train_entropy: 0.0994, dec_train_loss: 0.0170, train_edge_accuracy: 0.6817, 

Step 705, Epoch 9/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1674, enc_train_entropy: 0.0959, dec_train_loss: 0.0170, train_edge_accuracy: 0.6967, 

Step 710, Epoch 9/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0171, enc_train_loss: 7.0741, enc_train_entropy: 0.1036, dec_train_loss: 0.0171, train_edge_accuracy: 0.7000, 

Step 715, Epoch 9/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1306, enc_train_entropy: 0.0989, dec_train_loss: 0.0170, train_edge_accuracy: 0.7100, 


Epoch 9/20 completed, Global Step: 720
nri_train_loss: 0.0170, enc_train_loss: 7.1306, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0989, dec_train_loss: 0.0170, train_edge_accuracy: 0.7100
nri_val_loss: 0.0169, enc_val_loss: 7.1717, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0955, dec_val_loss: 0.0169, val_edge_accuracy: 0.6812

---------------------------------------------------------------------------

Step 720, Epoch 10/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0169, enc_train_loss: 7.2144, enc_train_entropy: 0.0920, dec_train_loss: 0.0169, train_edge_accuracy: 0.6867, 

Step 725, Epoch 10/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0169, enc_train_loss: 7.1677, enc_train_entropy: 0.0958, dec_train_loss: 0.0169, train_edge_accuracy: 0.6933, 

Step 730, Epoch 10/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1996, enc_train_entropy: 0.0932, dec_train_loss: 0.0170, train_edge_accuracy: 0.6867, 

Step 735, Epoch 10/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0169, enc_train_loss: 7.2094, enc_train_entropy: 0.0924, dec_train_loss: 0.0169, train_edge_accuracy: 0.6900, 

Step 740, Epoch 10/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0168, enc_train_loss: 7.1766, enc_train_entropy: 0.0951, dec_train_loss: 0.0168, train_edge_accuracy: 0.7167, 

Step 745, Epoch 10/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0169, enc_train_loss: 7.1965, enc_train_entropy: 0.0934, dec_train_loss: 0.0169, train_edge_accuracy: 0.6800, 

Step 750, Epoch 10/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0169, enc_train_loss: 7.2006, enc_train_entropy: 0.0931, dec_train_loss: 0.0169, train_edge_accuracy: 0.6800, 

Step 755, Epoch 10/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 7.1438, enc_train_entropy: 0.0978, dec_train_loss: 0.0166, train_edge_accuracy: 0.6900, 

Step 760, Epoch 10/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1612, enc_train_entropy: 0.0964, dec_train_loss: 0.0170, train_edge_accuracy: 0.6783, 

Step 765, Epoch 10/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0170, enc_train_loss: 7.1171, enc_train_entropy: 0.1001, dec_train_loss: 0.0170, train_edge_accuracy: 0.6950, 

Step 770, Epoch 10/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0167, enc_train_loss: 7.1733, enc_train_entropy: 0.0954, dec_train_loss: 0.0167, train_edge_accuracy: 0.7183, 

Step 775, Epoch 10/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 7.1510, enc_train_entropy: 0.0972, dec_train_loss: 0.0166, train_edge_accuracy: 0.6700, 

Step 780, Epoch 10/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0168, enc_train_loss: 7.1678, enc_train_entropy: 0.0958, dec_train_loss: 0.0168, train_edge_accuracy: 0.6733, 

Step 785, Epoch 10/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0169, enc_train_loss: 7.0550, enc_train_entropy: 0.1052, dec_train_loss: 0.0169, train_edge_accuracy: 0.7067, 

Step 790, Epoch 10/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0167, enc_train_loss: 7.1899, enc_train_entropy: 0.0940, dec_train_loss: 0.0167, train_edge_accuracy: 0.6967, 

Step 795, Epoch 10/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0167, enc_train_loss: 7.1184, enc_train_entropy: 0.0999, dec_train_loss: 0.0167, train_edge_accuracy: 0.6800, 


Epoch 10/20 completed, Global Step: 800
nri_train_loss: 0.0167, enc_train_loss: 7.1184, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0999, dec_train_loss: 0.0167, train_edge_accuracy: 0.6800
nri_val_loss: 0.0167, enc_val_loss: 7.2249, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0911, dec_val_loss: 0.0167, val_edge_accuracy: 0.6770

---------------------------------------------------------------------------

Step 800, Epoch 11/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0168, enc_train_loss: 7.1558, enc_train_entropy: 0.0968, dec_train_loss: 0.0168, train_edge_accuracy: 0.6800, 

Step 805, Epoch 11/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0168, enc_train_loss: 7.0776, enc_train_entropy: 0.1034, dec_train_loss: 0.0168, train_edge_accuracy: 0.6850, 

Step 810, Epoch 11/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0165, enc_train_loss: 7.2067, enc_train_entropy: 0.0926, dec_train_loss: 0.0165, train_edge_accuracy: 0.6683, 

Step 815, Epoch 11/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0167, enc_train_loss: 7.1459, enc_train_entropy: 0.0977, dec_train_loss: 0.0167, train_edge_accuracy: 0.6983, 

Step 820, Epoch 11/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 7.1547, enc_train_entropy: 0.0969, dec_train_loss: 0.0166, train_edge_accuracy: 0.6933, 

Step 825, Epoch 11/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 7.1120, enc_train_entropy: 0.1005, dec_train_loss: 0.0166, train_edge_accuracy: 0.6683, 

Step 830, Epoch 11/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0165, enc_train_loss: 7.1213, enc_train_entropy: 0.0997, dec_train_loss: 0.0165, train_edge_accuracy: 0.6950, 

Step 835, Epoch 11/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0165, enc_train_loss: 7.2611, enc_train_entropy: 0.0881, dec_train_loss: 0.0165, train_edge_accuracy: 0.6750, 

Step 840, Epoch 11/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0167, enc_train_loss: 7.2905, enc_train_entropy: 0.0856, dec_train_loss: 0.0167, train_edge_accuracy: 0.6800, 

Step 845, Epoch 11/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0164, enc_train_loss: 7.3361, enc_train_entropy: 0.0818, dec_train_loss: 0.0164, train_edge_accuracy: 0.6917, 

Step 850, Epoch 11/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0164, enc_train_loss: 7.1994, enc_train_entropy: 0.0932, dec_train_loss: 0.0164, train_edge_accuracy: 0.6867, 

Step 855, Epoch 11/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0165, enc_train_loss: 7.1438, enc_train_entropy: 0.0978, dec_train_loss: 0.0165, train_edge_accuracy: 0.6900, 

Step 860, Epoch 11/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 7.1082, enc_train_entropy: 0.1008, dec_train_loss: 0.0166, train_edge_accuracy: 0.6950, 

Step 865, Epoch 11/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 7.2120, enc_train_entropy: 0.0922, dec_train_loss: 0.0163, train_edge_accuracy: 0.7000, 

Step 870, Epoch 11/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0165, enc_train_loss: 7.1041, enc_train_entropy: 0.1011, dec_train_loss: 0.0165, train_edge_accuracy: 0.7117, 

Step 875, Epoch 11/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 7.1526, enc_train_entropy: 0.0971, dec_train_loss: 0.0163, train_edge_accuracy: 0.6917, 


Epoch 11/20 completed, Global Step: 880
nri_train_loss: 0.0163, enc_train_loss: 7.1526, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0971, dec_train_loss: 0.0163, train_edge_accuracy: 0.6917
nri_val_loss: 0.0164, enc_val_loss: 7.1947, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0936, dec_val_loss: 0.0164, val_edge_accuracy: 0.6840

---------------------------------------------------------------------------

Step 880, Epoch 12/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0164, enc_train_loss: 7.1434, enc_train_entropy: 0.0979, dec_train_loss: 0.0164, train_edge_accuracy: 0.6833, 

Step 885, Epoch 12/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0165, enc_train_loss: 7.1276, enc_train_entropy: 0.0992, dec_train_loss: 0.0165, train_edge_accuracy: 0.6983, 

Step 890, Epoch 12/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.0687, enc_train_entropy: 0.1041, dec_train_loss: 0.0162, train_edge_accuracy: 0.6833, 

Step 895, Epoch 12/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0166, enc_train_loss: 7.0443, enc_train_entropy: 0.1061, dec_train_loss: 0.0166, train_edge_accuracy: 0.6883, 

Step 900, Epoch 12/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 7.1661, enc_train_entropy: 0.0960, dec_train_loss: 0.0163, train_edge_accuracy: 0.6717, 

Step 905, Epoch 12/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 7.2135, enc_train_entropy: 0.0920, dec_train_loss: 0.0163, train_edge_accuracy: 0.6850, 

Step 910, Epoch 12/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.1926, enc_train_entropy: 0.0938, dec_train_loss: 0.0162, train_edge_accuracy: 0.6750, 

Step 915, Epoch 12/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.2995, enc_train_entropy: 0.0849, dec_train_loss: 0.0162, train_edge_accuracy: 0.7083, 

Step 920, Epoch 12/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.1695, enc_train_entropy: 0.0957, dec_train_loss: 0.0162, train_edge_accuracy: 0.6883, 

Step 925, Epoch 12/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.1973, enc_train_entropy: 0.0934, dec_train_loss: 0.0162, train_edge_accuracy: 0.6917, 

Step 930, Epoch 12/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.0689, enc_train_entropy: 0.1041, dec_train_loss: 0.0162, train_edge_accuracy: 0.7083, 

Step 935, Epoch 12/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0161, enc_train_loss: 7.1740, enc_train_entropy: 0.0953, dec_train_loss: 0.0161, train_edge_accuracy: 0.6867, 

Step 940, Epoch 12/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 7.1989, enc_train_entropy: 0.0932, dec_train_loss: 0.0163, train_edge_accuracy: 0.6967, 

Step 945, Epoch 12/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0161, enc_train_loss: 7.1229, enc_train_entropy: 0.0996, dec_train_loss: 0.0161, train_edge_accuracy: 0.6817, 

Step 950, Epoch 12/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.1320, enc_train_entropy: 0.0988, dec_train_loss: 0.0162, train_edge_accuracy: 0.7117, 

Step 955, Epoch 12/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0160, enc_train_loss: 7.0457, enc_train_entropy: 0.1060, dec_train_loss: 0.0160, train_edge_accuracy: 0.7033, 


Epoch 12/20 completed, Global Step: 960
nri_train_loss: 0.0160, enc_train_loss: 7.0457, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.1060, dec_train_loss: 0.0160, train_edge_accuracy: 0.7033
nri_val_loss: 0.0161, enc_val_loss: 7.2167, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0918, dec_val_loss: 0.0161, val_edge_accuracy: 0.6865

---------------------------------------------------------------------------

Step 960, Epoch 13/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.2829, enc_train_entropy: 0.0862, dec_train_loss: 0.0159, train_edge_accuracy: 0.6783, 

Step 965, Epoch 13/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0161, enc_train_loss: 7.1940, enc_train_entropy: 0.0936, dec_train_loss: 0.0161, train_edge_accuracy: 0.6883, 

Step 970, Epoch 13/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0160, enc_train_loss: 7.2191, enc_train_entropy: 0.0916, dec_train_loss: 0.0160, train_edge_accuracy: 0.6900, 

Step 975, Epoch 13/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0161, enc_train_loss: 7.1357, enc_train_entropy: 0.0985, dec_train_loss: 0.0161, train_edge_accuracy: 0.7100, 

Step 980, Epoch 13/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.2306, enc_train_entropy: 0.0906, dec_train_loss: 0.0159, train_edge_accuracy: 0.6867, 

Step 985, Epoch 13/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0163, enc_train_loss: 7.1181, enc_train_entropy: 0.1000, dec_train_loss: 0.0163, train_edge_accuracy: 0.6783, 

Step 990, Epoch 13/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.2629, enc_train_entropy: 0.0879, dec_train_loss: 0.0159, train_edge_accuracy: 0.6733, 

Step 995, Epoch 13/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0160, enc_train_loss: 7.1762, enc_train_entropy: 0.0951, dec_train_loss: 0.0160, train_edge_accuracy: 0.6800, 

Step 1000, Epoch 13/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0162, enc_train_loss: 7.1998, enc_train_entropy: 0.0932, dec_train_loss: 0.0162, train_edge_accuracy: 0.6700, 

Step 1005, Epoch 13/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0160, enc_train_loss: 7.2707, enc_train_entropy: 0.0873, dec_train_loss: 0.0160, train_edge_accuracy: 0.7017, 

Step 1010, Epoch 13/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0160, enc_train_loss: 7.1398, enc_train_entropy: 0.0982, dec_train_loss: 0.0160, train_edge_accuracy: 0.7033, 

Step 1015, Epoch 13/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.1935, enc_train_entropy: 0.0937, dec_train_loss: 0.0159, train_edge_accuracy: 0.6833, 

Step 1020, Epoch 13/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.1336, enc_train_entropy: 0.0987, dec_train_loss: 0.0159, train_edge_accuracy: 0.6717, 

Step 1025, Epoch 13/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.1603, enc_train_entropy: 0.0965, dec_train_loss: 0.0157, train_edge_accuracy: 0.7017, 

Step 1030, Epoch 13/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.1838, enc_train_entropy: 0.0945, dec_train_loss: 0.0158, train_edge_accuracy: 0.7100, 

Step 1035, Epoch 13/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.2082, enc_train_entropy: 0.0925, dec_train_loss: 0.0157, train_edge_accuracy: 0.6717, 


Epoch 13/20 completed, Global Step: 1040
nri_train_loss: 0.0157, enc_train_loss: 7.2082, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0925, dec_train_loss: 0.0157, train_edge_accuracy: 0.6717
nri_val_loss: 0.0159, enc_val_loss: 7.2480, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0891, dec_val_loss: 0.0159, val_edge_accuracy: 0.6877

---------------------------------------------------------------------------

Step 1040, Epoch 14/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.3549, enc_train_entropy: 0.0802, dec_train_loss: 0.0158, train_edge_accuracy: 0.6967, 

Step 1045, Epoch 14/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.2218, enc_train_entropy: 0.0913, dec_train_loss: 0.0159, train_edge_accuracy: 0.6517, 

Step 1050, Epoch 14/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.2711, enc_train_entropy: 0.0872, dec_train_loss: 0.0159, train_edge_accuracy: 0.6583, 

Step 1055, Epoch 14/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.2393, enc_train_entropy: 0.0899, dec_train_loss: 0.0158, train_edge_accuracy: 0.6917, 

Step 1060, Epoch 14/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0160, enc_train_loss: 7.1057, enc_train_entropy: 0.1010, dec_train_loss: 0.0160, train_edge_accuracy: 0.6983, 

Step 1065, Epoch 14/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.2241, enc_train_entropy: 0.0911, dec_train_loss: 0.0158, train_edge_accuracy: 0.6983, 

Step 1070, Epoch 14/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.1710, enc_train_entropy: 0.0956, dec_train_loss: 0.0159, train_edge_accuracy: 0.6983, 

Step 1075, Epoch 14/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.4095, enc_train_entropy: 0.0757, dec_train_loss: 0.0158, train_edge_accuracy: 0.6933, 

Step 1080, Epoch 14/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.1497, enc_train_entropy: 0.0973, dec_train_loss: 0.0159, train_edge_accuracy: 0.6967, 

Step 1085, Epoch 14/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.2416, enc_train_entropy: 0.0897, dec_train_loss: 0.0159, train_edge_accuracy: 0.7083, 

Step 1090, Epoch 14/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.2583, enc_train_entropy: 0.0883, dec_train_loss: 0.0157, train_edge_accuracy: 0.6700, 

Step 1095, Epoch 14/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.2799, enc_train_entropy: 0.0865, dec_train_loss: 0.0156, train_edge_accuracy: 0.6900, 

Step 1100, Epoch 14/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.2474, enc_train_entropy: 0.0892, dec_train_loss: 0.0158, train_edge_accuracy: 0.6667, 

Step 1105, Epoch 14/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.1786, enc_train_entropy: 0.0949, dec_train_loss: 0.0155, train_edge_accuracy: 0.7033, 

Step 1110, Epoch 14/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0159, enc_train_loss: 7.2984, enc_train_entropy: 0.0850, dec_train_loss: 0.0159, train_edge_accuracy: 0.7250, 

Step 1115, Epoch 14/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.2669, enc_train_entropy: 0.0876, dec_train_loss: 0.0157, train_edge_accuracy: 0.6700, 


Epoch 14/20 completed, Global Step: 1120
nri_train_loss: 0.0157, enc_train_loss: 7.2669, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0876, dec_train_loss: 0.0157, train_edge_accuracy: 0.6700
nri_val_loss: 0.0157, enc_val_loss: 7.2828, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0862, dec_val_loss: 0.0157, val_edge_accuracy: 0.6735

---------------------------------------------------------------------------

Step 1120, Epoch 15/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.1956, enc_train_entropy: 0.0935, dec_train_loss: 0.0158, train_edge_accuracy: 0.6500, 

Step 1125, Epoch 15/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.3896, enc_train_entropy: 0.0774, dec_train_loss: 0.0158, train_edge_accuracy: 0.6600, 

Step 1130, Epoch 15/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.3358, enc_train_entropy: 0.0818, dec_train_loss: 0.0156, train_edge_accuracy: 0.7167, 

Step 1135, Epoch 15/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.1864, enc_train_entropy: 0.0943, dec_train_loss: 0.0158, train_edge_accuracy: 0.6767, 

Step 1140, Epoch 15/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0158, enc_train_loss: 7.2679, enc_train_entropy: 0.0875, dec_train_loss: 0.0158, train_edge_accuracy: 0.7050, 

Step 1145, Epoch 15/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.2876, enc_train_entropy: 0.0858, dec_train_loss: 0.0156, train_edge_accuracy: 0.6717, 

Step 1150, Epoch 15/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.2549, enc_train_entropy: 0.0886, dec_train_loss: 0.0157, train_edge_accuracy: 0.6817, 

Step 1155, Epoch 15/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.2309, enc_train_entropy: 0.0906, dec_train_loss: 0.0157, train_edge_accuracy: 0.6883, 

Step 1160, Epoch 15/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.2231, enc_train_entropy: 0.0912, dec_train_loss: 0.0157, train_edge_accuracy: 0.7033, 

Step 1165, Epoch 15/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.2062, enc_train_entropy: 0.0926, dec_train_loss: 0.0155, train_edge_accuracy: 0.6883, 

Step 1170, Epoch 15/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.2463, enc_train_entropy: 0.0893, dec_train_loss: 0.0157, train_edge_accuracy: 0.6800, 

Step 1175, Epoch 15/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.2697, enc_train_entropy: 0.0873, dec_train_loss: 0.0156, train_edge_accuracy: 0.6733, 

Step 1180, Epoch 15/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.2155, enc_train_entropy: 0.0919, dec_train_loss: 0.0155, train_edge_accuracy: 0.6883, 

Step 1185, Epoch 15/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.1150, enc_train_entropy: 0.1002, dec_train_loss: 0.0156, train_edge_accuracy: 0.6867, 

Step 1190, Epoch 15/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2101, enc_train_entropy: 0.0923, dec_train_loss: 0.0153, train_edge_accuracy: 0.6767, 

Step 1195, Epoch 15/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.1154, enc_train_entropy: 0.1002, dec_train_loss: 0.0155, train_edge_accuracy: 0.6850, 


Epoch 15/20 completed, Global Step: 1200
nri_train_loss: 0.0155, enc_train_loss: 7.1154, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.1002, dec_train_loss: 0.0155, train_edge_accuracy: 0.6850
nri_val_loss: 0.0155, enc_val_loss: 7.2809, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0864, dec_val_loss: 0.0155, val_edge_accuracy: 0.6853

---------------------------------------------------------------------------

Step 1200, Epoch 16/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.1612, enc_train_entropy: 0.0964, dec_train_loss: 0.0156, train_edge_accuracy: 0.6917, 

Step 1205, Epoch 16/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0157, enc_train_loss: 7.1990, enc_train_entropy: 0.0932, dec_train_loss: 0.0157, train_edge_accuracy: 0.6767, 

Step 1210, Epoch 16/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.3682, enc_train_entropy: 0.0791, dec_train_loss: 0.0154, train_edge_accuracy: 0.6783, 

Step 1215, Epoch 16/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.2218, enc_train_entropy: 0.0913, dec_train_loss: 0.0155, train_edge_accuracy: 0.6750, 

Step 1220, Epoch 16/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.2388, enc_train_entropy: 0.0899, dec_train_loss: 0.0156, train_edge_accuracy: 0.6867, 

Step 1225, Epoch 16/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0156, enc_train_loss: 7.2342, enc_train_entropy: 0.0903, dec_train_loss: 0.0156, train_edge_accuracy: 0.6600, 

Step 1230, Epoch 16/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.2886, enc_train_entropy: 0.0858, dec_train_loss: 0.0154, train_edge_accuracy: 0.6600, 

Step 1235, Epoch 16/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.1382, enc_train_entropy: 0.0983, dec_train_loss: 0.0153, train_edge_accuracy: 0.6783, 

Step 1240, Epoch 16/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.2656, enc_train_entropy: 0.0877, dec_train_loss: 0.0154, train_edge_accuracy: 0.6750, 

Step 1245, Epoch 16/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.2014, enc_train_entropy: 0.0930, dec_train_loss: 0.0155, train_edge_accuracy: 0.6817, 

Step 1250, Epoch 16/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.1063, enc_train_entropy: 0.1010, dec_train_loss: 0.0155, train_edge_accuracy: 0.6817, 

Step 1255, Epoch 16/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.1866, enc_train_entropy: 0.0943, dec_train_loss: 0.0155, train_edge_accuracy: 0.6717, 

Step 1260, Epoch 16/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.2252, enc_train_entropy: 0.0911, dec_train_loss: 0.0154, train_edge_accuracy: 0.6800, 

Step 1265, Epoch 16/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.3605, enc_train_entropy: 0.0798, dec_train_loss: 0.0153, train_edge_accuracy: 0.6817, 

Step 1270, Epoch 16/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.1730, enc_train_entropy: 0.0954, dec_train_loss: 0.0155, train_edge_accuracy: 0.6883, 

Step 1275, Epoch 16/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0155, enc_train_loss: 7.2232, enc_train_entropy: 0.0912, dec_train_loss: 0.0155, train_edge_accuracy: 0.6917, 


Epoch 16/20 completed, Global Step: 1280
nri_train_loss: 0.0155, enc_train_loss: 7.2232, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0912, dec_train_loss: 0.0155, train_edge_accuracy: 0.6917
nri_val_loss: 0.0154, enc_val_loss: 7.2962, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0851, dec_val_loss: 0.0154, val_edge_accuracy: 0.6687

---------------------------------------------------------------------------

Step 1280, Epoch 17/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.1728, enc_train_entropy: 0.0954, dec_train_loss: 0.0153, train_edge_accuracy: 0.6817, 

Step 1285, Epoch 17/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.2967, enc_train_entropy: 0.0851, dec_train_loss: 0.0154, train_edge_accuracy: 0.6650, 

Step 1290, Epoch 17/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2802, enc_train_entropy: 0.0865, dec_train_loss: 0.0153, train_edge_accuracy: 0.6650, 

Step 1295, Epoch 17/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.3795, enc_train_entropy: 0.0782, dec_train_loss: 0.0154, train_edge_accuracy: 0.6850, 

Step 1300, Epoch 17/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.1665, enc_train_entropy: 0.0959, dec_train_loss: 0.0154, train_edge_accuracy: 0.6683, 

Step 1305, Epoch 17/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.2656, enc_train_entropy: 0.0877, dec_train_loss: 0.0152, train_edge_accuracy: 0.6933, 

Step 1310, Epoch 17/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.1968, enc_train_entropy: 0.0934, dec_train_loss: 0.0151, train_edge_accuracy: 0.6767, 

Step 1315, Epoch 17/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2673, enc_train_entropy: 0.0875, dec_train_loss: 0.0153, train_edge_accuracy: 0.6933, 

Step 1320, Epoch 17/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2273, enc_train_entropy: 0.0909, dec_train_loss: 0.0153, train_edge_accuracy: 0.6783, 

Step 1325, Epoch 17/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.3650, enc_train_entropy: 0.0794, dec_train_loss: 0.0152, train_edge_accuracy: 0.6817, 

Step 1330, Epoch 17/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.1143, enc_train_entropy: 0.1003, dec_train_loss: 0.0153, train_edge_accuracy: 0.6783, 

Step 1335, Epoch 17/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2412, enc_train_entropy: 0.0897, dec_train_loss: 0.0153, train_edge_accuracy: 0.6783, 

Step 1340, Epoch 17/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2702, enc_train_entropy: 0.0873, dec_train_loss: 0.0153, train_edge_accuracy: 0.6617, 

Step 1345, Epoch 17/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2856, enc_train_entropy: 0.0860, dec_train_loss: 0.0151, train_edge_accuracy: 0.6700, 

Step 1350, Epoch 17/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.3442, enc_train_entropy: 0.0811, dec_train_loss: 0.0153, train_edge_accuracy: 0.6717, 

Step 1355, Epoch 17/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.2655, enc_train_entropy: 0.0877, dec_train_loss: 0.0152, train_edge_accuracy: 0.6583, 


Epoch 17/20 completed, Global Step: 1360
nri_train_loss: 0.0152, enc_train_loss: 7.2655, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0877, dec_train_loss: 0.0152, train_edge_accuracy: 0.6583
nri_val_loss: 0.0152, enc_val_loss: 7.3257, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0827, dec_val_loss: 0.0152, val_edge_accuracy: 0.6705

---------------------------------------------------------------------------

Step 1360, Epoch 18/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2432, enc_train_entropy: 0.0896, dec_train_loss: 0.0153, train_edge_accuracy: 0.6917, 

Step 1365, Epoch 18/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.3317, enc_train_entropy: 0.0822, dec_train_loss: 0.0151, train_edge_accuracy: 0.6700, 

Step 1370, Epoch 18/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2522, enc_train_entropy: 0.0888, dec_train_loss: 0.0153, train_edge_accuracy: 0.6800, 

Step 1375, Epoch 18/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0153, enc_train_loss: 7.2546, enc_train_entropy: 0.0886, dec_train_loss: 0.0153, train_edge_accuracy: 0.6783, 

Step 1380, Epoch 18/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.2929, enc_train_entropy: 0.0854, dec_train_loss: 0.0152, train_edge_accuracy: 0.6667, 

Step 1385, Epoch 18/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.2174, enc_train_entropy: 0.0917, dec_train_loss: 0.0154, train_edge_accuracy: 0.6717, 

Step 1390, Epoch 18/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.3521, enc_train_entropy: 0.0805, dec_train_loss: 0.0152, train_edge_accuracy: 0.6817, 

Step 1395, Epoch 18/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0154, enc_train_loss: 7.2255, enc_train_entropy: 0.0910, dec_train_loss: 0.0154, train_edge_accuracy: 0.6633, 

Step 1400, Epoch 18/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2087, enc_train_entropy: 0.0924, dec_train_loss: 0.0151, train_edge_accuracy: 0.6633, 

Step 1405, Epoch 18/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.3058, enc_train_entropy: 0.0843, dec_train_loss: 0.0151, train_edge_accuracy: 0.6567, 

Step 1410, Epoch 18/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.2945, enc_train_entropy: 0.0853, dec_train_loss: 0.0149, train_edge_accuracy: 0.6650, 

Step 1415, Epoch 18/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.3554, enc_train_entropy: 0.0802, dec_train_loss: 0.0152, train_edge_accuracy: 0.6800, 

Step 1420, Epoch 18/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.2619, enc_train_entropy: 0.0880, dec_train_loss: 0.0150, train_edge_accuracy: 0.6700, 

Step 1425, Epoch 18/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.3850, enc_train_entropy: 0.0777, dec_train_loss: 0.0151, train_edge_accuracy: 0.6683, 

Step 1430, Epoch 18/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2818, enc_train_entropy: 0.0863, dec_train_loss: 0.0151, train_edge_accuracy: 0.6633, 

Step 1435, Epoch 18/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.3184, enc_train_entropy: 0.0833, dec_train_loss: 0.0150, train_edge_accuracy: 0.6600, 


Epoch 18/20 completed, Global Step: 1440
nri_train_loss: 0.0150, enc_train_loss: 7.3184, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0833, dec_train_loss: 0.0150, train_edge_accuracy: 0.6600
nri_val_loss: 0.0151, enc_val_loss: 7.3459, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0810, dec_val_loss: 0.0151, val_edge_accuracy: 0.6670

---------------------------------------------------------------------------

Step 1440, Epoch 19/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.2713, enc_train_entropy: 0.0872, dec_train_loss: 0.0152, train_edge_accuracy: 0.6483, 

Step 1445, Epoch 19/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.2567, enc_train_entropy: 0.0884, dec_train_loss: 0.0152, train_edge_accuracy: 0.6650, 

Step 1450, Epoch 19/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.3900, enc_train_entropy: 0.0773, dec_train_loss: 0.0150, train_edge_accuracy: 0.6583, 

Step 1455, Epoch 19/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2093, enc_train_entropy: 0.0924, dec_train_loss: 0.0151, train_edge_accuracy: 0.6533, 

Step 1460, Epoch 19/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2996, enc_train_entropy: 0.0848, dec_train_loss: 0.0151, train_edge_accuracy: 0.6717, 

Step 1465, Epoch 19/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.3867, enc_train_entropy: 0.0776, dec_train_loss: 0.0151, train_edge_accuracy: 0.6717, 

Step 1470, Epoch 19/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2666, enc_train_entropy: 0.0876, dec_train_loss: 0.0151, train_edge_accuracy: 0.6733, 

Step 1475, Epoch 19/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.3798, enc_train_entropy: 0.0782, dec_train_loss: 0.0150, train_edge_accuracy: 0.6583, 

Step 1480, Epoch 19/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.1858, enc_train_entropy: 0.0943, dec_train_loss: 0.0149, train_edge_accuracy: 0.6617, 

Step 1485, Epoch 19/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.3013, enc_train_entropy: 0.0847, dec_train_loss: 0.0152, train_edge_accuracy: 0.6783, 

Step 1490, Epoch 19/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2882, enc_train_entropy: 0.0858, dec_train_loss: 0.0151, train_edge_accuracy: 0.6750, 

Step 1495, Epoch 19/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2266, enc_train_entropy: 0.0909, dec_train_loss: 0.0151, train_edge_accuracy: 0.6733, 

Step 1500, Epoch 19/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.2806, enc_train_entropy: 0.0864, dec_train_loss: 0.0149, train_edge_accuracy: 0.6600, 

Step 1505, Epoch 19/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.2836, enc_train_entropy: 0.0862, dec_train_loss: 0.0150, train_edge_accuracy: 0.6783, 

Step 1510, Epoch 19/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.3102, enc_train_entropy: 0.0840, dec_train_loss: 0.0151, train_edge_accuracy: 0.6717, 

Step 1515, Epoch 19/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2525, enc_train_entropy: 0.0888, dec_train_loss: 0.0151, train_edge_accuracy: 0.6500, 


Epoch 19/20 completed, Global Step: 1520
nri_train_loss: 0.0151, enc_train_loss: 7.2525, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0888, dec_train_loss: 0.0151, train_edge_accuracy: 0.6500
nri_val_loss: 0.0150, enc_val_loss: 7.3629, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0796, dec_val_loss: 0.0150, val_edge_accuracy: 0.6653

---------------------------------------------------------------------------

Step 1520, Epoch 20/20, Batch 0/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.3015, enc_train_entropy: 0.0847, dec_train_loss: 0.0150, train_edge_accuracy: 0.6550, 

Step 1525, Epoch 20/20, Batch 5/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0148, enc_train_loss: 7.3046, enc_train_entropy: 0.0844, dec_train_loss: 0.0148, train_edge_accuracy: 0.6350, 

Step 1530, Epoch 20/20, Batch 10/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.3602, enc_train_entropy: 0.0798, dec_train_loss: 0.0150, train_edge_accuracy: 0.6550, 

Step 1535, Epoch 20/20, Batch 15/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.2827, enc_train_entropy: 0.0863, dec_train_loss: 0.0150, train_edge_accuracy: 0.6717, 

Step 1540, Epoch 20/20, Batch 20/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0152, enc_train_loss: 7.4113, enc_train_entropy: 0.0755, dec_train_loss: 0.0152, train_edge_accuracy: 0.6867, 

Step 1545, Epoch 20/20, Batch 25/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.3177, enc_train_entropy: 0.0833, dec_train_loss: 0.0149, train_edge_accuracy: 0.6567, 

Step 1550, Epoch 20/20, Batch 30/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.1769, enc_train_entropy: 0.0951, dec_train_loss: 0.0149, train_edge_accuracy: 0.6683, 

Step 1555, Epoch 20/20, Batch 35/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.3883, enc_train_entropy: 0.0775, dec_train_loss: 0.0150, train_edge_accuracy: 0.6517, 

Step 1560, Epoch 20/20, Batch 40/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2367, enc_train_entropy: 0.0901, dec_train_loss: 0.0151, train_edge_accuracy: 0.6500, 

Step 1565, Epoch 20/20, Batch 45/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.3849, enc_train_entropy: 0.0777, dec_train_loss: 0.0149, train_edge_accuracy: 0.6583, 

Step 1570, Epoch 20/20, Batch 50/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.4437, enc_train_entropy: 0.0728, dec_train_loss: 0.0150, train_edge_accuracy: 0.6650, 

Step 1575, Epoch 20/20, Batch 55/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.3435, enc_train_entropy: 0.0812, dec_train_loss: 0.0149, train_edge_accuracy: 0.6633, 

Step 1580, Epoch 20/20, Batch 60/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0150, enc_train_loss: 7.1844, enc_train_entropy: 0.0944, dec_train_loss: 0.0150, train_edge_accuracy: 0.6383, 

Step 1585, Epoch 20/20, Batch 65/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0151, enc_train_loss: 7.2459, enc_train_entropy: 0.0893, dec_train_loss: 0.0151, train_edge_accuracy: 0.6550, 

Step 1590, Epoch 20/20, Batch 70/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0149, enc_train_loss: 7.2714, enc_train_entropy: 0.0872, dec_train_loss: 0.0149, train_edge_accuracy: 0.6683, 

Step 1595, Epoch 20/20, Batch 75/80
temp: 0.9990, beta: 0.0000, gamma: 0.0000, 
enc_train_warmup_loss: 0.0000, 
nri_train_loss: 0.0147, enc_train_loss: 7.5073, enc_train_entropy: 0.0675, dec_train_loss: 0.0147, train_edge_accuracy: 0.6650, 


Epoch 20/20 completed, Global Step: 1600
nri_train_loss: 0.0147, enc_train_loss: 7.5073, enc_train_warmup_loss: 0.0000, enc_train_entropy: 0.0675, dec_train_loss: 0.0147, train_edge_accuracy: 0.6650
nri_val_loss: 0.0148, enc_val_loss: 7.3925, enc_val_warmup_loss: 0.0000, enc_val_entropy: 0.0771, dec_val_loss: 0.0148, val_edge_accuracy: 0.6580

---------------------------------------------------------------------------


Training completed in 516.47 seconds or 8.61 minutes or 0.14346322112613255 hours.
Total training steps: 1600

Training completed for model '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11'. Trained model saved at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11\checkpoints

---------------------------------------------------------------------------

<<<<<<<<<<<< TRAINING LOSS PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating training loss plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11...

Training loss (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11


<<<<<<<<<<<< ENCODER EDGE ACCURACY PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating encoder edge accuracy plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11...

Encoder edge accuracy (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11


<<<<<<<<<<<< ENCODER EDGE ENTROPY PLOT (TRAIN + VAL) >>>>>>>>>>>>

Creating encoder edge entropy plot for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11...

Encoder edge entropy (train + val) plot logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11


<<<<<<<<<<<< DECODER OUTPUT PLOT (TRAIN) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.237' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11...

Decoder output plot for rep '1001.2369' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11


<<<<<<<<<<<< DECODER OUTPUT PLOT (VAL) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.449' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11...

Decoder output plot for rep '1001.449' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11


---------------------------------------------------------------------------

TESTING TRAINED NRI MODEL...

.ckpt_files available in C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11\checkpoints:

['best-model-epoch=19-val_loss=0.0000.ckpt']

Trained NRI Model Loaded for testing.

Testing environment set. Testing will be logged at: C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11\test
C:\Anaconda3\envs\afd_env\Lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
 ... (more hidden) ...
Initializing input processors for encoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------

Initializing input processors for decoder model...

>> Domain transformer initialized: time(cutoff_freq=0)

>> Raw data normalizer initialized with 'min_max' normalization

>> No feature normalization is applied

>> No time feature extraction is applied

>> No feature reduction is applied

---------------------------------------------------------------------------
 ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ... ... (more hidden) ...
Testing completed in 1.26 seconds or 0.02 minutes or 0.0003498620457119412 hours.

nri_test_loss: 7.3395, enc_test_loss: 7.3244, dec_test_loss: 0.0151, test_edge_accuracy: 0.6600

Edge predictions are as follows (showing probabilities for each edge type):

Rep 1,001.444:
[[9.9943739e-01 5.6263676e-04]
 [9.9995506e-01 4.4997629e-05]
 [9.9999583e-01 4.2252586e-06]
 [9.9992049e-01 7.9552156e-05]
 [9.9491991e-02 9.0050793e-01]
 [2.3560321e-01 7.6439679e-01]
 [1.0000000e+00 5.4844556e-11]
 [1.0000000e+00 1.7680435e-08]
 [2.4055560e-05 9.9997592e-01]
 [1.0000000e+00 1.1748047e-16]
 [1.0000000e+00 9.8858069e-14]
 [9.9930048e-01 6.9945911e-04]]

Rep 1,001.652:
[[8.6245501e-01 1.3754502e-01]
 [9.9797279e-01 2.0271915e-03]
 [9.9999475e-01 5.2273140e-06]
 [3.8209136e-04 9.9961793e-01]
 [1.1179364e-02 9.8882061e-01]
 [3.8028327e-01 6.1971670e-01]
 [1.2147692e-03 9.9878520e-01]
 [1.2013693e-04 9.9987984e-01]
 [1.9635085e-05 9.9998033e-01]
 [9.7380859e-01 2.6191473e-02]
 [3.6167252e-01 6.3832754e-01]
 [9.7786879e-06 9.9999022e-01]]

Rep 1,001.446:
[[2.2841216e-06 9.9999774e-01]
 [7.2409521e-06 9.9999273e-01]
 [2.2294288e-04 9.9977702e-01]
 [3.7494893e-04 9.9962509e-01]
 [2.2710709e-07 9.9999976e-01]
 [2.8566810e-06 9.9999714e-01]
 [6.0637131e-02 9.3936288e-01]
 [3.1674560e-06 9.9999678e-01]
 [2.9603170e-08 1.0000000e+00]
 [9.9987304e-01 1.2691953e-04]
 [3.7594926e-02 9.6240503e-01]
 [1.4585833e-06 9.9999857e-01]]

Rep 1,001.131:
[[9.9986291e-01 1.3704863e-04]
 [1.0000000e+00 1.3089307e-08]
 [1.0000000e+00 1.7315726e-08]
 [8.6055184e-03 9.9139446e-01]
 [9.9975878e-01 2.4123302e-04]
 [9.6732116e-01 3.2678891e-02]
 [4.9296813e-04 9.9950707e-01]
 [1.4602308e-03 9.9853981e-01]
 [9.8548138e-05 9.9990141e-01]
 [9.9688280e-01 3.1171963e-03]
 [9.7781312e-01 2.2186866e-02]
 [8.0172575e-01 1.9827427e-01]]

Rep 1,001.460:
[[1.1906139e-02 9.8809391e-01]
 [9.9998772e-01 1.2332387e-05]
 [9.9989760e-01 1.0242363e-04]
 [3.3284455e-06 9.9999666e-01]
 [9.4765961e-01 5.2340358e-02]
 [7.8156650e-02 9.2184341e-01]
 [2.1539092e-01 7.8460908e-01]
 [9.8965257e-01 1.0347403e-02]
 [9.1983230e-07 9.9999905e-01]
 [9.9997878e-01 2.1277214e-05]
 [9.9999964e-01 3.9525725e-07]
 [1.2730262e-01 8.7269735e-01]]

Rep 1,001.235:
[[1.9886492e-01 8.0113512e-01]
 [9.9999440e-01 5.5468549e-06]
 [9.9999559e-01 4.4318431e-06]
 [8.1497485e-05 9.9991846e-01]
 [9.2029637e-01 7.9703651e-02]
 [1.9804297e-01 8.0195707e-01]
 [8.9255234e-05 9.9991071e-01]
 [7.9027719e-05 9.9992096e-01]
 [1.8641398e-06 9.9999809e-01]
 [1.0228628e-01 8.9771372e-01]
 [9.9903131e-03 9.9000973e-01]
 [1.6173889e-03 9.9838257e-01]]

Rep 1,001.386:
[[9.9227285e-01 7.7271205e-03]
 [1.0000000e+00 1.3838207e-11]
 [1.0000000e+00 8.5671695e-12]
 [3.8336450e-03 9.9616635e-01]
 [9.9999988e-01 1.4140726e-07]
 [9.9999654e-01 3.4928510e-06]
 [7.6083420e-03 9.9239165e-01]
 [1.3788536e-02 9.8621142e-01]
 [1.2302984e-02 9.8769706e-01]
 [9.9997985e-01 2.0161913e-05]
 [9.9987507e-01 1.2491252e-04]
 [9.9952364e-01 4.7634874e-04]]

Rep 1,001.350:
[[9.88747418e-01 1.12526165e-02]
 [8.78874540e-01 1.21125437e-01]
 [6.35757148e-01 3.64242822e-01]
 [9.99653935e-01 3.46101791e-04]
 [1.78416725e-03 9.98215854e-01]
 [2.46937343e-05 9.99975324e-01]
 [1.00000000e+00 5.49170154e-10]
 [9.99997020e-01 2.92242044e-06]
 [1.42134950e-05 9.99985814e-01]
 [1.00000000e+00 3.28593760e-14]
 [1.00000000e+00 2.71458939e-10]
 [9.98981416e-01 1.01850333e-03]]

Rep 1,001.374:
[[9.6317911e-01 3.6820892e-02]
 [1.0000000e+00 8.5477181e-11]
 [1.0000000e+00 1.5923389e-08]
 [2.8321341e-01 7.1678662e-01]
 [9.9999368e-01 6.2826125e-06]
 [9.7375286e-01 2.6247172e-02]
 [9.9999726e-01 2.7374076e-06]
 [9.9820232e-01 1.7976621e-03]
 [4.0677505e-06 9.9999595e-01]
 [1.0000000e+00 1.5363379e-11]
 [1.0000000e+00 1.8773184e-08]
 [1.6960035e-03 9.9830401e-01]]

Rep 1,001.279:
[[2.4047051e-01 7.5952947e-01]
 [9.9999821e-01 1.7361285e-06]
 [9.9936658e-01 6.3345011e-04]
 [2.9702897e-03 9.9702966e-01]
 [9.3885475e-01 6.1145283e-02]
 [2.2647434e-03 9.9773526e-01]
 [6.2453055e-01 3.7546948e-01]
 [4.0298584e-03 9.9597013e-01]
 [2.1647328e-07 9.9999976e-01]
 [9.9999130e-01 8.7066410e-06]
 [9.6508837e-01 3.4911610e-02]
 [4.3674870e-04 9.9956328e-01]]

Rep 1,001.281:
[[1.7230584e-05 9.9998271e-01]
 [8.9472914e-03 9.9105269e-01]
 [1.4531825e-01 8.5468173e-01]
 [2.7749849e-05 9.9997222e-01]
 [3.8550770e-05 9.9996150e-01]
 [1.5945923e-04 9.9984050e-01]
 [1.1772504e-03 9.9882275e-01]
 [4.1217086e-06 9.9999583e-01]
 [7.9968040e-06 9.9999201e-01]
 [9.2986399e-01 7.0136040e-02]
 [2.8040267e-03 9.9719596e-01]
 [2.2859302e-05 9.9997711e-01]]

Rep 1,001.592:
[[5.6231223e-02 9.4376880e-01]
 [7.4317527e-01 2.5682470e-01]
 [7.7876765e-01 2.2123231e-01]
 [2.6106384e-06 9.9999738e-01]
 [3.0174775e-03 9.9698251e-01]
 [1.5040224e-04 9.9984956e-01]
 [8.8851535e-05 9.9991119e-01]
 [1.7642645e-02 9.8235738e-01]
 [3.6756884e-07 9.9999964e-01]
 [1.3947877e-01 8.6052126e-01]
 [9.7738588e-01 2.2614149e-02]
 [9.6674100e-04 9.9903333e-01]]

Rep 1,001.259:
[[8.3823675e-01 1.6176331e-01]
 [1.0000000e+00 4.1442831e-08]
 [9.9999487e-01 5.1698944e-06]
 [1.5271767e-03 9.9847287e-01]
 [9.9519473e-01 4.8052836e-03]
 [4.9406134e-02 9.5059389e-01]
 [7.5808761e-04 9.9924195e-01]
 [1.6948725e-05 9.9998307e-01]
 [9.5254723e-07 9.9999905e-01]
 [9.8509294e-01 1.4907108e-02]
 [1.6306212e-02 9.8369378e-01]
 [1.0854104e-02 9.8914587e-01]]

Rep 1,001.108:
[[8.6430323e-01 1.3569681e-01]
 [9.9728537e-01 2.7146626e-03]
 [9.9987912e-01 1.2089701e-04]
 [9.9998665e-01 1.3327979e-05]
 [1.1233073e-02 9.8876697e-01]
 [4.0217269e-02 9.5978278e-01]
 [1.0000000e+00 1.5713655e-10]
 [9.9995708e-01 4.2856122e-05]
 [3.6511751e-04 9.9963486e-01]
 [1.0000000e+00 7.8092192e-16]
 [1.0000000e+00 1.9046699e-10]
 [9.9999774e-01 2.2144507e-06]]

Rep 1,001.119:
[[2.3088390e-03 9.9769115e-01]
 [9.3585569e-01 6.4144298e-02]
 [1.8225308e-01 8.1774688e-01]
 [3.9930820e-01 6.0069180e-01]
 [2.8530160e-02 9.7146982e-01]
 [1.5228300e-04 9.9984765e-01]
 [9.9955553e-01 4.4445644e-04]
 [3.9067637e-02 9.6093237e-01]
 [1.0789264e-06 9.9999893e-01]
 [1.0000000e+00 4.2354933e-09]
 [9.9818617e-01 1.8138585e-03]
 [2.0240394e-03 9.9797601e-01]]

Rep 1,001.141:
[[2.5841754e-04 9.9974161e-01]
 [2.2581860e-01 7.7418137e-01]
 [9.6043134e-01 3.9568681e-02]
 [1.7708280e-06 9.9999821e-01]
 [3.9172879e-05 9.9996078e-01]
 [4.4974269e-04 9.9955028e-01]
 [1.2028183e-04 9.9987972e-01]
 [2.2123195e-04 9.9977881e-01]
 [1.0822168e-06 9.9999893e-01]
 [8.4243566e-01 1.5756436e-01]
 [6.1070353e-01 3.8929641e-01]
 [2.5074268e-04 9.9974924e-01]]

Rep 1,001.237:
[[1.07659864e-04 9.99892354e-01]
 [6.34448111e-01 3.65551919e-01]
 [9.92612481e-01 7.38750771e-03]
 [3.17858576e-06 9.99996781e-01]
 [2.66370946e-04 9.99733627e-01]
 [9.98660573e-04 9.99001324e-01]
 [1.58422772e-05 9.99984145e-01]
 [2.07586459e-06 9.99997973e-01]
 [1.65368328e-06 9.99998331e-01]
 [1.37993440e-04 9.99861956e-01]
 [4.50565403e-06 9.99995470e-01]
 [4.55531881e-06 9.99995470e-01]]

Rep 1,001.453:
[[9.9999630e-01 3.6875142e-06]
 [9.9917525e-01 8.2475756e-04]
 [9.9779344e-01 2.2066054e-03]
 [9.9999964e-01 3.3660737e-07]
 [9.1997599e-03 9.9080020e-01]
 [3.1900310e-04 9.9968100e-01]
 [1.0000000e+00 9.2613178e-13]
 [1.0000000e+00 1.1986797e-08]
 [2.6462720e-05 9.9997354e-01]
 [1.0000000e+00 6.7585182e-17]
 [1.0000000e+00 8.3964790e-13]
 [9.9823558e-01 1.7644522e-03]]

Rep 1,001.429:
[[1.4803810e-05 9.9998522e-01]
 [4.6249588e-06 9.9999535e-01]
 [7.5589829e-05 9.9992442e-01]
 [6.8065718e-05 9.9993193e-01]
 [4.2706125e-07 9.9999952e-01]
 [3.4893153e-06 9.9999654e-01]
 [3.2123697e-03 9.9678767e-01]
 [3.4277902e-05 9.9996567e-01]
 [1.8009009e-08 1.0000000e+00]
 [9.9765778e-01 2.3421536e-03]
 [5.5001795e-01 4.4998202e-01]
 [1.4988922e-06 9.9999845e-01]]

Rep 1,001.220:
[[9.50429559e-01 4.95704971e-02]
 [1.00000000e+00 2.85565654e-10]
 [9.99996781e-01 3.20695767e-06]
 [6.54360652e-03 9.93456423e-01]
 [9.99929428e-01 7.05335915e-05]
 [4.57655080e-02 9.54234540e-01]
 [1.20777301e-01 8.79222751e-01]
 [3.13844357e-04 9.99686122e-01]
 [7.45461591e-07 9.99999285e-01]
 [9.99989152e-01 1.07943615e-05]
 [9.10410106e-01 8.95898566e-02]
 [3.84785682e-01 6.15214288e-01]]

Rep 1,001.315:
[[3.7224865e-01 6.2775135e-01]
 [9.9943084e-01 5.6917564e-04]
 [9.9983454e-01 1.6546792e-04]
 [5.1039086e-05 9.9994898e-01]
 [9.7050540e-02 9.0294945e-01]
 [6.9597205e-03 9.9304020e-01]
 [1.5086372e-05 9.9998486e-01]
 [5.6751296e-06 9.9999428e-01]
 [7.9126949e-07 9.9999917e-01]
 [1.1870644e-02 9.8812938e-01]
 [3.7380934e-04 9.9962616e-01]
 [6.3016887e-05 9.9993694e-01]]

Rep 1,001.197:
[[2.4045305e-02 9.7595471e-01]
 [9.9888271e-01 1.1172609e-03]
 [9.9947017e-01 5.2989402e-04]
 [2.0689417e-02 9.7931057e-01]
 [7.6983370e-02 9.2301667e-01]
 [3.7200317e-02 9.6279967e-01]
 [9.9985933e-01 1.4062607e-04]
 [9.9594301e-01 4.0569883e-03]
 [6.0954713e-05 9.9993908e-01]
 [1.0000000e+00 2.7069991e-10]
 [1.0000000e+00 1.5596086e-08]
 [9.9996018e-01 3.9818435e-05]]

Rep 1,001.390:
[[7.5031079e-02 9.2496896e-01]
 [9.9777752e-01 2.2224393e-03]
 [9.9994791e-01 5.2078205e-05]
 [1.2284308e-06 9.9999881e-01]
 [3.3544076e-03 9.9664551e-01]
 [1.8360551e-02 9.8163950e-01]
 [8.3669554e-05 9.9991632e-01]
 [7.6509896e-05 9.9992347e-01]
 [1.3513730e-06 9.9999869e-01]
 [2.5982794e-01 7.4017203e-01]
 [1.0804577e-01 8.9195418e-01]
 [1.8712626e-06 9.9999809e-01]]

Rep 1,001.325:
[[8.90264332e-01 1.09735645e-01]
 [1.00000000e+00 1.25320876e-09]
 [9.99999881e-01 1.04215474e-07]
 [8.38363245e-02 9.16163683e-01]
 [9.99831676e-01 1.68245897e-04]
 [8.31003487e-01 1.68996498e-01]
 [9.99992728e-01 7.22339155e-06]
 [9.93760407e-01 6.23954413e-03]
 [1.82941847e-06 9.99998212e-01]
 [1.00000000e+00 5.08079342e-11]
 [9.99999881e-01 7.56933076e-08]
 [2.80291133e-04 9.99719679e-01]]

Rep 1,001.235:
[[7.21827091e-05 9.99927759e-01]
 [2.05437973e-04 9.99794543e-01]
 [1.04572595e-04 9.99895453e-01]
 [2.63685542e-05 9.99973655e-01]
 [3.16757287e-05 9.99968290e-01]
 [1.34213178e-05 9.99986529e-01]
 [7.31263263e-03 9.92687345e-01]
 [3.39034898e-03 9.96609688e-01]
 [5.03981994e-07 9.99999523e-01]
 [9.99084234e-01 9.15772631e-04]
 [9.91736710e-01 8.26330576e-03]
 [4.84609482e-05 9.99951482e-01]]

Rep 1,001.218:
[[9.9896753e-01 1.0324999e-03]
 [1.0000000e+00 1.1505529e-13]
 [1.0000000e+00 6.0113816e-09]
 [2.5228906e-02 9.7477108e-01]
 [1.0000000e+00 3.9153853e-08]
 [9.3501616e-01 6.4983875e-02]
 [9.9172008e-01 8.2799299e-03]
 [1.4304551e-01 8.5695451e-01]
 [2.2671986e-05 9.9997735e-01]
 [1.0000000e+00 6.0749956e-09]
 [9.9999380e-01 6.2509621e-06]
 [9.9277109e-01 7.2288993e-03]]

Rep 1,001.110:
[[1.8563902e-03 9.9814367e-01]
 [3.3633888e-01 6.6366118e-01]
 [9.9565589e-01 4.3440834e-03]
 [1.0446040e-06 9.9999893e-01]
 [9.8538927e-05 9.9990141e-01]
 [5.0149485e-03 9.9498510e-01]
 [1.4173807e-05 9.9998581e-01]
 [5.7933048e-05 9.9994206e-01]
 [1.3171137e-05 9.9998689e-01]
 [4.5049579e-05 9.9995494e-01]
 [1.5314214e-04 9.9984682e-01]
 [1.6067061e-06 9.9999845e-01]]

Rep 1,001.335:
[[1.1561014e-04 9.9988437e-01]
 [9.5998193e-04 9.9903995e-01]
 [2.7997103e-01 7.2002894e-01]
 [3.8987247e-03 9.9610126e-01]
 [1.0514261e-05 9.9998951e-01]
 [1.9509812e-04 9.9980491e-01]
 [9.9866283e-01 1.3371374e-03]
 [2.4633075e-01 7.5366926e-01]
 [2.5749016e-06 9.9999738e-01]
 [1.0000000e+00 2.8069114e-09]
 [9.9998641e-01 1.3609732e-05]
 [9.3231484e-02 9.0676850e-01]]

Rep 1,001.342:
[[9.4006573e-05 9.9990594e-01]
 [9.9999201e-01 7.9720949e-06]
 [9.9999964e-01 3.3643374e-07]
 [6.7848191e-07 9.9999928e-01]
 [7.1704543e-01 2.8295463e-01]
 [8.8891989e-01 1.1108015e-01]
 [2.5314002e-03 9.9746859e-01]
 [5.0457325e-03 9.9495429e-01]
 [2.0472592e-06 9.9999797e-01]
 [9.9784327e-01 2.1567324e-03]
 [9.9295616e-01 7.0438734e-03]
 [3.7570624e-04 9.9962425e-01]]

Rep 1,001.222:
[[4.1608475e-04 9.9958390e-01]
 [2.1582215e-01 7.8417790e-01]
 [9.8891234e-01 1.1087657e-02]
 [4.6450876e-07 9.9999952e-01]
 [5.4120028e-05 9.9994588e-01]
 [1.5605697e-03 9.9843949e-01]
 [1.1460818e-05 9.9998856e-01]
 [2.4796105e-05 9.9997520e-01]
 [7.7532904e-06 9.9999225e-01]
 [1.5757898e-05 9.9998426e-01]
 [2.7736753e-05 9.9997222e-01]
 [1.2293216e-06 9.9999881e-01]]

Rep 1,001.745:
[[9.9193949e-01 8.0605429e-03]
 [9.9898022e-01 1.0196957e-03]
 [9.9998653e-01 1.3434867e-05]
 [8.2974774e-01 1.7025225e-01]
 [8.4450573e-02 9.1554940e-01]
 [7.1838480e-01 2.8161517e-01]
 [9.9999952e-01 5.1679342e-07]
 [9.9999893e-01 1.0364649e-06]
 [1.8273022e-04 9.9981731e-01]
 [1.0000000e+00 9.0523720e-13]
 [1.0000000e+00 2.5277404e-12]
 [9.9429107e-01 5.7089650e-03]]

Rep 1,001.269:
[[7.2756660e-04 9.9927241e-01]
 [6.0266802e-06 9.9999392e-01]
 [2.0003363e-03 9.9799961e-01]
 [4.5399946e-05 9.9995458e-01]
 [3.1409294e-07 9.9999964e-01]
 [2.4867972e-05 9.9997509e-01]
 [5.9780804e-04 9.9940217e-01]
 [7.4314245e-04 9.9925679e-01]
 [1.2653962e-06 9.9999869e-01]
 [9.8356426e-01 1.6435791e-02]
 [9.7822011e-01 2.1779945e-02]
 [4.2427117e-05 9.9995756e-01]]

Rep 1,001.230:
[[6.4993680e-01 3.5006326e-01]
 [1.0000000e+00 3.3601102e-10]
 [1.0000000e+00 1.4762642e-08]
 [1.6674609e-04 9.9983323e-01]
 [9.9999714e-01 2.8316672e-06]
 [9.9572670e-01 4.2733038e-03]
 [5.4470699e-02 9.4552928e-01]
 [4.5730895e-01 5.4269105e-01]
 [1.2538604e-04 9.9987459e-01]
 [9.9999082e-01 9.1359616e-06]
 [9.9999678e-01 3.2427679e-06]
 [9.9958283e-01 4.1720000e-04]]

Rep 1,001.464:
[[7.8369670e-02 9.2163032e-01]
 [1.0000000e+00 1.3105964e-09]
 [9.9999988e-01 6.8727630e-08]
 [1.8437615e-06 9.9999821e-01]
 [9.9998212e-01 1.7856031e-05]
 [9.5562768e-01 4.4372253e-02]
 [1.1920008e-02 9.8807997e-01]
 [7.2203314e-01 2.7796683e-01]
 [9.4487459e-06 9.9999058e-01]
 [9.9883419e-01 1.1658614e-03]
 [9.9997687e-01 2.3154798e-05]
 [8.5139298e-01 1.4860703e-01]]

Rep 1,001.277:
[[2.4462741e-02 9.7553730e-01]
 [7.6713365e-01 2.3286639e-01]
 [9.8611659e-01 1.3883409e-02]
 [9.6501112e-01 3.4988839e-02]
 [1.5398588e-03 9.9846017e-01]
 [1.6069462e-03 9.9839300e-01]
 [1.0000000e+00 5.3590488e-09]
 [9.9971479e-01 2.8519231e-04]
 [4.2383726e-05 9.9995756e-01]
 [1.0000000e+00 1.2136602e-13]
 [1.0000000e+00 1.8117985e-08]
 [6.9248319e-01 3.0751675e-01]]

Rep 1,001.316:
[[8.7201154e-01 1.2798846e-01]
 [9.9986148e-01 1.3854218e-04]
 [9.9911422e-01 8.8574411e-04]
 [9.8131609e-01 1.8683955e-02]
 [4.6958242e-02 9.5304179e-01]
 [1.5148709e-03 9.9848515e-01]
 [1.0000000e+00 1.2805612e-09]
 [9.9999368e-01 6.2701329e-06]
 [3.3737426e-06 9.9999666e-01]
 [1.0000000e+00 1.7820885e-14]
 [1.0000000e+00 2.2963270e-10]
 [7.4329980e-02 9.2567003e-01]]

Rep 1,001.295:
[[1.10723131e-05 9.99988914e-01]
 [1.37969619e-02 9.86203074e-01]
 [6.62334681e-01 3.37665319e-01]
 [3.03899560e-06 9.99997020e-01]
 [9.99176245e-06 9.99989986e-01]
 [1.85258614e-04 9.99814808e-01]
 [6.16474543e-04 9.99383450e-01]
 [4.37616063e-06 9.99995589e-01]
 [1.58878106e-06 9.99998450e-01]
 [8.88118446e-01 1.11881614e-01]
 [3.61998915e-03 9.96380031e-01]
 [4.32150102e-07 9.99999523e-01]]

Rep 1,001.363:
[[1.0094094e-03 9.9899060e-01]
 [9.1800183e-01 8.1998207e-02]
 [9.5111006e-01 4.8889916e-02]
 [2.1073324e-06 9.9999785e-01]
 [6.6863173e-03 9.9331373e-01]
 [4.8272480e-04 9.9951732e-01]
 [1.1042925e-03 9.9889565e-01]
 [4.0951271e-02 9.5904869e-01]
 [1.8191592e-07 9.9999976e-01]
 [9.8025399e-01 1.9745931e-02]
 [9.9670225e-01 3.2976838e-03]
 [8.9280400e-04 9.9910718e-01]]

Rep 1,001.153:
[[6.22999360e-05 9.99937654e-01]
 [4.22893256e-01 5.77106774e-01]
 [8.82884443e-01 1.17115565e-01]
 [1.22586550e-06 9.99998808e-01]
 [1.89176964e-04 9.99810874e-01]
 [3.68494075e-04 9.99631524e-01]
 [1.64708690e-05 9.99983549e-01]
 [9.61722981e-06 9.99990344e-01]
 [4.16774583e-06 9.99995828e-01]
 [8.30502395e-05 9.99916911e-01]
 [2.34548024e-05 9.99976516e-01]
 [2.41028647e-05 9.99975920e-01]]

Rep 1,001.202:
[[2.0543311e-03 9.9794573e-01]
 [9.5113313e-01 4.8866931e-02]
 [9.2207229e-01 7.7927761e-02]
 [2.2675015e-02 9.7732496e-01]
 [6.5592877e-03 9.9344069e-01]
 [7.6838443e-04 9.9923158e-01]
 [9.9998856e-01 1.1453290e-05]
 [9.8032939e-01 1.9670608e-02]
 [2.5898628e-06 9.9999738e-01]
 [1.0000000e+00 6.8932984e-11]
 [9.9999952e-01 4.3225810e-07]
 [2.8819766e-02 9.7118020e-01]]

Rep 1,001.229:
[[4.1182630e-02 9.5881736e-01]
 [9.6352041e-01 3.6479566e-02]
 [9.1952491e-01 8.0475047e-02]
 [2.4818897e-03 9.9751818e-01]
 [7.8537120e-03 9.9214631e-01]
 [1.3627892e-03 9.9863714e-01]
 [9.9970335e-01 2.9660875e-04]
 [9.9321681e-01 6.7831208e-03]
 [7.5395210e-06 9.9999249e-01]
 [1.0000000e+00 1.4133189e-09]
 [9.9999988e-01 8.3823544e-08]
 [3.9709777e-02 9.6029025e-01]]

Rep 1,001.382:
[[9.29361582e-01 7.06384331e-02]
 [1.00000000e+00 3.96926207e-11]
 [9.99999881e-01 1.27851237e-07]
 [7.80036626e-03 9.92199600e-01]
 [9.99991179e-01 8.87366514e-06]
 [4.15658712e-01 5.84341288e-01]
 [8.47934127e-01 1.52065843e-01]
 [4.46847361e-03 9.95531499e-01]
 [1.43594536e-06 9.99998569e-01]
 [9.99999285e-01 6.94188884e-07]
 [9.91851509e-01 8.14844482e-03]
 [1.18542075e-01 8.81457984e-01]]

Rep 1,001.140:
[[8.8395673e-01 1.1604325e-01]
 [1.0000000e+00 6.7020095e-10]
 [1.0000000e+00 1.4852383e-10]
 [2.2371982e-03 9.9776280e-01]
 [9.9997902e-01 2.1012935e-05]
 [9.9995124e-01 4.8755115e-05]
 [1.8178785e-03 9.9818218e-01]
 [2.9501395e-04 9.9970502e-01]
 [7.1387459e-04 9.9928612e-01]
 [9.9898821e-01 1.0117355e-03]
 [9.4269729e-01 5.7302650e-02]
 [8.0392295e-01 1.9607702e-01]]

Rep 1,001.166:
[[3.7996143e-03 9.9620038e-01]
 [6.2295800e-04 9.9937707e-01]
 [3.6474672e-01 6.3525325e-01]
 [2.6803015e-04 9.9973196e-01]
 [8.2712650e-06 9.9999177e-01]
 [7.5160491e-04 9.9924833e-01]
 [8.1846714e-01 1.8153284e-01]
 [7.7636737e-01 2.2363266e-01]
 [1.2092926e-05 9.9998796e-01]
 [9.9999976e-01 2.5378262e-07]
 [9.9999964e-01 3.6300779e-07]
 [2.9320776e-01 7.0679229e-01]]

Rep 1,001.118:
[[9.02334630e-01 9.76653621e-02]
 [9.99421239e-01 5.78770705e-04]
 [9.99967217e-01 3.28053138e-05]
 [9.96302843e-01 3.69715388e-03]
 [1.99247934e-02 9.80075240e-01]
 [7.61582628e-02 9.23841774e-01]
 [1.00000000e+00 6.10758466e-09]
 [9.99991059e-01 8.92803200e-06]
 [9.83605569e-05 9.99901652e-01]
 [1.00000000e+00 1.38775955e-14]
 [1.00000000e+00 2.41867273e-11]
 [9.99969244e-01 3.07798255e-05]]

Rep 1,001.338:
[[9.5478899e-05 9.9990451e-01]
 [4.2284182e-03 9.9577159e-01]
 [1.3695578e-01 8.6304414e-01]
 [6.9549560e-07 9.9999928e-01]
 [8.8348052e-06 9.9999118e-01]
 [8.0557533e-05 9.9991941e-01]
 [8.7520707e-04 9.9912483e-01]
 [1.9699757e-03 9.9803001e-01]
 [4.8357174e-07 9.9999952e-01]
 [9.6609735e-01 3.3902586e-02]
 [9.5808458e-01 4.1915454e-02]
 [5.1979765e-07 9.9999952e-01]]

Rep 1,001.304:
[[9.0334341e-02 9.0966564e-01]
 [9.9771333e-01 2.2866859e-03]
 [9.9994409e-01 5.5881032e-05]
 [3.7983108e-07 9.9999964e-01]
 [4.7516269e-03 9.9524838e-01]
 [2.2946933e-02 9.7705305e-01]
 [3.2852006e-06 9.9999666e-01]
 [9.1191278e-06 9.9999094e-01]
 [3.2634609e-06 9.9999678e-01]
 [1.2258877e-04 9.9987745e-01]
 [2.2858467e-04 9.9977142e-01]
 [4.2240099e-06 9.9999583e-01]]

Rep 1,001.259:
[[9.9998748e-01 1.2571865e-05]
 [1.0000000e+00 9.3750110e-09]
 [9.9999297e-01 7.0470796e-06]
 [9.9729687e-01 2.7030983e-03]
 [9.9589157e-01 4.1084131e-03]
 [9.0098098e-02 9.0990192e-01]
 [1.0000000e+00 2.4132099e-09]
 [9.9999976e-01 2.8685875e-07]
 [8.8810538e-07 9.9999917e-01]
 [1.0000000e+00 1.6039446e-14]
 [1.0000000e+00 3.2100791e-12]
 [1.1725488e-02 9.8827446e-01]]

Rep 1,001.927:
[[6.3933772e-03 9.9360657e-01]
 [9.9198252e-01 8.0174524e-03]
 [8.2631147e-01 1.7368850e-01]
 [2.0697992e-03 9.9793017e-01]
 [8.1663273e-02 9.1833669e-01]
 [1.7282680e-04 9.9982721e-01]
 [9.7709173e-01 2.2908222e-02]
 [9.3997675e-01 6.0023237e-02]
 [1.2874162e-06 9.9999869e-01]
 [1.0000000e+00 4.7043628e-08]
 [9.9999881e-01 1.1618466e-06]
 [9.9537969e-01 4.6203425e-03]]

Rep 1,001.502:
[[8.58971325e-05 9.99914050e-01]
 [5.46784580e-01 4.53215390e-01]
 [9.87585187e-01 1.24147674e-02]
 [3.51899666e-06 9.99996424e-01]
 [2.10861937e-04 9.99789178e-01]
 [2.43619503e-03 9.97563839e-01]
 [6.11419964e-05 9.99938846e-01]
 [1.07532642e-05 9.99989271e-01]
 [1.37451489e-05 9.99986291e-01]
 [4.75830748e-04 9.99524236e-01]
 [3.49107140e-05 9.99965072e-01]
 [1.49932575e-05 9.99984980e-01]]

Adjacency matrix from edge pred is as follows:

Rep 1,001.444:
[[[0.0000000e+00 0.0000000e+00]
  [9.9943739e-01 5.6263676e-04]
  [9.9995506e-01 4.4997629e-05]
  [9.9999583e-01 4.2252586e-06]]

 [[9.9992049e-01 7.9552156e-05]
  [0.0000000e+00 0.0000000e+00]
  [9.9491991e-02 9.0050793e-01]
  [2.3560321e-01 7.6439679e-01]]

 [[1.0000000e+00 5.4844556e-11]
  [1.0000000e+00 1.7680435e-08]
  [0.0000000e+00 0.0000000e+00]
  [2.4055560e-05 9.9997592e-01]]

 [[1.0000000e+00 1.1748047e-16]
  [1.0000000e+00 9.8858069e-14]
  [9.9930048e-01 6.9945911e-04]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.652:
[[[0.0000000e+00 0.0000000e+00]
  [8.6245501e-01 1.3754502e-01]
  [9.9797279e-01 2.0271915e-03]
  [9.9999475e-01 5.2273140e-06]]

 [[3.8209136e-04 9.9961793e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.1179364e-02 9.8882061e-01]
  [3.8028327e-01 6.1971670e-01]]

 [[1.2147692e-03 9.9878520e-01]
  [1.2013693e-04 9.9987984e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.9635085e-05 9.9998033e-01]]

 [[9.7380859e-01 2.6191473e-02]
  [3.6167252e-01 6.3832754e-01]
  [9.7786879e-06 9.9999022e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.446:
[[[0.0000000e+00 0.0000000e+00]
  [2.2841216e-06 9.9999774e-01]
  [7.2409521e-06 9.9999273e-01]
  [2.2294288e-04 9.9977702e-01]]

 [[3.7494893e-04 9.9962509e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.2710709e-07 9.9999976e-01]
  [2.8566810e-06 9.9999714e-01]]

 [[6.0637131e-02 9.3936288e-01]
  [3.1674560e-06 9.9999678e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.9603170e-08 1.0000000e+00]]

 [[9.9987304e-01 1.2691953e-04]
  [3.7594926e-02 9.6240503e-01]
  [1.4585833e-06 9.9999857e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.131:
[[[0.0000000e+00 0.0000000e+00]
  [9.9986291e-01 1.3704863e-04]
  [1.0000000e+00 1.3089307e-08]
  [1.0000000e+00 1.7315726e-08]]

 [[8.6055184e-03 9.9139446e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.9975878e-01 2.4123302e-04]
  [9.6732116e-01 3.2678891e-02]]

 [[4.9296813e-04 9.9950707e-01]
  [1.4602308e-03 9.9853981e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.8548138e-05 9.9990141e-01]]

 [[9.9688280e-01 3.1171963e-03]
  [9.7781312e-01 2.2186866e-02]
  [8.0172575e-01 1.9827427e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.460:
[[[0.0000000e+00 0.0000000e+00]
  [1.1906139e-02 9.8809391e-01]
  [9.9998772e-01 1.2332387e-05]
  [9.9989760e-01 1.0242363e-04]]

 [[3.3284455e-06 9.9999666e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.4765961e-01 5.2340358e-02]
  [7.8156650e-02 9.2184341e-01]]

 [[2.1539092e-01 7.8460908e-01]
  [9.8965257e-01 1.0347403e-02]
  [0.0000000e+00 0.0000000e+00]
  [9.1983230e-07 9.9999905e-01]]

 [[9.9997878e-01 2.1277214e-05]
  [9.9999964e-01 3.9525725e-07]
  [1.2730262e-01 8.7269735e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.235:
[[[0.0000000e+00 0.0000000e+00]
  [1.9886492e-01 8.0113512e-01]
  [9.9999440e-01 5.5468549e-06]
  [9.9999559e-01 4.4318431e-06]]

 [[8.1497485e-05 9.9991846e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.2029637e-01 7.9703651e-02]
  [1.9804297e-01 8.0195707e-01]]

 [[8.9255234e-05 9.9991071e-01]
  [7.9027719e-05 9.9992096e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.8641398e-06 9.9999809e-01]]

 [[1.0228628e-01 8.9771372e-01]
  [9.9903131e-03 9.9000973e-01]
  [1.6173889e-03 9.9838257e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.386:
[[[0.0000000e+00 0.0000000e+00]
  [9.9227285e-01 7.7271205e-03]
  [1.0000000e+00 1.3838207e-11]
  [1.0000000e+00 8.5671695e-12]]

 [[3.8336450e-03 9.9616635e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.9999988e-01 1.4140726e-07]
  [9.9999654e-01 3.4928510e-06]]

 [[7.6083420e-03 9.9239165e-01]
  [1.3788536e-02 9.8621142e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.2302984e-02 9.8769706e-01]]

 [[9.9997985e-01 2.0161913e-05]
  [9.9987507e-01 1.2491252e-04]
  [9.9952364e-01 4.7634874e-04]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.350:
[[[0.00000000e+00 0.00000000e+00]
  [9.88747418e-01 1.12526165e-02]
  [8.78874540e-01 1.21125437e-01]
  [6.35757148e-01 3.64242822e-01]]

 [[9.99653935e-01 3.46101791e-04]
  [0.00000000e+00 0.00000000e+00]
  [1.78416725e-03 9.98215854e-01]
  [2.46937343e-05 9.99975324e-01]]

 [[1.00000000e+00 5.49170154e-10]
  [9.99997020e-01 2.92242044e-06]
  [0.00000000e+00 0.00000000e+00]
  [1.42134950e-05 9.99985814e-01]]

 [[1.00000000e+00 3.28593760e-14]
  [1.00000000e+00 2.71458939e-10]
  [9.98981416e-01 1.01850333e-03]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.374:
[[[0.0000000e+00 0.0000000e+00]
  [9.6317911e-01 3.6820892e-02]
  [1.0000000e+00 8.5477181e-11]
  [1.0000000e+00 1.5923389e-08]]

 [[2.8321341e-01 7.1678662e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.9999368e-01 6.2826125e-06]
  [9.7375286e-01 2.6247172e-02]]

 [[9.9999726e-01 2.7374076e-06]
  [9.9820232e-01 1.7976621e-03]
  [0.0000000e+00 0.0000000e+00]
  [4.0677505e-06 9.9999595e-01]]

 [[1.0000000e+00 1.5363379e-11]
  [1.0000000e+00 1.8773184e-08]
  [1.6960035e-03 9.9830401e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.279:
[[[0.0000000e+00 0.0000000e+00]
  [2.4047051e-01 7.5952947e-01]
  [9.9999821e-01 1.7361285e-06]
  [9.9936658e-01 6.3345011e-04]]

 [[2.9702897e-03 9.9702966e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.3885475e-01 6.1145283e-02]
  [2.2647434e-03 9.9773526e-01]]

 [[6.2453055e-01 3.7546948e-01]
  [4.0298584e-03 9.9597013e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.1647328e-07 9.9999976e-01]]

 [[9.9999130e-01 8.7066410e-06]
  [9.6508837e-01 3.4911610e-02]
  [4.3674870e-04 9.9956328e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.281:
[[[0.0000000e+00 0.0000000e+00]
  [1.7230584e-05 9.9998271e-01]
  [8.9472914e-03 9.9105269e-01]
  [1.4531825e-01 8.5468173e-01]]

 [[2.7749849e-05 9.9997222e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.8550770e-05 9.9996150e-01]
  [1.5945923e-04 9.9984050e-01]]

 [[1.1772504e-03 9.9882275e-01]
  [4.1217086e-06 9.9999583e-01]
  [0.0000000e+00 0.0000000e+00]
  [7.9968040e-06 9.9999201e-01]]

 [[9.2986399e-01 7.0136040e-02]
  [2.8040267e-03 9.9719596e-01]
  [2.2859302e-05 9.9997711e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.592:
[[[0.0000000e+00 0.0000000e+00]
  [5.6231223e-02 9.4376880e-01]
  [7.4317527e-01 2.5682470e-01]
  [7.7876765e-01 2.2123231e-01]]

 [[2.6106384e-06 9.9999738e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.0174775e-03 9.9698251e-01]
  [1.5040224e-04 9.9984956e-01]]

 [[8.8851535e-05 9.9991119e-01]
  [1.7642645e-02 9.8235738e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.6756884e-07 9.9999964e-01]]

 [[1.3947877e-01 8.6052126e-01]
  [9.7738588e-01 2.2614149e-02]
  [9.6674100e-04 9.9903333e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.259:
[[[0.0000000e+00 0.0000000e+00]
  [8.3823675e-01 1.6176331e-01]
  [1.0000000e+00 4.1442831e-08]
  [9.9999487e-01 5.1698944e-06]]

 [[1.5271767e-03 9.9847287e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.9519473e-01 4.8052836e-03]
  [4.9406134e-02 9.5059389e-01]]

 [[7.5808761e-04 9.9924195e-01]
  [1.6948725e-05 9.9998307e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.5254723e-07 9.9999905e-01]]

 [[9.8509294e-01 1.4907108e-02]
  [1.6306212e-02 9.8369378e-01]
  [1.0854104e-02 9.8914587e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.108:
[[[0.0000000e+00 0.0000000e+00]
  [8.6430323e-01 1.3569681e-01]
  [9.9728537e-01 2.7146626e-03]
  [9.9987912e-01 1.2089701e-04]]

 [[9.9998665e-01 1.3327979e-05]
  [0.0000000e+00 0.0000000e+00]
  [1.1233073e-02 9.8876697e-01]
  [4.0217269e-02 9.5978278e-01]]

 [[1.0000000e+00 1.5713655e-10]
  [9.9995708e-01 4.2856122e-05]
  [0.0000000e+00 0.0000000e+00]
  [3.6511751e-04 9.9963486e-01]]

 [[1.0000000e+00 7.8092192e-16]
  [1.0000000e+00 1.9046699e-10]
  [9.9999774e-01 2.2144507e-06]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.119:
[[[0.0000000e+00 0.0000000e+00]
  [2.3088390e-03 9.9769115e-01]
  [9.3585569e-01 6.4144298e-02]
  [1.8225308e-01 8.1774688e-01]]

 [[3.9930820e-01 6.0069180e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.8530160e-02 9.7146982e-01]
  [1.5228300e-04 9.9984765e-01]]

 [[9.9955553e-01 4.4445644e-04]
  [3.9067637e-02 9.6093237e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.0789264e-06 9.9999893e-01]]

 [[1.0000000e+00 4.2354933e-09]
  [9.9818617e-01 1.8138585e-03]
  [2.0240394e-03 9.9797601e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.141:
[[[0.0000000e+00 0.0000000e+00]
  [2.5841754e-04 9.9974161e-01]
  [2.2581860e-01 7.7418137e-01]
  [9.6043134e-01 3.9568681e-02]]

 [[1.7708280e-06 9.9999821e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.9172879e-05 9.9996078e-01]
  [4.4974269e-04 9.9955028e-01]]

 [[1.2028183e-04 9.9987972e-01]
  [2.2123195e-04 9.9977881e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.0822168e-06 9.9999893e-01]]

 [[8.4243566e-01 1.5756436e-01]
  [6.1070353e-01 3.8929641e-01]
  [2.5074268e-04 9.9974924e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.237:
[[[0.00000000e+00 0.00000000e+00]
  [1.07659864e-04 9.99892354e-01]
  [6.34448111e-01 3.65551919e-01]
  [9.92612481e-01 7.38750771e-03]]

 [[3.17858576e-06 9.99996781e-01]
  [0.00000000e+00 0.00000000e+00]
  [2.66370946e-04 9.99733627e-01]
  [9.98660573e-04 9.99001324e-01]]

 [[1.58422772e-05 9.99984145e-01]
  [2.07586459e-06 9.99997973e-01]
  [0.00000000e+00 0.00000000e+00]
  [1.65368328e-06 9.99998331e-01]]

 [[1.37993440e-04 9.99861956e-01]
  [4.50565403e-06 9.99995470e-01]
  [4.55531881e-06 9.99995470e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.453:
[[[0.0000000e+00 0.0000000e+00]
  [9.9999630e-01 3.6875142e-06]
  [9.9917525e-01 8.2475756e-04]
  [9.9779344e-01 2.2066054e-03]]

 [[9.9999964e-01 3.3660737e-07]
  [0.0000000e+00 0.0000000e+00]
  [9.1997599e-03 9.9080020e-01]
  [3.1900310e-04 9.9968100e-01]]

 [[1.0000000e+00 9.2613178e-13]
  [1.0000000e+00 1.1986797e-08]
  [0.0000000e+00 0.0000000e+00]
  [2.6462720e-05 9.9997354e-01]]

 [[1.0000000e+00 6.7585182e-17]
  [1.0000000e+00 8.3964790e-13]
  [9.9823558e-01 1.7644522e-03]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.429:
[[[0.0000000e+00 0.0000000e+00]
  [1.4803810e-05 9.9998522e-01]
  [4.6249588e-06 9.9999535e-01]
  [7.5589829e-05 9.9992442e-01]]

 [[6.8065718e-05 9.9993193e-01]
  [0.0000000e+00 0.0000000e+00]
  [4.2706125e-07 9.9999952e-01]
  [3.4893153e-06 9.9999654e-01]]

 [[3.2123697e-03 9.9678767e-01]
  [3.4277902e-05 9.9996567e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.8009009e-08 1.0000000e+00]]

 [[9.9765778e-01 2.3421536e-03]
  [5.5001795e-01 4.4998202e-01]
  [1.4988922e-06 9.9999845e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.220:
[[[0.00000000e+00 0.00000000e+00]
  [9.50429559e-01 4.95704971e-02]
  [1.00000000e+00 2.85565654e-10]
  [9.99996781e-01 3.20695767e-06]]

 [[6.54360652e-03 9.93456423e-01]
  [0.00000000e+00 0.00000000e+00]
  [9.99929428e-01 7.05335915e-05]
  [4.57655080e-02 9.54234540e-01]]

 [[1.20777301e-01 8.79222751e-01]
  [3.13844357e-04 9.99686122e-01]
  [0.00000000e+00 0.00000000e+00]
  [7.45461591e-07 9.99999285e-01]]

 [[9.99989152e-01 1.07943615e-05]
  [9.10410106e-01 8.95898566e-02]
  [3.84785682e-01 6.15214288e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.315:
[[[0.0000000e+00 0.0000000e+00]
  [3.7224865e-01 6.2775135e-01]
  [9.9943084e-01 5.6917564e-04]
  [9.9983454e-01 1.6546792e-04]]

 [[5.1039086e-05 9.9994898e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.7050540e-02 9.0294945e-01]
  [6.9597205e-03 9.9304020e-01]]

 [[1.5086372e-05 9.9998486e-01]
  [5.6751296e-06 9.9999428e-01]
  [0.0000000e+00 0.0000000e+00]
  [7.9126949e-07 9.9999917e-01]]

 [[1.1870644e-02 9.8812938e-01]
  [3.7380934e-04 9.9962616e-01]
  [6.3016887e-05 9.9993694e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.197:
[[[0.0000000e+00 0.0000000e+00]
  [2.4045305e-02 9.7595471e-01]
  [9.9888271e-01 1.1172609e-03]
  [9.9947017e-01 5.2989402e-04]]

 [[2.0689417e-02 9.7931057e-01]
  [0.0000000e+00 0.0000000e+00]
  [7.6983370e-02 9.2301667e-01]
  [3.7200317e-02 9.6279967e-01]]

 [[9.9985933e-01 1.4062607e-04]
  [9.9594301e-01 4.0569883e-03]
  [0.0000000e+00 0.0000000e+00]
  [6.0954713e-05 9.9993908e-01]]

 [[1.0000000e+00 2.7069991e-10]
  [1.0000000e+00 1.5596086e-08]
  [9.9996018e-01 3.9818435e-05]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.390:
[[[0.0000000e+00 0.0000000e+00]
  [7.5031079e-02 9.2496896e-01]
  [9.9777752e-01 2.2224393e-03]
  [9.9994791e-01 5.2078205e-05]]

 [[1.2284308e-06 9.9999881e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.3544076e-03 9.9664551e-01]
  [1.8360551e-02 9.8163950e-01]]

 [[8.3669554e-05 9.9991632e-01]
  [7.6509896e-05 9.9992347e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.3513730e-06 9.9999869e-01]]

 [[2.5982794e-01 7.4017203e-01]
  [1.0804577e-01 8.9195418e-01]
  [1.8712626e-06 9.9999809e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.325:
[[[0.00000000e+00 0.00000000e+00]
  [8.90264332e-01 1.09735645e-01]
  [1.00000000e+00 1.25320876e-09]
  [9.99999881e-01 1.04215474e-07]]

 [[8.38363245e-02 9.16163683e-01]
  [0.00000000e+00 0.00000000e+00]
  [9.99831676e-01 1.68245897e-04]
  [8.31003487e-01 1.68996498e-01]]

 [[9.99992728e-01 7.22339155e-06]
  [9.93760407e-01 6.23954413e-03]
  [0.00000000e+00 0.00000000e+00]
  [1.82941847e-06 9.99998212e-01]]

 [[1.00000000e+00 5.08079342e-11]
  [9.99999881e-01 7.56933076e-08]
  [2.80291133e-04 9.99719679e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.235:
[[[0.00000000e+00 0.00000000e+00]
  [7.21827091e-05 9.99927759e-01]
  [2.05437973e-04 9.99794543e-01]
  [1.04572595e-04 9.99895453e-01]]

 [[2.63685542e-05 9.99973655e-01]
  [0.00000000e+00 0.00000000e+00]
  [3.16757287e-05 9.99968290e-01]
  [1.34213178e-05 9.99986529e-01]]

 [[7.31263263e-03 9.92687345e-01]
  [3.39034898e-03 9.96609688e-01]
  [0.00000000e+00 0.00000000e+00]
  [5.03981994e-07 9.99999523e-01]]

 [[9.99084234e-01 9.15772631e-04]
  [9.91736710e-01 8.26330576e-03]
  [4.84609482e-05 9.99951482e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.218:
[[[0.0000000e+00 0.0000000e+00]
  [9.9896753e-01 1.0324999e-03]
  [1.0000000e+00 1.1505529e-13]
  [1.0000000e+00 6.0113816e-09]]

 [[2.5228906e-02 9.7477108e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.0000000e+00 3.9153853e-08]
  [9.3501616e-01 6.4983875e-02]]

 [[9.9172008e-01 8.2799299e-03]
  [1.4304551e-01 8.5695451e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.2671986e-05 9.9997735e-01]]

 [[1.0000000e+00 6.0749956e-09]
  [9.9999380e-01 6.2509621e-06]
  [9.9277109e-01 7.2288993e-03]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.110:
[[[0.0000000e+00 0.0000000e+00]
  [1.8563902e-03 9.9814367e-01]
  [3.3633888e-01 6.6366118e-01]
  [9.9565589e-01 4.3440834e-03]]

 [[1.0446040e-06 9.9999893e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.8538927e-05 9.9990141e-01]
  [5.0149485e-03 9.9498510e-01]]

 [[1.4173807e-05 9.9998581e-01]
  [5.7933048e-05 9.9994206e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.3171137e-05 9.9998689e-01]]

 [[4.5049579e-05 9.9995494e-01]
  [1.5314214e-04 9.9984682e-01]
  [1.6067061e-06 9.9999845e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.335:
[[[0.0000000e+00 0.0000000e+00]
  [1.1561014e-04 9.9988437e-01]
  [9.5998193e-04 9.9903995e-01]
  [2.7997103e-01 7.2002894e-01]]

 [[3.8987247e-03 9.9610126e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.0514261e-05 9.9998951e-01]
  [1.9509812e-04 9.9980491e-01]]

 [[9.9866283e-01 1.3371374e-03]
  [2.4633075e-01 7.5366926e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.5749016e-06 9.9999738e-01]]

 [[1.0000000e+00 2.8069114e-09]
  [9.9998641e-01 1.3609732e-05]
  [9.3231484e-02 9.0676850e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.342:
[[[0.0000000e+00 0.0000000e+00]
  [9.4006573e-05 9.9990594e-01]
  [9.9999201e-01 7.9720949e-06]
  [9.9999964e-01 3.3643374e-07]]

 [[6.7848191e-07 9.9999928e-01]
  [0.0000000e+00 0.0000000e+00]
  [7.1704543e-01 2.8295463e-01]
  [8.8891989e-01 1.1108015e-01]]

 [[2.5314002e-03 9.9746859e-01]
  [5.0457325e-03 9.9495429e-01]
  [0.0000000e+00 0.0000000e+00]
  [2.0472592e-06 9.9999797e-01]]

 [[9.9784327e-01 2.1567324e-03]
  [9.9295616e-01 7.0438734e-03]
  [3.7570624e-04 9.9962425e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.222:
[[[0.0000000e+00 0.0000000e+00]
  [4.1608475e-04 9.9958390e-01]
  [2.1582215e-01 7.8417790e-01]
  [9.8891234e-01 1.1087657e-02]]

 [[4.6450876e-07 9.9999952e-01]
  [0.0000000e+00 0.0000000e+00]
  [5.4120028e-05 9.9994588e-01]
  [1.5605697e-03 9.9843949e-01]]

 [[1.1460818e-05 9.9998856e-01]
  [2.4796105e-05 9.9997520e-01]
  [0.0000000e+00 0.0000000e+00]
  [7.7532904e-06 9.9999225e-01]]

 [[1.5757898e-05 9.9998426e-01]
  [2.7736753e-05 9.9997222e-01]
  [1.2293216e-06 9.9999881e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.745:
[[[0.0000000e+00 0.0000000e+00]
  [9.9193949e-01 8.0605429e-03]
  [9.9898022e-01 1.0196957e-03]
  [9.9998653e-01 1.3434867e-05]]

 [[8.2974774e-01 1.7025225e-01]
  [0.0000000e+00 0.0000000e+00]
  [8.4450573e-02 9.1554940e-01]
  [7.1838480e-01 2.8161517e-01]]

 [[9.9999952e-01 5.1679342e-07]
  [9.9999893e-01 1.0364649e-06]
  [0.0000000e+00 0.0000000e+00]
  [1.8273022e-04 9.9981731e-01]]

 [[1.0000000e+00 9.0523720e-13]
  [1.0000000e+00 2.5277404e-12]
  [9.9429107e-01 5.7089650e-03]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.269:
[[[0.0000000e+00 0.0000000e+00]
  [7.2756660e-04 9.9927241e-01]
  [6.0266802e-06 9.9999392e-01]
  [2.0003363e-03 9.9799961e-01]]

 [[4.5399946e-05 9.9995458e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.1409294e-07 9.9999964e-01]
  [2.4867972e-05 9.9997509e-01]]

 [[5.9780804e-04 9.9940217e-01]
  [7.4314245e-04 9.9925679e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.2653962e-06 9.9999869e-01]]

 [[9.8356426e-01 1.6435791e-02]
  [9.7822011e-01 2.1779945e-02]
  [4.2427117e-05 9.9995756e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.230:
[[[0.0000000e+00 0.0000000e+00]
  [6.4993680e-01 3.5006326e-01]
  [1.0000000e+00 3.3601102e-10]
  [1.0000000e+00 1.4762642e-08]]

 [[1.6674609e-04 9.9983323e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.9999714e-01 2.8316672e-06]
  [9.9572670e-01 4.2733038e-03]]

 [[5.4470699e-02 9.4552928e-01]
  [4.5730895e-01 5.4269105e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.2538604e-04 9.9987459e-01]]

 [[9.9999082e-01 9.1359616e-06]
  [9.9999678e-01 3.2427679e-06]
  [9.9958283e-01 4.1720000e-04]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.464:
[[[0.0000000e+00 0.0000000e+00]
  [7.8369670e-02 9.2163032e-01]
  [1.0000000e+00 1.3105964e-09]
  [9.9999988e-01 6.8727630e-08]]

 [[1.8437615e-06 9.9999821e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.9998212e-01 1.7856031e-05]
  [9.5562768e-01 4.4372253e-02]]

 [[1.1920008e-02 9.8807997e-01]
  [7.2203314e-01 2.7796683e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.4487459e-06 9.9999058e-01]]

 [[9.9883419e-01 1.1658614e-03]
  [9.9997687e-01 2.3154798e-05]
  [8.5139298e-01 1.4860703e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.277:
[[[0.0000000e+00 0.0000000e+00]
  [2.4462741e-02 9.7553730e-01]
  [7.6713365e-01 2.3286639e-01]
  [9.8611659e-01 1.3883409e-02]]

 [[9.6501112e-01 3.4988839e-02]
  [0.0000000e+00 0.0000000e+00]
  [1.5398588e-03 9.9846017e-01]
  [1.6069462e-03 9.9839300e-01]]

 [[1.0000000e+00 5.3590488e-09]
  [9.9971479e-01 2.8519231e-04]
  [0.0000000e+00 0.0000000e+00]
  [4.2383726e-05 9.9995756e-01]]

 [[1.0000000e+00 1.2136602e-13]
  [1.0000000e+00 1.8117985e-08]
  [6.9248319e-01 3.0751675e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.316:
[[[0.0000000e+00 0.0000000e+00]
  [8.7201154e-01 1.2798846e-01]
  [9.9986148e-01 1.3854218e-04]
  [9.9911422e-01 8.8574411e-04]]

 [[9.8131609e-01 1.8683955e-02]
  [0.0000000e+00 0.0000000e+00]
  [4.6958242e-02 9.5304179e-01]
  [1.5148709e-03 9.9848515e-01]]

 [[1.0000000e+00 1.2805612e-09]
  [9.9999368e-01 6.2701329e-06]
  [0.0000000e+00 0.0000000e+00]
  [3.3737426e-06 9.9999666e-01]]

 [[1.0000000e+00 1.7820885e-14]
  [1.0000000e+00 2.2963270e-10]
  [7.4329980e-02 9.2567003e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.295:
[[[0.00000000e+00 0.00000000e+00]
  [1.10723131e-05 9.99988914e-01]
  [1.37969619e-02 9.86203074e-01]
  [6.62334681e-01 3.37665319e-01]]

 [[3.03899560e-06 9.99997020e-01]
  [0.00000000e+00 0.00000000e+00]
  [9.99176245e-06 9.99989986e-01]
  [1.85258614e-04 9.99814808e-01]]

 [[6.16474543e-04 9.99383450e-01]
  [4.37616063e-06 9.99995589e-01]
  [0.00000000e+00 0.00000000e+00]
  [1.58878106e-06 9.99998450e-01]]

 [[8.88118446e-01 1.11881614e-01]
  [3.61998915e-03 9.96380031e-01]
  [4.32150102e-07 9.99999523e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.363:
[[[0.0000000e+00 0.0000000e+00]
  [1.0094094e-03 9.9899060e-01]
  [9.1800183e-01 8.1998207e-02]
  [9.5111006e-01 4.8889916e-02]]

 [[2.1073324e-06 9.9999785e-01]
  [0.0000000e+00 0.0000000e+00]
  [6.6863173e-03 9.9331373e-01]
  [4.8272480e-04 9.9951732e-01]]

 [[1.1042925e-03 9.9889565e-01]
  [4.0951271e-02 9.5904869e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.8191592e-07 9.9999976e-01]]

 [[9.8025399e-01 1.9745931e-02]
  [9.9670225e-01 3.2976838e-03]
  [8.9280400e-04 9.9910718e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.153:
[[[0.00000000e+00 0.00000000e+00]
  [6.22999360e-05 9.99937654e-01]
  [4.22893256e-01 5.77106774e-01]
  [8.82884443e-01 1.17115565e-01]]

 [[1.22586550e-06 9.99998808e-01]
  [0.00000000e+00 0.00000000e+00]
  [1.89176964e-04 9.99810874e-01]
  [3.68494075e-04 9.99631524e-01]]

 [[1.64708690e-05 9.99983549e-01]
  [9.61722981e-06 9.99990344e-01]
  [0.00000000e+00 0.00000000e+00]
  [4.16774583e-06 9.99995828e-01]]

 [[8.30502395e-05 9.99916911e-01]
  [2.34548024e-05 9.99976516e-01]
  [2.41028647e-05 9.99975920e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.202:
[[[0.0000000e+00 0.0000000e+00]
  [2.0543311e-03 9.9794573e-01]
  [9.5113313e-01 4.8866931e-02]
  [9.2207229e-01 7.7927761e-02]]

 [[2.2675015e-02 9.7732496e-01]
  [0.0000000e+00 0.0000000e+00]
  [6.5592877e-03 9.9344069e-01]
  [7.6838443e-04 9.9923158e-01]]

 [[9.9998856e-01 1.1453290e-05]
  [9.8032939e-01 1.9670608e-02]
  [0.0000000e+00 0.0000000e+00]
  [2.5898628e-06 9.9999738e-01]]

 [[1.0000000e+00 6.8932984e-11]
  [9.9999952e-01 4.3225810e-07]
  [2.8819766e-02 9.7118020e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.229:
[[[0.0000000e+00 0.0000000e+00]
  [4.1182630e-02 9.5881736e-01]
  [9.6352041e-01 3.6479566e-02]
  [9.1952491e-01 8.0475047e-02]]

 [[2.4818897e-03 9.9751818e-01]
  [0.0000000e+00 0.0000000e+00]
  [7.8537120e-03 9.9214631e-01]
  [1.3627892e-03 9.9863714e-01]]

 [[9.9970335e-01 2.9660875e-04]
  [9.9321681e-01 6.7831208e-03]
  [0.0000000e+00 0.0000000e+00]
  [7.5395210e-06 9.9999249e-01]]

 [[1.0000000e+00 1.4133189e-09]
  [9.9999988e-01 8.3823544e-08]
  [3.9709777e-02 9.6029025e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.382:
[[[0.00000000e+00 0.00000000e+00]
  [9.29361582e-01 7.06384331e-02]
  [1.00000000e+00 3.96926207e-11]
  [9.99999881e-01 1.27851237e-07]]

 [[7.80036626e-03 9.92199600e-01]
  [0.00000000e+00 0.00000000e+00]
  [9.99991179e-01 8.87366514e-06]
  [4.15658712e-01 5.84341288e-01]]

 [[8.47934127e-01 1.52065843e-01]
  [4.46847361e-03 9.95531499e-01]
  [0.00000000e+00 0.00000000e+00]
  [1.43594536e-06 9.99998569e-01]]

 [[9.99999285e-01 6.94188884e-07]
  [9.91851509e-01 8.14844482e-03]
  [1.18542075e-01 8.81457984e-01]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.140:
[[[0.0000000e+00 0.0000000e+00]
  [8.8395673e-01 1.1604325e-01]
  [1.0000000e+00 6.7020095e-10]
  [1.0000000e+00 1.4852383e-10]]

 [[2.2371982e-03 9.9776280e-01]
  [0.0000000e+00 0.0000000e+00]
  [9.9997902e-01 2.1012935e-05]
  [9.9995124e-01 4.8755115e-05]]

 [[1.8178785e-03 9.9818218e-01]
  [2.9501395e-04 9.9970502e-01]
  [0.0000000e+00 0.0000000e+00]
  [7.1387459e-04 9.9928612e-01]]

 [[9.9898821e-01 1.0117355e-03]
  [9.4269729e-01 5.7302650e-02]
  [8.0392295e-01 1.9607702e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.166:
[[[0.0000000e+00 0.0000000e+00]
  [3.7996143e-03 9.9620038e-01]
  [6.2295800e-04 9.9937707e-01]
  [3.6474672e-01 6.3525325e-01]]

 [[2.6803015e-04 9.9973196e-01]
  [0.0000000e+00 0.0000000e+00]
  [8.2712650e-06 9.9999177e-01]
  [7.5160491e-04 9.9924833e-01]]

 [[8.1846714e-01 1.8153284e-01]
  [7.7636737e-01 2.2363266e-01]
  [0.0000000e+00 0.0000000e+00]
  [1.2092926e-05 9.9998796e-01]]

 [[9.9999976e-01 2.5378262e-07]
  [9.9999964e-01 3.6300779e-07]
  [2.9320776e-01 7.0679229e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.118:
[[[0.00000000e+00 0.00000000e+00]
  [9.02334630e-01 9.76653621e-02]
  [9.99421239e-01 5.78770705e-04]
  [9.99967217e-01 3.28053138e-05]]

 [[9.96302843e-01 3.69715388e-03]
  [0.00000000e+00 0.00000000e+00]
  [1.99247934e-02 9.80075240e-01]
  [7.61582628e-02 9.23841774e-01]]

 [[1.00000000e+00 6.10758466e-09]
  [9.99991059e-01 8.92803200e-06]
  [0.00000000e+00 0.00000000e+00]
  [9.83605569e-05 9.99901652e-01]]

 [[1.00000000e+00 1.38775955e-14]
  [1.00000000e+00 2.41867273e-11]
  [9.99969244e-01 3.07798255e-05]
  [0.00000000e+00 0.00000000e+00]]]

Rep 1,001.338:
[[[0.0000000e+00 0.0000000e+00]
  [9.5478899e-05 9.9990451e-01]
  [4.2284182e-03 9.9577159e-01]
  [1.3695578e-01 8.6304414e-01]]

 [[6.9549560e-07 9.9999928e-01]
  [0.0000000e+00 0.0000000e+00]
  [8.8348052e-06 9.9999118e-01]
  [8.0557533e-05 9.9991941e-01]]

 [[8.7520707e-04 9.9912483e-01]
  [1.9699757e-03 9.9803001e-01]
  [0.0000000e+00 0.0000000e+00]
  [4.8357174e-07 9.9999952e-01]]

 [[9.6609735e-01 3.3902586e-02]
  [9.5808458e-01 4.1915454e-02]
  [5.1979765e-07 9.9999952e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.304:
[[[0.0000000e+00 0.0000000e+00]
  [9.0334341e-02 9.0966564e-01]
  [9.9771333e-01 2.2866859e-03]
  [9.9994409e-01 5.5881032e-05]]

 [[3.7983108e-07 9.9999964e-01]
  [0.0000000e+00 0.0000000e+00]
  [4.7516269e-03 9.9524838e-01]
  [2.2946933e-02 9.7705305e-01]]

 [[3.2852006e-06 9.9999666e-01]
  [9.1191278e-06 9.9999094e-01]
  [0.0000000e+00 0.0000000e+00]
  [3.2634609e-06 9.9999678e-01]]

 [[1.2258877e-04 9.9987745e-01]
  [2.2858467e-04 9.9977142e-01]
  [4.2240099e-06 9.9999583e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.259:
[[[0.0000000e+00 0.0000000e+00]
  [9.9998748e-01 1.2571865e-05]
  [1.0000000e+00 9.3750110e-09]
  [9.9999297e-01 7.0470796e-06]]

 [[9.9729687e-01 2.7030983e-03]
  [0.0000000e+00 0.0000000e+00]
  [9.9589157e-01 4.1084131e-03]
  [9.0098098e-02 9.0990192e-01]]

 [[1.0000000e+00 2.4132099e-09]
  [9.9999976e-01 2.8685875e-07]
  [0.0000000e+00 0.0000000e+00]
  [8.8810538e-07 9.9999917e-01]]

 [[1.0000000e+00 1.6039446e-14]
  [1.0000000e+00 3.2100791e-12]
  [1.1725488e-02 9.8827446e-01]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.927:
[[[0.0000000e+00 0.0000000e+00]
  [6.3933772e-03 9.9360657e-01]
  [9.9198252e-01 8.0174524e-03]
  [8.2631147e-01 1.7368850e-01]]

 [[2.0697992e-03 9.9793017e-01]
  [0.0000000e+00 0.0000000e+00]
  [8.1663273e-02 9.1833669e-01]
  [1.7282680e-04 9.9982721e-01]]

 [[9.7709173e-01 2.2908222e-02]
  [9.3997675e-01 6.0023237e-02]
  [0.0000000e+00 0.0000000e+00]
  [1.2874162e-06 9.9999869e-01]]

 [[1.0000000e+00 4.7043628e-08]
  [9.9999881e-01 1.1618466e-06]
  [9.9537969e-01 4.6203425e-03]
  [0.0000000e+00 0.0000000e+00]]]

Rep 1,001.502:
[[[0.00000000e+00 0.00000000e+00]
  [8.58971325e-05 9.99914050e-01]
  [5.46784580e-01 4.53215390e-01]
  [9.87585187e-01 1.24147674e-02]]

 [[3.51899666e-06 9.99996424e-01]
  [0.00000000e+00 0.00000000e+00]
  [2.10861937e-04 9.99789178e-01]
  [2.43619503e-03 9.97563839e-01]]

 [[6.11419964e-05 9.99938846e-01]
  [1.07532642e-05 9.99989271e-01]
  [0.00000000e+00 0.00000000e+00]
  [1.37451489e-05 9.99986291e-01]]

 [[4.75830748e-04 9.99524236e-01]
  [3.49107140e-05 9.99965072e-01]
  [1.49932575e-05 9.99984980e-01]
  [0.00000000e+00 0.00000000e+00]]]

Test metrics and hyperparameters logged for tensorboard at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11\test

---------------------------------------------------------------------------

<<<<<<<<<<<< DECODER OUTPUT PLOT (TEST) >>>>>>>>>>>>

Creating decoder output plot for rep '1,001.444' for [m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11...

Decoder output plot for rep '1001.4443' logged at C:\Aryan_Savant\Thesis_Projects\my_work\AFD_thesis\topology_estimation\logs\mass_sp_dm\M004\scene_1\nri\train\etypes=2\m004\apv\set_G\E=f_mlp1_og_D=gru\tswp_0\[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11\test

 ... (more hidden) ...

        Test metric               DataLoader 0        

       dec/test_loss          0.015057684853672981    
  enc/test_edge_accuracy       0.6600000858306885     
     enc/test_entropy          0.0827791765332222     
       enc/test_loss            7.324416160583496     
       nri/test_loss            7.339473724365234     


===========================================================================

Nri model '[m004_(apv+G)]-(E=f_mlp1_og_D=gru)_edge_est_2.11' training completed.


=== EXECUTION COMPLETED ===
Log saved at: 2025-09-17 11:56:06
